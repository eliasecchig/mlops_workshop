{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https:/www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Run experiments with Pipelines\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/comparing_pipeline_runs.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/comparing_pipeline_runs.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/experiments/comparing_pipeline_runs.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Before going to production, it's important to run the last series of experiments directly on the pipeline that you would run in production. Vertex Pipelines will allow you to experiment and track training Pipeline runs and its associated parameters. Then, you can  compare runs of these Pipelines to each others in order to figure out which is the best configuration generates the model you will register in the Vertex AI Model Registry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d220917f1302"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this notebook, you learn how to use `Vertex AI Experiments` and `Vertex AI Pipelines` to log a pipeline job and compare different pipeline jobs.\n",
    "\n",
    "The steps covered include:\n",
    "\n",
    "* Formalize a training component\n",
    "* Build a training pipeline\n",
    "* Run several Pipeline jobs and log their results\n",
    "* Compare different Pipeline jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "263933842022"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "In the following example, you train a simple distributed neural network (DNN) model to predict automobile's miles per gallon (MPG) based on automobile information in the [auto-mpg dataset](https://www.kaggle.com/devanshbesain/exploration-and-analysis-auto-mpg).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}-mlops\" \n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Experiments\n",
    "EXPERIMENT_NAME = \"test-experiment\"\n",
    "\n",
    "# Pipeline\n",
    "COMPILED_PIPELINE_FILE = \"pipeline.json\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipelines\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "pRUOFELefqf1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "# General\n",
    "import os\n",
    "import time\n",
    "\n",
    "logger = logging.getLogger(\"logger\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from kfp.v2.dsl import Model, importer\n",
    "\n",
    "import kfp.v2.compiler as compiler\n",
    "# Pipeline Experiments\n",
    "import kfp.v2.dsl as dsl\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform_v1.types.pipeline_state import PipelineState\n",
    "from kfp.v2.dsl import Metrics, Model, Output, component, Input, Dataset, Condition, Artifact, HTML\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "from kfp.v2.components import importer_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inR70nh38PeK"
   },
   "source": [
    "from google.cloud import aiplatform\n",
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "Nz0nasrh8T3c"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1NLYz1R-KWv"
   },
   "source": [
    "## Formalize your training as pipeline component\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnfKxpj0-Z0H"
   },
   "source": [
    "Before you start running your pipeline experiments, you have to formalize your training as pipeline component.\n",
    "\n",
    "To do that, you build the pipeline by using the `kfp.v2.dsl.component` decorator to convert your training task into a pipeline component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"tensorflow\"]\n",
    ")\n",
    "def extract_splits_op(\n",
    "    data_url: str,\n",
    "    training_split: Output[Dataset],\n",
    "    test_split: Output[Dataset],\n",
    "    split_frac: float =0.8, \n",
    "    random_state: int = 0\n",
    "):  \n",
    "    from tensorflow.python.keras.utils import data_utils\n",
    "    import pandas as pd\n",
    "    dataset_path = data_utils.get_file(\"auto-mpg.data\", data_url)\n",
    "    column_names = [\n",
    "        \"MPG\",\n",
    "        \"Cylinders\",\n",
    "        \"Displacement\",\n",
    "        \"Horsepower\",\n",
    "        \"Weight\",\n",
    "        \"Acceleration\",\n",
    "        \"Model Year\",\n",
    "        \"Origin\",\n",
    "    ]\n",
    "    raw_dataset = pd.read_csv(\n",
    "        dataset_path,\n",
    "        names=column_names,\n",
    "        na_values=\"?\",\n",
    "        comment=\"\\t\",\n",
    "        sep=\" \",\n",
    "        skipinitialspace=True,\n",
    "    )\n",
    "    dataset = raw_dataset.dropna()\n",
    "    dataset[\"Origin\"] = dataset[\"Origin\"].map(\n",
    "        lambda x: {1: \"USA\", 2: \"Europe\", 3: \"Japan\"}.get(x)\n",
    "    )\n",
    "    dataset = pd.get_dummies(dataset, prefix=\"\", prefix_sep=\"\")\n",
    "    train_dataset = dataset.sample(frac=split_frac, random_state=random_state)\n",
    "    test_dataset = dataset.drop(train_dataset.index)\n",
    "    train_dataset.to_csv(training_split.path, index=False)\n",
    "    test_dataset.to_csv(test_split.path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "jv_-vU46_eFN"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"gcsfs\",\n",
    "        \"tensorflow==2.8.0\"\n",
    "    ]\n",
    ")\n",
    "def custom_trainer_op(\n",
    "    training_split: Input[Dataset],\n",
    "    num_units: int,\n",
    "    epochs: int,\n",
    "    dropout_rate:float,\n",
    "    model: Output[Model],\n",
    ") -> str:\n",
    "    import pandas as pd\n",
    "    from tensorflow.python.keras import Sequential, layers\n",
    "    from tensorflow.python.keras.utils import data_utils    \n",
    "    \n",
    "    def normalize_train_dataset(train_dataset):\n",
    "        train_stats = train_dataset.describe()\n",
    "        train_stats = train_stats.transpose()\n",
    "        def norm(x):\n",
    "            return (x - train_stats[\"mean\"]) / train_stats[\"std\"]\n",
    "        normed_train_data = norm(train_dataset)\n",
    "\n",
    "        return normed_train_data\n",
    "    \n",
    "    \n",
    "    def train(\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        num_units=64,\n",
    "        activation=\"relu\",\n",
    "        dropout_rate=0.0,\n",
    "        validation_split=0.2,\n",
    "        epochs=1000,\n",
    "    ):\n",
    "\n",
    "        model = Sequential(\n",
    "            [\n",
    "                layers.Dense(\n",
    "                    num_units,\n",
    "                    activation=activation,\n",
    "                    input_shape=[len(train_dataset.keys())],\n",
    "                ),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "                layers.Dense(num_units, activation=activation),\n",
    "                layers.Dense(1),\n",
    "            ]\n",
    "        )\n",
    "        model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", \"mse\"])\n",
    "        print(model.summary())\n",
    "\n",
    "        history = model.fit(\n",
    "            train_data, train_labels, epochs=epochs, validation_split=validation_split\n",
    "        )\n",
    "\n",
    "        return model, history\n",
    "    \n",
    "    # Preprocessing ----------------------------------------------\n",
    "\n",
    "    train_dataset =  pd.read_csv(training_split.path)\n",
    "    train_labels = train_dataset.pop(\"MPG\")\n",
    "\n",
    "    normed_train_data = normalize_train_dataset(train_dataset)\n",
    "    \n",
    "    # Train ----------------------------------------------\n",
    "    model_obj, history = train(\n",
    "        normed_train_data,\n",
    "        train_labels,\n",
    "        num_units=num_units,\n",
    "        activation=\"relu\",\n",
    "        epochs=epochs,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    model_obj.save(model.uri)\n",
    "    return model.uri\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"gcsfs\",\n",
    "        \"tensorflow==2.8.0\"\n",
    "    ]\n",
    ")\n",
    "def evaluate_op(\n",
    "    training_split: Input[Dataset],\n",
    "    test_split: Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    metrics_metadata: Output[Metrics],\n",
    ")-> float:\n",
    "    import pandas as pd\n",
    "    from tensorflow.python.keras import Sequential, layers\n",
    "    from tensorflow.python.keras.utils import data_utils    \n",
    "    from tensorflow import keras\n",
    "    def normalize_test_dataset(train_dataset, test_dataset):\n",
    "        train_stats = train_dataset.describe()\n",
    "        train_stats = train_stats.transpose()\n",
    "        def norm(x):\n",
    "            return (x - train_stats[\"mean\"]) / train_stats[\"std\"]\n",
    "        normed_test_data = norm(test_dataset)\n",
    "\n",
    "        return normed_test_data\n",
    "\n",
    "    # Preprocess data ----------------------------------------------\n",
    "\n",
    "    train_dataset =  pd.read_csv(training_split.path)\n",
    "    train_labels = train_dataset.pop(\"MPG\")\n",
    "    test_dataset =  pd.read_csv(test_split.path)\n",
    "    test_labels = test_dataset.pop(\"MPG\")\n",
    "    normed_test_data = normalize_test_dataset(train_dataset, test_dataset)\n",
    "    \n",
    "    # Load model from disk ----------------------------------------------\n",
    "\n",
    "    model_object = keras.models.load_model(model.path)\n",
    "\n",
    "    # Evaluate ----------------------------------------------\n",
    "    loss, mae, mse = model_object.evaluate(normed_test_data, test_labels, verbose=2)\n",
    "\n",
    "    metrics_metadata.log_metric(\"test_loss\", loss)\n",
    "    metrics_metadata.log_metric(\"test_mae\", mae)\n",
    "    metrics_metadata.log_metric(\"test_mse\", mse)\n",
    "    return mae\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"tensorflow-data-validation\"\n",
    "    ]\n",
    ")\n",
    "def generate_statistics_op(\n",
    "    dataset: Input[Dataset],\n",
    "    statistics: Output[Artifact],\n",
    "    statistics_view: Output[HTML]\n",
    "):\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from tensorflow_data_validation.utils.display_util import get_statistics_html\n",
    "\n",
    "    dataset_statistics =  tfdv.generate_statistics_from_csv(\n",
    "        data_location=dataset.uri, output_path=statistics.uri\n",
    "    )\n",
    "\n",
    "    html_content = get_statistics_html(lhs_statistics=dataset_statistics)\n",
    "    statistics_view.path = f\"{statistics_view.path}.html\"\n",
    "    with open(statistics_view.path, \"w\") as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"tensorflow-data-validation\"\n",
    "    ]\n",
    ")\n",
    "def generate_statistics_view_comparison_op(\n",
    "    lhs_statistics: Input[Artifact],\n",
    "    rhs_statistics: Input[Artifact],\n",
    "    statistics_view: Output[HTML],\n",
    "    lhs_name: str = \"lhs_statistics\",\n",
    "    rhs_name: str = \"rhs_statistics\"\n",
    "):\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from tensorflow_data_validation.utils.display_util import get_statistics_html\n",
    "\n",
    "    lhs_statistics = tfdv.load_statistics(input_path=lhs_statistics.uri)\n",
    "    rhs_statistics = tfdv.load_statistics(input_path=rhs_statistics.uri)\n",
    "    html_content = get_statistics_html(\n",
    "        lhs_statistics=lhs_statistics,\n",
    "        rhs_statistics=rhs_statistics,\n",
    "        lhs_name=lhs_name,\n",
    "        rhs_name=rhs_name,\n",
    "    )\n",
    "\n",
    "    with open(statistics_view.path, \"w\") as f:\n",
    "        f.write(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1UiTZhkVoFM"
   },
   "source": [
    "## Build a pipeline\n",
    "\n",
    "Below code will perform creating pipelineJob in associated project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "id": "9Gfr6pNLU-dB"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"custom-training-pipeline\")\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    epochs: int = 2,\n",
    "    dropout_rate: float = 0.1,\n",
    "    num_units: int = 16,\n",
    "    region: str =\"us-central1\",\n",
    "    profile_data: str = \"False\",\n",
    "    push_model: str = \"False\"\n",
    "):\n",
    "    extract_splits_task = extract_splits_op(data_url=\"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")   \n",
    "    train_task = custom_trainer_op(\n",
    "            training_split=extract_splits_task.outputs['training_split'],\n",
    "            epochs=epochs,\n",
    "            dropout_rate=dropout_rate,\n",
    "            num_units=num_units\n",
    "    ).set_cpu_limit('4').set_memory_limit('16G')\n",
    "    \n",
    "    mae_model = evaluate_op(\n",
    "            training_split=extract_splits_task.outputs['training_split'],\n",
    "            test_split=extract_splits_task.outputs['test_split'],\n",
    "            model=train_task.outputs['model'],\n",
    "    ).outputs[\"output\"]\n",
    "    \n",
    "    with Condition(\n",
    "        profile_data != \"False\",\n",
    "        name=\"Profile data\",\n",
    "    ):\n",
    "        training_statistics_op = generate_statistics_op(dataset=extract_splits_task.outputs['training_split'])\n",
    "        test_statistics_op = generate_statistics_op(dataset=extract_splits_task.outputs['test_split'])\n",
    "        generate_statistics_view_comparison_op(\n",
    "            lhs_statistics=training_statistics_op.outputs['statistics'], \n",
    "            rhs_statistics=test_statistics_op.outputs['statistics'],\n",
    "            lhs_name=\"train_statistics\",\n",
    "            rhs_name=\"test_statistics\"\n",
    "        )\n",
    "   \n",
    "    with Condition(\n",
    "        mae_model < 100,\n",
    "        name=\"MAE is below threshold\",\n",
    "    ):\n",
    "        with Condition(\n",
    "            push_model != \"False\",\n",
    "            name=\"push model is below threshold\",\n",
    "        ):\n",
    "            managed_model = importer_node.importer(\n",
    "                artifact_uri=train_task.outputs['output'],\n",
    "                artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "                metadata={\n",
    "                    \"containerSpec\": {\n",
    "                        \"imageUri\": \"us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-9:latest\"\n",
    "                    }\n",
    "                },\n",
    "            ).outputs[\"artifact\"]\n",
    "\n",
    "            model_upload_op = ModelUploadOp(\n",
    "                project=PROJECT_ID,\n",
    "                display_name=\"mlops_model\",\n",
    "                unmanaged_container_model=managed_model,\n",
    "            )\n",
    "\n",
    "\n",
    "            endpoint_op = EndpointCreateOp(\n",
    "                project=project, location=region, display_name=\"mlops_model_endpoint\"\n",
    "            )\n",
    "            _ = ModelDeployOp(\n",
    "                        model=model_upload_op.outputs[\"model\"],\n",
    "                        endpoint=endpoint_op.outputs[\"endpoint\"],\n",
    "                        dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "                        dedicated_resources_min_replica_count=1,\n",
    "                        dedicated_resources_max_replica_count=1,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkfZ7qVAVjBO"
   },
   "source": [
    "### Task 1: Submit your first training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "id": "oYlLBGUSVibG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eliasecchi/.local/lib/python3.8/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=COMPILED_PIPELINE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "id": "95vG4-zPWc0B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230120222847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230120222847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230120222847')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230120222847')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230120222847?project=370018035372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230120222847?project=370018035372\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "        display_name=f\"pipeline-run\",\n",
    "        template_path=COMPILED_PIPELINE_FILE,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        parameter_values={\n",
    "        },\n",
    "    )\n",
    "job.submit()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 2: Monitor your data with automatic data profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "        display_name=f\"pipeline-run-deploy-endpoint\",\n",
    "        template_path=COMPILED_PIPELINE_FILE,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        parameter_values={\n",
    "           \"profile_data\":\"True\"\n",
    "        },\n",
    "    )\n",
    "job.submit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNb6kZ2l5t-O"
   },
   "source": [
    "### Task 3: Experiment with pipelines\n",
    "\n",
    "Now that you have the pipeline, you define its training configuration depending on the defined parameters. Below you have an example and how you can submit several pipeline runs with different parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "XPy0Jc8xXgpa"
   },
   "outputs": [],
   "source": [
    "parameters_list = [\n",
    "    {\"num_units\": 16, \"epochs\": 3, \"dropout_rate\": 0.1},\n",
    "    {\"num_units\": 16, \"epochs\": 10, \"dropout_rate\": 0.1},\n",
    "    {\"num_units\": 16, \"epochs\": 10, \"dropout_rate\": 0.2},\n",
    "    {\"num_units\": 32, \"epochs\": 10, \"dropout_rate\": 0.1},\n",
    "    {\"num_units\": 32, \"epochs\": 10, \"dropout_rate\": 0.2},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "id": "G0hm1no_WY8o",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113737')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113737')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230117113737?project=370018035372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230117113737?project=370018035372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113737 to Experiment: test-experiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.metadata.experiment_resources:Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113737 to Experiment: test-experiment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113740')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113740')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230117113740?project=370018035372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230117113740?project=370018035372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113740 to Experiment: test-experiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.metadata.experiment_resources:Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113740 to Experiment: test-experiment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113744')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113744')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230117113744?project=370018035372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230117113744?project=370018035372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113744 to Experiment: test-experiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.metadata.experiment_resources:Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113744 to Experiment: test-experiment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113747')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113747')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230117113747?project=370018035372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230117113747?project=370018035372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113747 to Experiment: test-experiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.metadata.experiment_resources:Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113747 to Experiment: test-experiment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113750')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113750')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230117113750?project=370018035372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230117113750?project=370018035372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113750 to Experiment: test-experiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.metadata.experiment_resources:Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230117113750 to Experiment: test-experiment\n"
     ]
    }
   ],
   "source": [
    "for i, parameters in enumerate(parameters_list):\n",
    "\n",
    "    job = aiplatform.PipelineJob(\n",
    "        display_name=f\"{EXPERIMENT_NAME}-pipeline-run-{i}\",\n",
    "        template_path=COMPILED_PIPELINE_FILE,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        parameter_values={\n",
    "            **parameters,\n",
    "        },\n",
    "    )\n",
    "#     We set the experiment name before submitting the pipeline, so that this pipeline can be associated to the experiment.\n",
    "    job.submit(experiment=EXPERIMENT_NAME)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 4: Prepare for production \n",
    "Push the model to the registry and deploying an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "        display_name=f\"{EXPERIMENT_NAME}-pipeline-run-{i}\",\n",
    "        template_path=COMPILED_PIPELINE_FILE,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        parameter_values={\n",
    "           \"profile_data\"=\"True\",\n",
    "           \"push_model\"=\"True\"\n",
    "            \n",
    "        },\n",
    "    )\n",
    "job.submit()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "comparing_pipeline_runs.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
