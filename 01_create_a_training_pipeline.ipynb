{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https:/www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Run experiments with Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Vertex Pipelines will allow you to experiment and track Training Pipeline runs and its associated parameters.\n",
    "You can  compare runs of these Pipelines to each others in order to figure out which is the best configuration generates the model you will register in the Vertex AI Model Registry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d220917f1302"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this notebook, you learn how to use `Vertex AI Experiments` and `Vertex AI Pipelines` to log a pipeline job and compare different pipeline jobs.\n",
    "\n",
    "The steps covered include:\n",
    "\n",
    "* Formalize a training component\n",
    "* Build a training pipeline\n",
    "* Run several Pipeline jobs and log their results\n",
    "* Compare different Pipeline jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "263933842022"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "In the following example, you train a simple distributed neural network (DNN) model to predict automobile's miles per gallon (MPG) based on automobile information in the [auto-mpg dataset](https://www.kaggle.com/devanshbesain/exploration-and-analysis-auto-mpg).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}-mlops\" \n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Experiments\n",
    "EXPERIMENT_NAME = \"test-experiment\"\n",
    "\n",
    "# Pipeline\n",
    "COMPILED_PIPELINE_FILE = \"pipeline.json\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipelines\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "pRUOFELefqf1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "# General\n",
    "import os\n",
    "import time\n",
    "\n",
    "logger = logging.getLogger(\"logger\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from kfp.v2.dsl import Model, importer\n",
    "\n",
    "import kfp.v2.compiler as compiler\n",
    "# Pipeline Experiments\n",
    "import kfp.v2.dsl as dsl\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform_v1.types.pipeline_state import PipelineState\n",
    "from kfp.v2.dsl import Metrics, Model, Output, component, Input, Dataset, Condition, Artifact, HTML\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "from kfp.v2.components import importer_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inR70nh38PeK"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Nz0nasrh8T3c"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1NLYz1R-KWv"
   },
   "source": [
    "## Formalize your training as pipeline component\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnfKxpj0-Z0H"
   },
   "source": [
    "Before you start running your pipeline experiments, you have to formalize your training as pipeline component.\n",
    "\n",
    "To do that, you build the pipeline by using the `kfp.v2.dsl.component` decorator to convert the typical steps of a training pipeline into pipeline components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"tensorflow\"]\n",
    ")\n",
    "def extract_splits_op(\n",
    "    data_url: str,\n",
    "    training_split: Output[Dataset],\n",
    "    test_split: Output[Dataset],\n",
    "    split_frac: float =0.8, \n",
    "    random_state: int = 0\n",
    "):  \n",
    "    from tensorflow.python.keras.utils import data_utils\n",
    "    import pandas as pd\n",
    "    dataset_path = data_utils.get_file(\"auto-mpg.data\", data_url)\n",
    "    column_names = [\n",
    "        \"MPG\",\n",
    "        \"Cylinders\",\n",
    "        \"Displacement\",\n",
    "        \"Horsepower\",\n",
    "        \"Weight\",\n",
    "        \"Acceleration\",\n",
    "        \"Model Year\",\n",
    "        \"Origin\",\n",
    "    ]\n",
    "    raw_dataset = pd.read_csv(\n",
    "        dataset_path,\n",
    "        names=column_names,\n",
    "        na_values=\"?\",\n",
    "        comment=\"\\t\",\n",
    "        sep=\" \",\n",
    "        skipinitialspace=True,\n",
    "    )\n",
    "    dataset = raw_dataset.dropna()\n",
    "    dataset[\"Origin\"] = dataset[\"Origin\"].map(\n",
    "        lambda x: {1: \"USA\", 2: \"Europe\", 3: \"Japan\"}.get(x)\n",
    "    )\n",
    "    dataset = pd.get_dummies(dataset, prefix=\"\", prefix_sep=\"\")\n",
    "    train_dataset = dataset.sample(frac=split_frac, random_state=random_state)\n",
    "    test_dataset = dataset.drop(train_dataset.index)\n",
    "    train_dataset.to_csv(training_split.path, index=False)\n",
    "    test_dataset.to_csv(test_split.path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "jv_-vU46_eFN"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"gcsfs\",\n",
    "        \"tensorflow==2.8.0\"\n",
    "    ]\n",
    ")\n",
    "def custom_trainer_op(\n",
    "    training_split: Input[Dataset],\n",
    "    num_units: int,\n",
    "    epochs: int,\n",
    "    dropout_rate:float,\n",
    "    model: Output[Model],\n",
    ") -> str:\n",
    "    import pandas as pd\n",
    "    from tensorflow.python.keras import Sequential, layers\n",
    "    from tensorflow.python.keras.utils import data_utils    \n",
    "    \n",
    "    def normalize_train_dataset(train_dataset):\n",
    "        train_stats = train_dataset.describe()\n",
    "        train_stats = train_stats.transpose()\n",
    "        def norm(x):\n",
    "            return (x - train_stats[\"mean\"]) / train_stats[\"std\"]\n",
    "        normed_train_data = norm(train_dataset)\n",
    "\n",
    "        return normed_train_data\n",
    "    \n",
    "    \n",
    "    def train(\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        num_units=64,\n",
    "        activation=\"relu\",\n",
    "        dropout_rate=0.0,\n",
    "        validation_split=0.2,\n",
    "        epochs=1000,\n",
    "    ):\n",
    "\n",
    "        model = Sequential(\n",
    "            [\n",
    "                layers.Dense(\n",
    "                    num_units,\n",
    "                    activation=activation,\n",
    "                    input_shape=[len(train_dataset.keys())],\n",
    "                ),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "                layers.Dense(num_units, activation=activation),\n",
    "                layers.Dense(1),\n",
    "            ]\n",
    "        )\n",
    "        model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", \"mse\"])\n",
    "        print(model.summary())\n",
    "\n",
    "        history = model.fit(\n",
    "            train_data, train_labels, epochs=epochs, validation_split=validation_split\n",
    "        )\n",
    "\n",
    "        return model, history\n",
    "    \n",
    "    # Preprocessing ----------------------------------------------\n",
    "\n",
    "    train_dataset =  pd.read_csv(training_split.path)\n",
    "    train_labels = train_dataset.pop(\"MPG\")\n",
    "\n",
    "    normed_train_data = normalize_train_dataset(train_dataset)\n",
    "    \n",
    "    # Train ----------------------------------------------\n",
    "    model_obj, history = train(\n",
    "        normed_train_data,\n",
    "        train_labels,\n",
    "        num_units=num_units,\n",
    "        activation=\"relu\",\n",
    "        epochs=epochs,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    model_obj.save(model.uri)\n",
    "    return model.uri\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"gcsfs\",\n",
    "        \"tensorflow==2.8.0\"\n",
    "    ]\n",
    ")\n",
    "def evaluate_op(\n",
    "    training_split: Input[Dataset],\n",
    "    test_split: Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    metrics_metadata: Output[Metrics],\n",
    ")-> float:\n",
    "    import pandas as pd\n",
    "    from tensorflow.python.keras import Sequential, layers\n",
    "    from tensorflow.python.keras.utils import data_utils    \n",
    "    from tensorflow import keras\n",
    "    def normalize_test_dataset(train_dataset, test_dataset):\n",
    "        train_stats = train_dataset.describe()\n",
    "        train_stats = train_stats.transpose()\n",
    "        def norm(x):\n",
    "            return (x - train_stats[\"mean\"]) / train_stats[\"std\"]\n",
    "        normed_test_data = norm(test_dataset)\n",
    "\n",
    "        return normed_test_data\n",
    "\n",
    "    # Preprocess data ----------------------------------------------\n",
    "\n",
    "    train_dataset =  pd.read_csv(training_split.path)\n",
    "    train_labels = train_dataset.pop(\"MPG\")\n",
    "    test_dataset =  pd.read_csv(test_split.path)\n",
    "    test_labels = test_dataset.pop(\"MPG\")\n",
    "    normed_test_data = normalize_test_dataset(train_dataset, test_dataset)\n",
    "    \n",
    "    # Load model from disk ----------------------------------------------\n",
    "\n",
    "    model_object = keras.models.load_model(model.path)\n",
    "\n",
    "    # Evaluate ----------------------------------------------\n",
    "    loss, mae, mse = model_object.evaluate(normed_test_data, test_labels, verbose=2)\n",
    "\n",
    "    metrics_metadata.log_metric(\"test_loss\", loss)\n",
    "    metrics_metadata.log_metric(\"test_mae\", mae)\n",
    "    metrics_metadata.log_metric(\"test_mse\", mse)\n",
    "    return mae\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1UiTZhkVoFM"
   },
   "source": [
    "## Build a pipeline\n",
    "\n",
    "We create here the pipeline which will assemble all the several components defined above.\n",
    "The pipeline will:\n",
    "1. Extract the data from an URL\n",
    "2. Train a Tensorflow Model\n",
    "3. Evaluate the model on a test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "9Gfr6pNLU-dB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"custom-training-pipeline\")\n",
    "def pipeline(\n",
    "    epochs: int = 2,\n",
    "    dropout_rate: float = 0.1,\n",
    "    num_units: int = 16\n",
    "):\n",
    "    extract_splits_task = extract_splits_op(data_url=\"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")   \n",
    "    train_task = custom_trainer_op(\n",
    "            training_split=extract_splits_task.outputs['training_split'],\n",
    "            epochs=epochs,\n",
    "            dropout_rate=dropout_rate,\n",
    "            num_units=num_units\n",
    "#    We can specify different CPU, Memory and GPU requirements for every task of our pipeline.     \n",
    "    ).set_cpu_limit('4').set_memory_limit('16G')\n",
    "    \n",
    "    mae_model = evaluate_op(\n",
    "            training_split=extract_splits_task.outputs['training_split'],\n",
    "            test_split=extract_splits_task.outputs['test_split'],\n",
    "            model=train_task.outputs['model'],\n",
    "    ).outputs[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkfZ7qVAVjBO"
   },
   "source": [
    "### Task 1: Submit your first training pipeline\n",
    "Submit your first training pipeline using all the default parameters and explore the result in the Console. To do that you can click the Pipeline Job link generated by the Vertex AI SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "oYlLBGUSVibG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=COMPILED_PIPELINE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "95vG4-zPWc0B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183930')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183930')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230123183930?project=370018035372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230123183930?project=370018035372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSolution of pipeline triggered with parameters:\\n\\njob = aiplatform.PipelineJob(\\n        display_name=f\"pipeline-run\",\\n        template_path=COMPILED_PIPELINE_FILE,\\n        pipeline_root=PIPELINE_ROOT,\\n        parameter_values={\\n            \"num_units\": 32, \\n            \"epochs\": 10, \\n            \"dropout_rate\": 0.1\\n        },\\n    )\\njob.submit()\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "        display_name=f\"pipeline-run\",\n",
    "        template_path=COMPILED_PIPELINE_FILE,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        parameter_values={\n",
    "# You can add parameters to the pipeline here. See an example below\n",
    "\n",
    "        },\n",
    "    )\n",
    "job.submit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Solution of pipeline triggered with parameters:\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "        display_name=f\"pipeline-run\",\n",
    "        template_path=COMPILED_PIPELINE_FILE,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        parameter_values={\n",
    "            \"num_units\": 32, \n",
    "            \"epochs\": 10, \n",
    "            \"dropout_rate\": 0.1\n",
    "        },\n",
    "    )\n",
    "job.submit()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNb6kZ2l5t-O"
   },
   "source": [
    "### Task 2: Experiment with pipelines\n",
    "\n",
    "Now that your pipeline is ready to train ML models, you can define its training configuration based on parameters. Below you have an example and how you can submit several pipeline runs with different parameters. \n",
    "Once the pipelines are triggered you can explore the results by clicking each individual pipeline or by looking at the [generated experiment page](https://console.cloud.google.com/vertex-ai/locations/us-central1/experiments/test-experiment) to have an overall view of the experiment itself, grouping all the different pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "XPy0Jc8xXgpa"
   },
   "outputs": [],
   "source": [
    "parameters_list = [\n",
    "    {\"num_units\": 16, \"epochs\": 3, \"dropout_rate\": 0.1},\n",
    "    {\"num_units\": 16, \"epochs\": 10, \"dropout_rate\": 0.1},\n",
    "    {\"num_units\": 16, \"epochs\": 10, \"dropout_rate\": 0.2},\n",
    "    {\"num_units\": 32, \"epochs\": 10, \"dropout_rate\": 0.1},\n",
    "    {\"num_units\": 32, \"epochs\": 10, \"dropout_rate\": 0.2},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "G0hm1no_WY8o",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183936')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183936')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230123183936?project=370018035372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230123183936?project=370018035372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183936 to Experiment: test-experiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.metadata.experiment_resources:Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183936 to Experiment: test-experiment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183939')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183939')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230123183939?project=370018035372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230123183939?project=370018035372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183939 to Experiment: test-experiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.metadata.experiment_resources:Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183939 to Experiment: test-experiment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183942')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183942')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230123183942?project=370018035372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230123183942?project=370018035372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183942 to Experiment: test-experiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.metadata.experiment_resources:Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183942 to Experiment: test-experiment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183945')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183945')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230123183945?project=370018035372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230123183945?project=370018035372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183945 to Experiment: test-experiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.metadata.experiment_resources:Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183945 to Experiment: test-experiment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183948')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183948')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230123183948?project=370018035372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230123183948?project=370018035372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183948 to Experiment: test-experiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.metadata.experiment_resources:Associating projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183948 to Experiment: test-experiment\n"
     ]
    }
   ],
   "source": [
    "for i, parameters in enumerate(parameters_list):\n",
    "\n",
    "    job = aiplatform.PipelineJob(\n",
    "        display_name=f\"{EXPERIMENT_NAME}-pipeline-run-{i}\",\n",
    "        template_path=COMPILED_PIPELINE_FILE,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        parameter_values=parameters,\n",
    "    )\n",
    "#     We set the experiment name before submitting the pipeline, so that this pipeline can be associated to the experiment.\n",
    "    job.submit(experiment=EXPERIMENT_NAME)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the pipeline\n",
    "\n",
    "We now extend the pipeline to add new components to it.\n",
    "\n",
    "- If the pipeline parameter \"profile_data\" is not \"False\" the pipeline will perform a profile of the training data\n",
    "- If the MAE of the model is below 100 and the pipeline parameter \"push_model\" is not \"False\" the pipeline will perform a deployment of the model to an Endpoint to allow low latency serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data validation and profiling components\n",
    "\n",
    "As part of an ML pipeline it's important to monitor the data you use to train and serve your model.\n",
    "In this example we use TFX Tensorflow Data Validation, a comprehensive python package which allows you to perform:\n",
    "1. Data Profiling: automatically produce reports on your training data to understand the distributions of your features.\n",
    "2. Data Validation: set expectations on your training data to make sure you only train on good quality data.\n",
    "3. Training Serving Skew detection: compare training and serving data to detect skew and potentially retrain the model.\n",
    "\n",
    "We create here 2 components to generate a profile of the data and to visualise a comparison between the train and the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"tensorflow-data-validation\"\n",
    "    ]\n",
    ")\n",
    "def generate_statistics_op(\n",
    "    dataset: Input[Dataset],\n",
    "    statistics: Output[Artifact],\n",
    "    statistics_view: Output[HTML]\n",
    "):\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from tensorflow_data_validation.utils.display_util import get_statistics_html\n",
    "\n",
    "    dataset_statistics =  tfdv.generate_statistics_from_csv(\n",
    "        data_location=dataset.uri, output_path=statistics.uri\n",
    "    )\n",
    "\n",
    "    html_content = get_statistics_html(lhs_statistics=dataset_statistics)\n",
    "    statistics_view.path = f\"{statistics_view.path}.html\"\n",
    "    with open(statistics_view.path, \"w\") as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"tensorflow-data-validation\"\n",
    "    ]\n",
    ")\n",
    "def generate_statistics_view_comparison_op(\n",
    "    lhs_statistics: Input[Artifact],\n",
    "    rhs_statistics: Input[Artifact],\n",
    "    statistics_view: Output[HTML],\n",
    "    lhs_name: str = \"lhs_statistics\",\n",
    "    rhs_name: str = \"rhs_statistics\"\n",
    "):\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from tensorflow_data_validation.utils.display_util import get_statistics_html\n",
    "\n",
    "    lhs_statistics = tfdv.load_statistics(input_path=lhs_statistics.uri)\n",
    "    rhs_statistics = tfdv.load_statistics(input_path=rhs_statistics.uri)\n",
    "    html_content = get_statistics_html(\n",
    "        lhs_statistics=lhs_statistics,\n",
    "        rhs_statistics=rhs_statistics,\n",
    "        lhs_name=lhs_name,\n",
    "        rhs_name=rhs_name,\n",
    "    )\n",
    "\n",
    "    with open(statistics_view.path, \"w\") as f:\n",
    "        f.write(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"custom-training-pipeline\")\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    epochs: int = 2,\n",
    "    dropout_rate: float = 0.1,\n",
    "    num_units: int = 16,\n",
    "    region: str =\"us-central1\",\n",
    "    profile_data: str = \"False\",\n",
    "    push_model: str = \"False\"\n",
    "):\n",
    "    extract_splits_task = extract_splits_op(data_url=\"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")   \n",
    "    train_task = custom_trainer_op(\n",
    "            training_split=extract_splits_task.outputs['training_split'],\n",
    "            epochs=epochs,\n",
    "            dropout_rate=dropout_rate,\n",
    "            num_units=num_units\n",
    "#    We can specify different CPU, Memory and GPU requirements for every task of our pipeline.     \n",
    "    ).set_cpu_limit('4').set_memory_limit('16G')\n",
    "    \n",
    "    mae_model = evaluate_op(\n",
    "            training_split=extract_splits_task.outputs['training_split'],\n",
    "            test_split=extract_splits_task.outputs['test_split'],\n",
    "            model=train_task.outputs['model'],\n",
    "    ).outputs[\"output\"]\n",
    "    \n",
    "    with Condition(\n",
    "        profile_data != \"False\",\n",
    "        name=\"Profile data\",\n",
    "    ):\n",
    "        training_statistics_op = generate_statistics_op(dataset=extract_splits_task.outputs['training_split'])\n",
    "        test_statistics_op = generate_statistics_op(dataset=extract_splits_task.outputs['test_split'])\n",
    "        generate_statistics_view_comparison_op(\n",
    "            lhs_statistics=training_statistics_op.outputs['statistics'], \n",
    "            rhs_statistics=test_statistics_op.outputs['statistics'],\n",
    "            lhs_name=\"train_statistics\",\n",
    "            rhs_name=\"test_statistics\"\n",
    "        )\n",
    "   \n",
    "    with Condition(\n",
    "        mae_model < 100,\n",
    "        name=\"MAE is below threshold\",\n",
    "    ):\n",
    "        with Condition(\n",
    "            push_model != \"False\",\n",
    "            name=\"push model is below threshold\",\n",
    "        ):\n",
    "            managed_model = importer_node.importer(\n",
    "                artifact_uri=train_task.outputs['output'],\n",
    "                artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "                metadata={\n",
    "                    \"containerSpec\": {\n",
    "                        \"imageUri\": \"us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-9:latest\"\n",
    "                    }\n",
    "                },\n",
    "            ).outputs[\"artifact\"]\n",
    "\n",
    "            model_upload_op = ModelUploadOp(\n",
    "                project=PROJECT_ID,\n",
    "                display_name=\"mlops_model\",\n",
    "                unmanaged_container_model=managed_model,\n",
    "            )\n",
    "\n",
    "\n",
    "            endpoint_op = EndpointCreateOp(\n",
    "                project=project, location=region, display_name=\"mlops_model_endpoint\"\n",
    "            )\n",
    "            _ = ModelDeployOp(\n",
    "                        model=model_upload_op.outputs[\"model\"],\n",
    "                        endpoint=endpoint_op.outputs[\"endpoint\"],\n",
    "                        dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "                        dedicated_resources_min_replica_count=1,\n",
    "                        dedicated_resources_max_replica_count=1,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=COMPILED_PIPELINE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 3: Monitor your data with automatic data profiling\n",
    "Enable automated data profiling in your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183954')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/custom-training-pipeline-20230123183954')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230123183954?project=370018035372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20230123183954?project=370018035372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSolution of pipeline triggered with data profiling enabled\\n\\njob = aiplatform.PipelineJob(\\n        display_name=f\"pipeline-run-deploy-endpoint\",\\n        template_path=COMPILED_PIPELINE_FILE,\\n        pipeline_root=PIPELINE_ROOT,\\n        parameter_values={\\n           \"profile_data\" : \"True\"\\n        },\\n    )\\njob.submit()\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "        display_name=f\"pipeline-run-deploy-endpoint\",\n",
    "        template_path=COMPILED_PIPELINE_FILE,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        parameter_values={\n",
    "# Enable here the data profiling. Have a look at the pipeline definition to understand how to do it!\n",
    "           \"\" : \"\"\n",
    "        },\n",
    "    )\n",
    "job.submit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Solution of pipeline triggered with data profiling enabled\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "        display_name=f\"pipeline-run-deploy-endpoint\",\n",
    "        template_path=COMPILED_PIPELINE_FILE,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        parameter_values={\n",
    "           \"profile_data\" : \"True\"\n",
    "        },\n",
    "    )\n",
    "job.submit()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 4: Prepare for production \n",
    "Once you are ready to schedule a certain pipeline to run in production, you can enable the pipeline to push the created model to the Model Registry and then deploy the model to an Endpoint if the MAE is below 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "        display_name=f\"{EXPERIMENT_NAME}-pipeline-run-{i}\",\n",
    "        template_path=COMPILED_PIPELINE_FILE,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        parameter_values={\n",
    "# Customise the pipeline parameters to enable data_profiling and pushing a model to the endpoint.\n",
    "            \"\":\"\",\n",
    "            \"\":\"\"\n",
    "        },\n",
    "    )\n",
    "# We trigger the pipeline synchrounosly so that we run the next cell only when this pipeline is completed.\n",
    "job.run()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Solution of pipeline triggered with data profiling enabled\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "        display_name=f\"{EXPERIMENT_NAME}-pipeline-run-{i}\",\n",
    "        template_path=COMPILED_PIPELINE_FILE,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        parameter_values={\n",
    "            \"profile_data\":\"True\",\n",
    "            \"push_model\":\"True\"    \n",
    "        },\n",
    "    )\n",
    "job.run()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send request to the Endpoint\n",
    "\n",
    "Once the above pipeline is completed, you will be able to test your online serving endpoint. \n",
    "You can do that with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Union, List, Dict\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import random\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import model_monitoring\n",
    "\n",
    "ENDPOINT_NAME= \"mlops_model_endpoint\"\n",
    "\n",
    "endpoint = vertex_ai.Endpoint.list(filter=f\"display_name={ENDPOINT_NAME}\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request input [[5, 7, 8, 3, 4, 6, 9, 1, 2]]\n",
      "Predicted value [[15.9476852]]\n",
      "Request input [[3, 2, 7, 5, 1, 9, 4, 8, 6]]\n",
      "Predicted value [[21.0523109]]\n",
      "Request input [[2, 5, 7, 9, 1, 3, 6, 8, 4]]\n",
      "Predicted value [[15.3422489]]\n",
      "Request input [[3, 9, 2, 5, 8, 4, 7, 6, 1]]\n",
      "Predicted value [[14.421196]]\n",
      "Request input [[4, 5, 3, 7, 6, 8, 2, 1, 9]]\n",
      "Predicted value [[22.3368282]]\n",
      "Request input [[4, 6, 3, 9, 2, 8, 7, 1, 5]]\n",
      "Predicted value [[20.2386265]]\n",
      "Request input [[8, 2, 4, 6, 3, 1, 7, 9, 5]]\n",
      "Predicted value [[19.5185356]]\n",
      "Request input [[4, 3, 5, 9, 6, 8, 7, 1, 2]]\n",
      "Predicted value [[21.3011894]]\n",
      "Request input [[8, 2, 1, 5, 4, 7, 3, 6, 9]]\n",
      "Predicted value [[23.8133774]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_request():\n",
    "#     We generate a list of 9 random numbers, feel free to customise the input here!\n",
    "    request =  [random.sample(range(1,10),9)]\n",
    "    return request\n",
    "\n",
    "for online_sample in range(1,10):\n",
    "    request = get_request()\n",
    "    print(f\"Request input {request}\")\n",
    "    prediction = endpoint.predict(request)\n",
    "\n",
    "    print(f\"Predicted value {prediction.predictions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the same test in the Console: \n",
    "1. Visit the [Endpoint page](https://console.cloud.google.com/vertex-ai/endpoints)\n",
    "2. Click on the model endpoint `mlops_model_endpoint`\n",
    "3. Click on the model you want to test, in this case `mlops_model`\n",
    "4. Go to `Deploy & Test`\n",
    "5. Test by sending a request. You can copy past for example the following one `{\"instances\": [[1,2,3,4,5,6,7,8,9]]}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "comparing_pipeline_runs.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m102"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
