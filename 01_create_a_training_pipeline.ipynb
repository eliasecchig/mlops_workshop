{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https:/www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Run experiments with Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Vertex Pipelines will allow you to experiment and track Training Pipeline runs and its associated parameters.\n",
    "You can  compare runs of these Pipelines to each others in order to figure out which is the best configuration generates the model you will register in the Vertex AI Model Registry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d220917f1302"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this notebook, you learn how to use `Vertex AI Experiments` and `Vertex AI Pipelines` to log a pipeline job and compare different pipeline jobs.\n",
    "\n",
    "The steps covered include:\n",
    "\n",
    "* Formalize a training component\n",
    "* Build a training pipeline\n",
    "* Run several Pipeline jobs and log their results\n",
    "* Compare different Pipeline jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "263933842022"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "In the following example, you train a simple distributed neural network (DNN) model to predict automobile's miles per gallon (MPG) based on automobile information in the [auto-mpg dataset](https://www.kaggle.com/devanshbesain/exploration-and-analysis-auto-mpg).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}-mlops\" \n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Experiments\n",
    "EXPERIMENT_NAME = \"test-experiment\"\n",
    "\n",
    "# Pipeline\n",
    "COMPILED_PIPELINE_FILE = \"pipeline.json\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipelines\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "pRUOFELefqf1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "# General\n",
    "import os\n",
    "import time\n",
    "\n",
    "from kfp.v2.dsl import Model, importer\n",
    "\n",
    "import kfp.v2.compiler as compiler\n",
    "# Pipeline Experiments\n",
    "import kfp.v2.dsl as dsl\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform_v1.types.pipeline_state import PipelineState\n",
    "from kfp.v2.dsl import Metrics, Model, Output, component, Input, Dataset, Condition, Artifact, HTML\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "from kfp.v2.dsl import importer as importer_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inR70nh38PeK"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "Nz0nasrh8T3c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1NLYz1R-KWv"
   },
   "source": [
    "## Formalize your training as pipeline component\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnfKxpj0-Z0H"
   },
   "source": [
    "Before you start running your pipeline experiments, you have to formalize your training as pipeline component.\n",
    "\n",
    "To do that, you build the pipeline by using the `kfp.v2.dsl.component` decorator to convert the typical steps of a training pipeline into pipeline components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"tensorflow\"]\n",
    ")\n",
    "def extract_splits_op(\n",
    "    data_url: str,\n",
    "    training_split: Output[Dataset],\n",
    "    test_split: Output[Dataset],\n",
    "    split_frac: float =0.8, \n",
    "    random_state: int = 0\n",
    "):  \n",
    "    from tensorflow.python.keras.utils import data_utils\n",
    "    import pandas as pd\n",
    "    dataset_path = data_utils.get_file(\"auto-mpg.data\", data_url)\n",
    "    column_names = [\n",
    "        \"MPG\",\n",
    "        \"Cylinders\",\n",
    "        \"Displacement\",\n",
    "        \"Horsepower\",\n",
    "        \"Weight\",\n",
    "        \"Acceleration\",\n",
    "        \"Model Year\",\n",
    "        \"Origin\",\n",
    "    ]\n",
    "    raw_dataset = pd.read_csv(\n",
    "        dataset_path,\n",
    "        names=column_names,\n",
    "        na_values=\"?\",\n",
    "        comment=\"\\t\",\n",
    "        sep=\" \",\n",
    "        skipinitialspace=True,\n",
    "    )\n",
    "    dataset = raw_dataset.dropna()\n",
    "    dataset[\"Origin\"] = dataset[\"Origin\"].map(\n",
    "        lambda x: {1: \"USA\", 2: \"Europe\", 3: \"Japan\"}.get(x)\n",
    "    )\n",
    "    dataset = pd.get_dummies(dataset, prefix=\"\", prefix_sep=\"\")\n",
    "    train_dataset = dataset.sample(frac=split_frac, random_state=random_state)\n",
    "    test_dataset = dataset.drop(train_dataset.index)\n",
    "    train_dataset.to_csv(training_split.path, index=False)\n",
    "    test_dataset.to_csv(test_split.path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "jv_-vU46_eFN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"gcsfs\",\n",
    "        \"tensorflow==2.8.0\",\n",
    "        \"protobuf<=3.20.3\", \"numpy<2\"\n",
    "    ]\n",
    ")\n",
    "def custom_trainer_op(\n",
    "    training_split: Input[Dataset],\n",
    "    num_units: int,\n",
    "    epochs: int,\n",
    "    dropout_rate:float,\n",
    "    model: Output[Model],\n",
    ") -> str:\n",
    "    import pandas as pd\n",
    "    from tensorflow.python.keras import Sequential, layers\n",
    "    from tensorflow.python.keras.utils import data_utils    \n",
    "    \n",
    "    def normalize_train_dataset(train_dataset):\n",
    "        train_stats = train_dataset.describe()\n",
    "        train_stats = train_stats.transpose()\n",
    "        def norm(x):\n",
    "            return (x - train_stats[\"mean\"]) / train_stats[\"std\"]\n",
    "        normed_train_data = norm(train_dataset)\n",
    "\n",
    "        return normed_train_data\n",
    "    \n",
    "    \n",
    "    def train(\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        num_units=64,\n",
    "        activation=\"relu\",\n",
    "        dropout_rate=0.0,\n",
    "        validation_split=0.2,\n",
    "        epochs=1000,\n",
    "    ):\n",
    "\n",
    "        model = Sequential(\n",
    "            [\n",
    "                layers.Dense(\n",
    "                    num_units,\n",
    "                    activation=activation,\n",
    "                    input_shape=[len(train_dataset.keys())],\n",
    "                ),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "                layers.Dense(num_units, activation=activation),\n",
    "                layers.Dense(1),\n",
    "            ]\n",
    "        )\n",
    "        model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", \"mse\"])\n",
    "        print(model.summary())\n",
    "\n",
    "        history = model.fit(\n",
    "            train_data, train_labels, epochs=epochs, validation_split=validation_split\n",
    "        )\n",
    "\n",
    "        return model, history\n",
    "    \n",
    "    # Preprocessing ----------------------------------------------\n",
    "\n",
    "    train_dataset =  pd.read_csv(training_split.path)\n",
    "    train_labels = train_dataset.pop(\"MPG\")\n",
    "\n",
    "    normed_train_data = normalize_train_dataset(train_dataset)\n",
    "    \n",
    "    # Train ----------------------------------------------\n",
    "    model_obj, history = train(\n",
    "        normed_train_data,\n",
    "        train_labels,\n",
    "        num_units=num_units,\n",
    "        activation=\"relu\",\n",
    "        epochs=epochs,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    model_obj.save(model.uri)\n",
    "    return model.uri\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"gcsfs\",\n",
    "        \"tensorflow==2.8.0\",\n",
    "        \"protobuf<=3.20.3\",\"numpy<2\"\n",
    "    ]\n",
    ")\n",
    "def evaluate_op(\n",
    "    training_split: Input[Dataset],\n",
    "    test_split: Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    metrics_metadata: Output[Metrics],\n",
    ")-> float:\n",
    "    import pandas as pd\n",
    "    from tensorflow.python.keras import Sequential, layers\n",
    "    from tensorflow.python.keras.utils import data_utils    \n",
    "    from tensorflow import keras\n",
    "    def normalize_test_dataset(train_dataset, test_dataset):\n",
    "        train_stats = train_dataset.describe()\n",
    "        train_stats = train_stats.transpose()\n",
    "        def norm(x):\n",
    "            return (x - train_stats[\"mean\"]) / train_stats[\"std\"]\n",
    "        normed_test_data = norm(test_dataset)\n",
    "\n",
    "        return normed_test_data\n",
    "\n",
    "    # Preprocess data ----------------------------------------------\n",
    "\n",
    "    train_dataset =  pd.read_csv(training_split.path)\n",
    "    train_labels = train_dataset.pop(\"MPG\")\n",
    "    test_dataset =  pd.read_csv(test_split.path)\n",
    "    test_labels = test_dataset.pop(\"MPG\")\n",
    "    normed_test_data = normalize_test_dataset(train_dataset, test_dataset)\n",
    "    \n",
    "    # Load model from disk ----------------------------------------------\n",
    "\n",
    "    model_object = keras.models.load_model(model.path)\n",
    "\n",
    "    # Evaluate ----------------------------------------------\n",
    "    loss, mae, mse = model_object.evaluate(normed_test_data, test_labels, verbose=2)\n",
    "\n",
    "    metrics_metadata.log_metric(\"test_loss\", 1.0)\n",
    "    metrics_metadata.log_metric(\"test_mae\", 1.0)\n",
    "    metrics_metadata.log_metric(\"test_mse\", 1.0)\n",
    "    return 1.0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1UiTZhkVoFM"
   },
   "source": [
    "## Build a pipeline\n",
    "\n",
    "We create here the pipeline which will assemble all the several components defined above.\n",
    "The pipeline will:\n",
    "1. Extract the data from an URL\n",
    "2. Train a Tensorflow Model\n",
    "3. Evaluate the model on a test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "9Gfr6pNLU-dB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"custom-training-pipeline\")\n",
    "def pipeline(\n",
    "    epochs: int = 2,\n",
    "    dropout_rate: float = 0.1,\n",
    "    num_units: int = 16\n",
    "):\n",
    "    extract_splits_task = extract_splits_op(data_url=\"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")   \n",
    "    train_task = custom_trainer_op(\n",
    "            training_split=extract_splits_task.outputs['training_split'],\n",
    "            epochs=epochs,\n",
    "            dropout_rate=dropout_rate,\n",
    "            num_units=num_units\n",
    "#    We can specify different CPU, Memory and GPU requirements for every task of our pipeline.     \n",
    "    ).set_cpu_limit('4').set_memory_limit('16G')\n",
    "    \n",
    "    mae_model = evaluate_op(\n",
    "            training_split=extract_splits_task.outputs['training_split'],\n",
    "            test_split=extract_splits_task.outputs['test_split'],\n",
    "            model=train_task.outputs['model'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkfZ7qVAVjBO"
   },
   "source": [
    "### Task 1: Submit your first training pipeline\n",
    "Submit your first training pipeline using all the default parameters and explore the result in the Console. To do that you can click the Pipeline Job link generated by the Vertex AI SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "oYlLBGUSVibG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=COMPILED_PIPELINE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "95vG4-zPWc0B",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507105716\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507105716')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20250507105716?project=323656405210\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "        display_name=f\"pipeline-run\",\n",
    "        template_path=COMPILED_PIPELINE_FILE,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        parameter_values={\n",
    "            \"num_units\": 32, \n",
    "            \"epochs\": 10, \n",
    "            \"dropout_rate\": 0.1\n",
    "        },\n",
    "    )\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNb6kZ2l5t-O"
   },
   "source": [
    "### Task 2: Experiment with pipelines\n",
    "\n",
    "Now that your pipeline is ready to train ML models, you can define its training configuration based on parameters. Below you have an example and how you can submit several pipeline runs with different parameters. \n",
    "Once the pipelines are triggered you can explore the results by clicking each individual pipeline or by looking at the [generated experiment page](https://console.cloud.google.com/vertex-ai/locations/us-central1/experiments/test-experiment) to have an overall view of the experiment itself, grouping all the different pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "XPy0Jc8xXgpa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters_list = [\n",
    "    {\"num_units\": 16, \"epochs\": 3, \"dropout_rate\": 0.1},\n",
    "    {\"num_units\": 16, \"epochs\": 10, \"dropout_rate\": 0.1},\n",
    "    {\"num_units\": 16, \"epochs\": 10, \"dropout_rate\": 0.2},\n",
    "    {\"num_units\": 32, \"epochs\": 10, \"dropout_rate\": 0.1},\n",
    "    {\"num_units\": 32, \"epochs\": 10, \"dropout_rate\": 0.2},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "G0hm1no_WY8o",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507111306\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507111306')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20250507111306?project=323656405210\n",
      "Associating projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507111306 to Experiment: test-experiment\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507111308\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507111308')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20250507111308?project=323656405210\n",
      "Associating projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507111308 to Experiment: test-experiment\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507111309\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507111309')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20250507111309?project=323656405210\n",
      "Associating projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507111309 to Experiment: test-experiment\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507111311\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507111311')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20250507111311?project=323656405210\n",
      "Associating projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507111311 to Experiment: test-experiment\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507111313\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507111313')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20250507111313?project=323656405210\n",
      "Associating projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507111313 to Experiment: test-experiment\n"
     ]
    }
   ],
   "source": [
    "for i, parameters in enumerate(parameters_list):\n",
    "\n",
    "    job = aiplatform.PipelineJob(\n",
    "        display_name=f\"{EXPERIMENT_NAME}-pipeline-run-{i}\",\n",
    "        template_path=COMPILED_PIPELINE_FILE,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        parameter_values=parameters,\n",
    "    )\n",
    "#     We set the experiment name before submitting the pipeline, so that this pipeline can be associated to the experiment.\n",
    "    job.submit(experiment=EXPERIMENT_NAME)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the pipeline\n",
    "\n",
    "We now extend the pipeline to add new components to it.\n",
    "\n",
    "- If the pipeline parameter \"profile_data\" is not \"False\" the pipeline will perform a profile of the training data\n",
    "- If the MAE of the model is below 100 and the pipeline parameter \"push_model\" is not \"False\" the pipeline will perform a deployment of the model to an Endpoint to allow low latency serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data validation and profiling components\n",
    "\n",
    "As part of an ML pipeline it's important to monitor the data you use to train and serve your model.\n",
    "In this example we use TFX Tensorflow Data Validation, a comprehensive python package which allows you to perform:\n",
    "1. Data Profiling: automatically produce reports on your training data to understand the distributions of your features.\n",
    "2. Data Validation: set expectations on your training data to make sure you only train on good quality data.\n",
    "3. Training Serving Skew detection: compare training and serving data to detect skew and potentially retrain the model.\n",
    "\n",
    "We create here 2 components to generate a profile of the data and to visualise a comparison between the train and the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"tensorflow-data-validation\"\n",
    "    ]\n",
    ")\n",
    "def generate_statistics_op(\n",
    "    dataset: Input[Dataset],\n",
    "    statistics: Output[Artifact],\n",
    "    statistics_view: Output[HTML]\n",
    "):\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from tensorflow_data_validation.utils.display_util import get_statistics_html\n",
    "\n",
    "    dataset_statistics =  tfdv.generate_statistics_from_csv(\n",
    "        data_location=dataset.uri, output_path=statistics.uri\n",
    "    )\n",
    "\n",
    "    html_content = get_statistics_html(lhs_statistics=dataset_statistics)\n",
    "    statistics_view.path = f\"{statistics_view.path}.html\"\n",
    "    with open(statistics_view.path, \"w\") as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"tensorflow-data-validation\"\n",
    "    ]\n",
    ")\n",
    "def generate_statistics_view_comparison_op(\n",
    "    lhs_statistics: Input[Artifact],\n",
    "    rhs_statistics: Input[Artifact],\n",
    "    statistics_view: Output[HTML],\n",
    "    lhs_name: str = \"lhs_statistics\",\n",
    "    rhs_name: str = \"rhs_statistics\"\n",
    "):\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from tensorflow_data_validation.utils.display_util import get_statistics_html\n",
    "\n",
    "    lhs_statistics = tfdv.load_statistics(input_path=lhs_statistics.uri)\n",
    "    rhs_statistics = tfdv.load_statistics(input_path=rhs_statistics.uri)\n",
    "    html_content = get_statistics_html(\n",
    "        lhs_statistics=lhs_statistics,\n",
    "        rhs_statistics=rhs_statistics,\n",
    "        lhs_name=lhs_name,\n",
    "        rhs_name=rhs_name,\n",
    "    )\n",
    "\n",
    "    with open(statistics_view.path, \"w\") as f:\n",
    "        f.write(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_6658/3239116294.py:26: DeprecationWarning: dsl.Condition is deprecated. Please use dsl.If instead.\n",
      "  with Condition(\n",
      "/var/tmp/ipykernel_6658/3239116294.py:39: DeprecationWarning: dsl.Condition is deprecated. Please use dsl.If instead.\n",
      "  with Condition(\n",
      "/var/tmp/ipykernel_6658/3239116294.py:43: DeprecationWarning: dsl.Condition is deprecated. Please use dsl.If instead.\n",
      "  with Condition(\n"
     ]
    }
   ],
   "source": [
    "@dsl.pipeline(name=\"custom-training-pipeline\")\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    epochs: int = 2,\n",
    "    dropout_rate: float = 0.1,\n",
    "    num_units: int = 16,\n",
    "    region: str =\"us-central1\",\n",
    "    profile_data: str = \"False\",\n",
    "    push_model: str = \"False\"\n",
    "):\n",
    "    extract_splits_task = extract_splits_op(data_url=\"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")   \n",
    "    train_task = custom_trainer_op(\n",
    "            training_split=extract_splits_task.outputs['training_split'],\n",
    "            epochs=epochs,\n",
    "            dropout_rate=dropout_rate,\n",
    "            num_units=num_units\n",
    "#    We can specify different CPU, Memory and GPU requirements for every task of our pipeline.     \n",
    "    ).set_cpu_limit('4').set_memory_limit('16G')\n",
    "    \n",
    "    mae_model = evaluate_op(\n",
    "            training_split=extract_splits_task.outputs['training_split'],\n",
    "            test_split=extract_splits_task.outputs['test_split'],\n",
    "            model=train_task.outputs['model'],\n",
    "    ).outputs[\"Output\"]\n",
    "    \n",
    "    with Condition(\n",
    "        profile_data != \"False\",\n",
    "        name=\"Profile data\",\n",
    "    ):\n",
    "        training_statistics_op = generate_statistics_op(dataset=extract_splits_task.outputs['training_split'])\n",
    "        test_statistics_op = generate_statistics_op(dataset=extract_splits_task.outputs['test_split'])\n",
    "        generate_statistics_view_comparison_op(\n",
    "            lhs_statistics=training_statistics_op.outputs['statistics'], \n",
    "            rhs_statistics=test_statistics_op.outputs['statistics'],\n",
    "            lhs_name=\"train_statistics\",\n",
    "            rhs_name=\"test_statistics\"\n",
    "        )\n",
    "   \n",
    "    with Condition(\n",
    "        mae_model < 100,\n",
    "        name=\"MAE is below threshold\",\n",
    "    ):\n",
    "        with Condition(\n",
    "            push_model != \"False\",\n",
    "            name=\"push model is below threshold\",\n",
    "        ):\n",
    "            managed_model = importer_node(\n",
    "                artifact_uri=train_task.outputs['Output'],\n",
    "                artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "                metadata={\n",
    "                    \"containerSpec\": {\n",
    "                        \"imageUri\": \"us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-9:latest\"\n",
    "                    }\n",
    "                },\n",
    "            ).outputs[\"artifact\"]\n",
    "\n",
    "            model_upload_op = ModelUploadOp(\n",
    "                project=PROJECT_ID,\n",
    "                display_name=\"mlops_model\",\n",
    "                unmanaged_container_model=managed_model,\n",
    "            )\n",
    "\n",
    "\n",
    "            endpoint_op = EndpointCreateOp(\n",
    "                project=project, location=region, display_name=\"mlops_model_endpoint\"\n",
    "            )\n",
    "            _ = ModelDeployOp(\n",
    "                        model=model_upload_op.outputs[\"model\"],\n",
    "                        endpoint=endpoint_op.outputs[\"endpoint\"],\n",
    "                        dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "                        dedicated_resources_min_replica_count=1,\n",
    "                        dedicated_resources_max_replica_count=1,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=COMPILED_PIPELINE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 3: Monitor your data with automatic data profiling\n",
    "Enable automated data profiling in your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507110427\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507110427')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20250507110427?project=323656405210\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "        display_name=f\"pipeline-run-deploy-endpoint\",\n",
    "        template_path=COMPILED_PIPELINE_FILE,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        parameter_values={\n",
    "           \"profile_data\" : \"True\"\n",
    "        },\n",
    "    )\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 4: Prepare for production \n",
    "Once you are ready to schedule a certain pipeline to run in production, you can enable the pipeline to push the created model to the Model Registry and then deploy the model to an Endpoint if the MAE is below 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507110750\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507110750')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20250507110750?project=323656405210\n",
      "PipelineJob projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507110750 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507110750 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507110750 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507110750 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/323656405210/locations/us-central1/pipelineJobs/custom-training-pipeline-20250507110750 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m job \u001b[38;5;241m=\u001b[39m aiplatform\u001b[38;5;241m.\u001b[39mPipelineJob(\n\u001b[1;32m      2\u001b[0m         display_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEXPERIMENT_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-pipeline-run-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m         template_path\u001b[38;5;241m=\u001b[39mCOMPILED_PIPELINE_FILE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m         },\n\u001b[1;32m      9\u001b[0m     )\n\u001b[0;32m---> 10\u001b[0m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:334\u001b[0m, in \u001b[0;36mPipelineJob.run\u001b[0;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout, enable_preflight_validations)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m        Optional. Whether to enable preflight validations for the PipelineJob.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    332\u001b[0m network \u001b[38;5;241m=\u001b[39m network \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mnetwork\n\u001b[0;32m--> 334\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_preflight_validations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_preflight_validations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:863\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    862\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    866\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:382\u001b[0m, in \u001b[0;36mPipelineJob._run\u001b[0;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout, enable_preflight_validations)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to ensure network synchronization and to run\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03mthe configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m        Optional. Whether to enable preflight validations for the PipelineJob.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m    375\u001b[0m     service_account\u001b[38;5;241m=\u001b[39mservice_account,\n\u001b[1;32m    376\u001b[0m     network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m     enable_preflight_validations\u001b[38;5;241m=\u001b[39menable_preflight_validations,\n\u001b[1;32m    380\u001b[0m )\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# AutoSxS view model evaluations\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m details \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_details:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:788\u001b[0m, in \u001b[0;36mPipelineJob._block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m         log_wait \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(log_wait \u001b[38;5;241m*\u001b[39m multiplier, max_wait)\n\u001b[1;32m    787\u001b[0m         previous_time \u001b[38;5;241m=\u001b[39m current_time\n\u001b[0;32m--> 788\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# Error is only populated when the job state is\u001b[39;00m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;66;03m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01min\u001b[39;00m _PIPELINE_ERROR_STATES:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "        display_name=f\"{EXPERIMENT_NAME}-pipeline-run-{i}\",\n",
    "        template_path=COMPILED_PIPELINE_FILE,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        parameter_values={\n",
    "            \"profile_data\":\"True\",\n",
    "            \"push_model\":\"True\"    \n",
    "        },\n",
    "    )\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send request to the Endpoint\n",
    "\n",
    "Once the above pipeline is completed, you will be able to test your online serving endpoint. \n",
    "You can do that with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Union, List, Dict\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import random\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import model_monitoring\n",
    "\n",
    "ENDPOINT_NAME= \"mlops_model_endpoint\"\n",
    "\n",
    "endpoint = vertex_ai.Endpoint.list(filter=f\"display_name={ENDPOINT_NAME}\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_request():\n",
    "#     We generate a list of 9 random numbers, feel free to customise the input here!\n",
    "    request =  [random.sample(range(1,10),9)]\n",
    "    return request\n",
    "\n",
    "for online_sample in range(1,10):\n",
    "    request = get_request()\n",
    "    print(f\"Request input {request}\")\n",
    "    prediction = endpoint.predict(request)\n",
    "\n",
    "    print(f\"Predicted value {prediction.predictions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the same test in the Console: \n",
    "1. Visit the [Endpoint page](https://console.cloud.google.com/vertex-ai/endpoints)\n",
    "2. Click on the model endpoint `mlops_model_endpoint`\n",
    "3. Click on the model you want to test, in this case `mlops_model`\n",
    "4. Go to `Deploy & Test`\n",
    "5. Test by sending a request. You can copy past for example the following one `{\"instances\": [[1,2,3,4,5,6,7,8,9]]}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "comparing_pipeline_runs.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
