{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15dd06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "# Authors: \n",
    "# Fabian Hirschmann <fhirschmann@google.com>, \n",
    "# Elia Secchi <eliasecchi@google.com>,\n",
    "# Megha Agarwal <meghaag@google.com>,\n",
    "# Mandie Quartly <mandieq@google.com>\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d70205c",
   "metadata": {},
   "source": [
    "# Automated MLOps pipeline build, testing and deployment\n",
    "\n",
    "In the previous notebook, you created a machine learning pipeline to train a model. In this session, it's all about automating the training and deployment of this model. Hence, the objective this notebook is to:\n",
    "\n",
    "1. Refactor your Kubeflow pipeline into a Python file that can be compiled into YAML in an automated fashion.\n",
    "1. Write a script to deploy a compiled Kubeflow pipeline to Vertex AI.\n",
    "1. Use Cloud Build (CI/CD) to compile, test, and run your Kubeflow pipeline.\n",
    "1. Create a Cloud Source Repository (Git) to automatically trigger Cloud Build on every change on the master branch\n",
    "1. [Optional] Create a [pipeline template](https://cloud.google.com/vertex-ai/docs/pipelines/create-pipeline-template) to allow for the pipeline to be reused and retriggered\n",
    "1. [Optional] Setup a schedule for the pipeline, which can be done in 2 ways:\n",
    "      - using Cloud Scheduler job, which sends a message to a Pub/Sub topic, and then calls a Cloud Function to trigger the VertexAI pipeline.\n",
    "      - using the new Vertex AI Pipelines Schedules API\n",
    "      \n",
    "      \n",
    "As part of this notebook, you'll create the following files using the `%%writefile` directive.\n",
    "- `src/requirements.txt`: Python requirements file listing all dependencies.\n",
    "- `src/pipeline.py`: File containing your Kubeflow pipeline in Python and logic to compile the pipeline into YAML.\n",
    "- `src/create-pipeline-template.py`: Python script to create a Kubeflow Pipeline Template in Vertex AI.\n",
    "- `src/submit-pipeline.py`: Python script to submit pipeline to Vertex AI.\n",
    "- `src/cloudbuild.yml`: Cloud build pipeline to run your CI/CD process.\n",
    "- `src/tests/test_pipeline.py`: Unit test for pipeline\n",
    "- `cf-trigger/main.py`: Cloud Function to trigger your Kubeflow Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e533bcd",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a26a4f0",
   "metadata": {},
   "source": [
    "Ensure your Compute Engine default service account `{PROJECT_NUMBER}-compute@developer.gserviceaccount.com` has following permissions enabled:\n",
    "1. `Vertex AI User`\n",
    "1. `Cloud Build Editor`\n",
    "1. `Artifact Registry Writer`\n",
    "1. `Source Repository Administrator`\n",
    "1. `Storage Object Creator`\n",
    "1. `Storage Object Viewer`\n",
    "\n",
    "Ensure your Cloud Build default service account `{PROJECT_NUMBER}-@cloudbuild.gserviceaccount.com` has following permissions enabled:\n",
    "1. `Service Account User`\n",
    "1. `Vertex AI User`\n",
    "\n",
    "Please note relevant IAM permissions can sometimes take time (up to 15 min) to trickle through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fcba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669e8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/requirements.txt\n",
    "kfp==2.0.0b9\n",
    "pytest==7.2.0\n",
    "pytz==2022.7\n",
    "google-cloud-aiplatform==1.20.0\n",
    "google-api-core==2.10.2\n",
    "google-auth==1.35.0\n",
    "google-cloud-bigquery==1.20.0\n",
    "google-cloud-core==1.7.3\n",
    "google-cloud-resource-manager==1.6.3\n",
    "google-cloud-storage==2.2.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf05ca5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r src/requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca0511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf90436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "     \n",
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)\n",
    "\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ddbb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\"\n",
    "    \n",
    "BUCKET_NAME = f\"mlops-coaching-2-{PROJECT_ID}\"\n",
    "EXPERIMENT_NAME = \"mlops-coaching-2-experiment\"\n",
    "PIPELINE_NAME = \"mlops-coaching-2-pipeline\"\n",
    "ENDPOINT_NAME = \"mlops-coaching-2-endpoint\"\n",
    "REPOSITORY_NAME = f\"mlops-coaching-2-{PROJECT_ID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -c regional -l $REGION gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce3c4c-cd58-45aa-9e74-eb341a116ac8",
   "metadata": {},
   "source": [
    "Using the cell below, ensure the following cloud service APIs are enabled for this lab:\n",
    "1. `Vertex AI API`\n",
    "1. `Cloud Build API`\n",
    "1. `Artifact Registry API`\n",
    "1. `Cloud Source Repositories API`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2de0bd-0940-4eb5-82a8-8e254e21cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable aiplatform.googleapis.com\n",
    "!gcloud services enable cloudbuild.googleapis.com\n",
    "!gcloud services enable artifactregistry.googleapis.com\n",
    "!gcloud services enable sourcerepo.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a66b6f",
   "metadata": {},
   "source": [
    "## Automated MLOps pipeline creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a84feaa",
   "metadata": {},
   "source": [
    "### Create a script containing your Vertex AI/Kubeflow Pipeline to compile the pipeline into `pipeline.yaml`\n",
    "\n",
    "> <font color='green'>**Task 1**</font>\n",
    ">\n",
    "> Create a Python script `src/pipeline.py` that creates a file name `pipeline.yaml` from the Kubeflow pipeline you developed last week. The output file should be in YAML and not JSON format.\n",
    ">\n",
    "> If you were unable to produce a Kubeflow pipeline last week, please use the one provided below. Otherwise, replace it with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d71ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/pipeline.py\n",
    "\n",
    "## Your code goes below this line\n",
    "\n",
    "#### TODO: INSERT PIPELINE FROM MLOPS PART 1 ####\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp.dsl import pipeline\n",
    "from kfp.dsl import component\n",
    "from kfp import compiler\n",
    "\n",
    "@component() \n",
    "def concat(a: str, b: str) -> str:\n",
    "    return a + b\n",
    "\n",
    "@component\n",
    "def reverse(a: str) -> NamedTuple(\"outputs\", [(\"before\", str), (\"after\", str)]):\n",
    "    return a, a[::-1]\n",
    "\n",
    "@pipeline(name=\"mlops-coaching-pipeline\")\n",
    "def basic_pipeline(a: str='stres', b: str='sed'):\n",
    "    concat_task = concat(a=a, b=b)\n",
    "    reverse_task = reverse(a=concat_task.output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    compiler.Compiler().compile(pipeline_func=basic_pipeline, package_path=\"pipeline.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d4148",
   "metadata": {},
   "source": [
    "Using the next command, you can test the materialized pipeline generated by your script. You can view the output in a file named `pipeline.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c94289",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bb273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n20 pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca25b67-c432-4d42-8a10-2b9b94e7a980",
   "metadata": {},
   "source": [
    "### Test the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef73fcf-6a9c-49da-bb65-7f724a412c5e",
   "metadata": {},
   "source": [
    "> <font color='green'>**Task 2**</font>\n",
    "> Write unit/integration tests for the pipeline you created to ensure the component logic that you added works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e162c6-1cbb-456c-8b5c-d513fbe34064",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p src/tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8186aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/tests/test_pipeline.py\n",
    "\n",
    "import unittest\n",
    "from pipeline import concat, reverse, basic_pipeline\n",
    "\n",
    "class TestBasicPipeline(unittest.TestCase):\n",
    "    # def setUp(self):\n",
    "        # Get relevant component\n",
    "    \n",
    "    def test_concat_component(self):\n",
    "        self.assertEqual(concat.python_func(3, 3), 6)\n",
    "\n",
    "    def test_reverse(self):\n",
    "        self.assertEqual(reverse.python_func(\"stressed\")[1], \"desserts\")\n",
    "\n",
    "    def test_pipeline(self):\n",
    "        pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a7b41-9996-42c1-8849-70aef63273f9",
   "metadata": {},
   "source": [
    "Using the next command, you can run the tests in the script using python `unittest` test runner. It discovers all the test files that start with `test_*`\n",
    "\n",
    "You can also use other testing framework of your choice (e.g. `pytest`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191cfe48-0206-477c-a894-7c4ca66d6eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYTHONPATH=src python -m unittest discover -s src/tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a576513",
   "metadata": {},
   "source": [
    "### Create a script to submit your compile kubeflow pipeline (`pipeline.yaml`) to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c708bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/submit-pipeline.py\n",
    "import os\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "import google.auth\n",
    "\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "if not PROJECT_ID:\n",
    "    creds, PROJECT_ID = google.auth.default()\n",
    "\n",
    "REGION = os.environ[\"REGION\"]\n",
    "BUCKET_NAME = os.environ[\"BUCKET_NAME\"]\n",
    "EXPERIMENT_NAME = os.environ[\"EXPERIMENT_NAME\"]\n",
    "ENDPOINT_NAME = os.environ[\"ENDPOINT_NAME\"]\n",
    "PIPELINE_NAME = os.environ[\"PIPELINE_NAME\"]\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "sync_pipeline = os.getenv(\"SUBMIT_PIPELINE_SYNC\", 'False').lower() in ('true', '1', 't')\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path='pipeline.yaml',\n",
    "    location=REGION,\n",
    "    project=PROJECT_ID,\n",
    "    enable_caching=True,\n",
    "    pipeline_root=f'gs://{BUCKET_NAME}'\n",
    ")\n",
    "print(f\"Submitting pipeline {PIPELINE_NAME} in experiment {EXPERIMENT_NAME}.\")\n",
    "job.submit(experiment=EXPERIMENT_NAME)\n",
    "\n",
    "if sync_pipeline:\n",
    "    job.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3de2c1",
   "metadata": {},
   "source": [
    "Let's test this script in the Notebook. You can check the pipeline's status by clicking on the link printed by the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d630863",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%set_env REGION=$REGION\n",
    "%set_env BUCKET_NAME=$BUCKET_NAME\n",
    "%set_env EXPERIMENT_NAME=$EXPERIMENT_NAME\n",
    "%set_env PIPELINE_NAME=$PIPELINE_NAME\n",
    "%set_env ENDPOINT_NAME=$ENDPOINT_NAME\n",
    "%set_env SUBMIT_PIPELINE_SYNC=1\n",
    "\n",
    "!python src/submit-pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faaf9dd",
   "metadata": {},
   "source": [
    "### Create a pipeline template in Artifact Registry from `pipeline.yaml`\n",
    "\n",
    "A pipeline template is a resource that you can use to publish a workflow definition so that it can be reused multiple times, by a single user or by multiple users. This feature is [documented here](https://cloud.google.com/vertex-ai/docs/pipelines/create-pipeline-template).\n",
    "\n",
    "The Kubeflow Pipelines SDK registry client is a new client interface that you can use with a compatible registry server, such as Artifact Registry, for version control of your Kubeflow Pipelines (KFP) templates.\n",
    "\n",
    "> <font color='green'>**Task 4**</font>\n",
    ">\n",
    "> Create a Python script `src/create-pipeline-template.py` that uploads `pipeline.yaml` to the Vertex AI Pipeline registry.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e22a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create mlops2-repo --location=$REGION --repository-format=KFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5a84da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/create-pipeline-template.py\n",
    "import os\n",
    "import google.auth\n",
    "\n",
    "from kfp.registry import RegistryClient\n",
    "\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "if not PROJECT_ID:\n",
    "    creds, PROJECT_ID = google.auth.default()\n",
    "REGION = os.environ[\"REGION\"]\n",
    "\n",
    "## Your code goes below this line\n",
    "\n",
    "client = RegistryClient(host=f\"https://{REGION}-kfp.pkg.dev/{PROJECT_ID}/mlops2-repo\")\n",
    "\n",
    "template_name, template_version = client.upload_pipeline(\n",
    "  file_name=\"pipeline.yaml\",\n",
    "  tags=[\"v1\", \"latest\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c7182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/create-pipeline-template.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82cf0be",
   "metadata": {},
   "source": [
    "### Automate Kubeflow pipeline compilation, template generation, and execution through Cloud Build\n",
    "\n",
    "Cloud Build is a service that executes your builds on Google Cloud. In this exercise, we want to use it to both compile and run your machine learning pipeline. For more information, please refer to the [Cloud Build documentation](https://cloud.google.com/build/docs/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243972cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/cloudbuild.yaml\n",
    "steps:\n",
    "  # Install dependencies\n",
    "  - name: 'python'\n",
    "    entrypoint: 'pip'\n",
    "    args: [\"install\", \"-r\", \"requirements.txt\", \"--user\"]\n",
    "\n",
    "  # Compile pipeline\n",
    "  - name: 'python'\n",
    "    entrypoint: 'python'\n",
    "    args: ['pipeline.py']\n",
    "    id: 'compile'\n",
    "\n",
    "  # Test the Pipeline Components \n",
    "  - name: 'python'\n",
    "    entrypoint: 'python'\n",
    "    args: ['-m', 'unittest', 'discover', 'tests/']\n",
    "    id: 'test_pipeline'\n",
    "    waitFor: ['compile']\n",
    "\n",
    "  # Upload compiled pipeline to GCS\n",
    "  - name: 'gcr.io/cloud-builders/gsutil'\n",
    "    args: ['cp', 'pipeline.yaml', 'gs://${_BUCKET_NAME}']\n",
    "    id: 'upload'\n",
    "    waitFor: ['test_pipeline']\n",
    "        \n",
    "  # Run the Vertex AI Pipeline (synchronously for test/qa environment).\n",
    "  - name: 'python'\n",
    "    id: 'test'\n",
    "    entrypoint: 'python'\n",
    "    env: ['BUCKET_NAME=${_BUCKET_NAME}', 'EXPERIMENT_NAME=qa-${_EXPERIMENT_NAME}', 'PIPELINE_NAME=${_PIPELINE_NAME}',\n",
    "          'REGION=${_REGION}', 'ENDPOINT_NAME=qa-${_ENDPOINT_NAME}', 'SUBMIT_PIPELINE_SYNC=true']\n",
    "    args: ['submit-pipeline.py']\n",
    "\n",
    "  # Create pipeline template and upload it to the artifact registry\n",
    "  - name: 'python'\n",
    "    id: 'template'\n",
    "    entrypoint: 'python'\n",
    "    env: ['REGION=${_REGION}']\n",
    "    args: ['create-pipeline-template.py']\n",
    "    \n",
    "  # Run the Vertex AI Pipeline (asynchronously for prod environment). In a real production scenario, this would run in a different GCP project.\n",
    "  - name: 'python'\n",
    "    id: 'prod'\n",
    "    entrypoint: 'python'\n",
    "    env: ['BUCKET_NAME=${_BUCKET_NAME}', 'EXPERIMENT_NAME=prod-${_EXPERIMENT_NAME}', 'PIPELINE_NAME=${_PIPELINE_NAME}',\n",
    "          'REGION=${_REGION}', 'ENDPOINT_NAME=prod-${_ENDPOINT_NAME}', 'SUBMIT_PIPELINE_SYNC=false']\n",
    "    args: ['submit-pipeline.py']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4958d76",
   "metadata": {},
   "source": [
    "Cloud Build uses a special service account to execute builds on your behalf. When you enable the Cloud Build API on a Google Cloud project, the Cloud Build service account is automatically created and granted the Cloud Build Service Account role for the project. This role gives the service account permissions to perform several tasks, however you can grant more permissions to the service account to perform additional tasks. [This page](https://cloud.google.com/build/docs/securing-builds/configure-access-for-cloud-build-service-account) explains how to grant and revoke permissions to the Cloud Build service account.\n",
    "\n",
    "For Cloud Build to be able to deploy your pipeline, you need to give its' service account `{PROJECT_NUMBER}@cloudbuild.gserviceaccount.com` the **Vertex AI User** and **Service Account User** role. Note that it may take up to 5 minutes until the new permissions propagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d3ea3b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit ./src --config=src/cloudbuild.yaml --substitutions=_BUCKET_NAME=$BUCKET_NAME,_EXPERIMENT_NAME=$EXPERIMENT_NAME,_PIPELINE_NAME=$PIPELINE_NAME,_REGION=$REGION,_ENDPOINT_NAME=$ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68abf9",
   "metadata": {},
   "source": [
    "### Create a git repository and trigger Cloud Build execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f3a221",
   "metadata": {},
   "source": [
    "Before you can create the repository here, please **enable Cloud Source Repositories API** in the Google Cloud console.\n",
    "\n",
    "> <font color='green'>**Task 5**</font>\n",
    ">\n",
    "> Create a build trigger on the source repository that executes `cloudbuild.yaml`. Make sure to pass all **--substitutions**.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4953b256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud source repos create $REPOSITORY_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f9b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes below this line\n",
    "\n",
    "!gcloud beta builds triggers create cloud-source-repositories \\\n",
    "    --name=mlops2-source-trigger \\\n",
    "    --repo=$REPOSITORY_NAME \\\n",
    "    --branch-pattern=master \\\n",
    "    --build-config=cloudbuild.yaml \\\n",
    "    --substitutions=_BUCKET_NAME=$BUCKET_NAME,_EXPERIMENT_NAME=$EXPERIMENT_NAME,_PIPELINE_NAME=$PIPELINE_NAME,_REGION=$REGION,_ENDPOINT_NAME=$ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud source repos clone $REPOSITORY_NAME --project=$PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11fd771",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -av src/* $REPOSITORY_NAME/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd844c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $REPOSITORY_NAME && git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09baca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email \"{YOUR_EMAIl}\"\n",
    "!git config --global user.name \"{YOUR_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a067a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $REPOSITORY_NAME && git commit -a -m \"initial commit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6893f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $REPOSITORY_NAME && git push -u origin master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999b8347",
   "metadata": {},
   "source": [
    "## Schedule pipeline execution\n",
    "\n",
    "> <font color='green'>**Task 6**</font>\n",
    ">\n",
    "> Schedule the execution of the pipeline such that it runs every day.\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe7a9b",
   "metadata": {},
   "source": [
    "### Schedule Method 1: Cloud Scheduler → Pub/Sub → Cloud Functions\n",
    "\n",
    "Using this method, you create a Pub/Sub topic that triggers a Cloud Function that triggers a Vertex AI Pipeline. Note that the service account the Cloud Function runs as needs access to Cloud Storage and Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a16acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud pubsub topics create trigger-mlops-coaching-2-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66e5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p cf-trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed20d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cf-trigger/requirements.txt\n",
    "kfp==2.0.0b9\n",
    "pytest==7.2.0\n",
    "google-cloud-aiplatform==1.20.0\n",
    "pytz==2022.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cf-trigger/main.py\n",
    "\n",
    "## Your code goes below this line\n",
    "\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "REGION = os.environ[\"REGION\"]\n",
    "PIPELINE_NAME = os.environ[\"PIPELINE_NAME\"]\n",
    "PROJECT_ID = os.environ[\"PROJECT_ID\"]\n",
    "BUCKET_NAME = os.environ[\"BUCKET_NAME\"]\n",
    "\n",
    "\n",
    "def subscribe(event, context):\n",
    "    aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "    \n",
    "    job = aiplatform.PipelineJob(\n",
    "        display_name=PIPELINE_NAME,\n",
    "        template_path=f'gs://{BUCKET_NAME}/pipeline.yaml',\n",
    "        location=REGION,\n",
    "        project=PROJECT_ID,\n",
    "        enable_caching=False,\n",
    "        pipeline_root=f'gs://{BUCKET_NAME}'\n",
    "    )\n",
    "\n",
    "    job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b053fa0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Your code goes below this line\n",
    "\n",
    "!gcloud functions deploy mlops-coaching-2-function \\\n",
    "--source=./cf-trigger \\\n",
    "--entry-point=subscribe \\\n",
    "--trigger-topic trigger-mlops-coaching-2-pipeline \\\n",
    "--runtime python37 \\\n",
    "--ingress-settings internal-and-gclb \\\n",
    "--set-env-vars REGION=$REGION,PIPELINE_NAME=$PIPELINE_NAME,PROJECT_ID=$PROJECT_ID,BUCKET_NAME=$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d4775",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes below this line\n",
    "\n",
    "# TODO\n",
    "\n",
    "!gcloud scheduler jobs create pubsub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2113e3",
   "metadata": {},
   "source": [
    "### Schedule Method 2: Vertex AI Scheduler (in preview)\n",
    "\n",
    "To use the Schedules API during the Private Preview release, your project ID must be allowlisted in the `SCHEDULED_RUNS_TRUSTED_TESTER` group, you can use projects that you previously signed up with—for example. If you’re not in the trusted testers group, sign up by using this [sign-up form](https://docs.google.com/forms/d/e/1FAIpQLScDxABxIvqjeM_279dwTMmVfFBJD7qmW2leyU_ZBTYutJ62uA/viewform?usp=sf_link).\n",
    "\n",
    "The Vertex AI Schedule Service API is a new resource that lets you schedule ad hoc or recurring Vertex AI Pipeline runs. The goal of this service is to make scheduling a one off or recurring pipeline run simple, such that any Vertex AI user can quickly understand and leverage schedules to implement naive continuous training in their business.\n",
    "\n",
    "This new API will replace our previous guidance of using Cloud Scheduler with Cloud Functions to schedule a pipeline run. It may support additional Vertex AI resources in the future.\n",
    "\n",
    "Note that **Scheduler is currently only available via its REST interface and not the Vertex AI SDK**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f29797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "import json\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path='pipeline.yaml',\n",
    "    location=REGION,\n",
    "    project=PROJECT_ID,\n",
    "    pipeline_root=f'gs://{BUCKET_NAME}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ec59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_TEMPLATE = job.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc66e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "token = creds.token\n",
    "service_account_email = creds.service_account_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59bf5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{REGION}/schedules\"\n",
    "headers = {\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "\n",
    "## Your code goes below this line\n",
    "\n",
    "payload = {\n",
    "    \"display_name\": \"mlops2-schedule\",\n",
    "    \"cron\": \"0 1 * * *\",\n",
    "    \"max_concurrent_run_count\": \"2\",\n",
    "}\n",
    "\n",
    "print(requests.post(endpoint, json=payload, headers=headers).json())"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
