{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a66b6f",
   "metadata": {},
   "source": [
    "## 02) Automated MLOps pipeline build, testing and deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a84feaa",
   "metadata": {},
   "source": [
    "### Create a script containing your Vertex AI/Kubeflow Pipeline to compile the pipeline into `pipeline.yaml`\n",
    "\n",
    "> <font color='green'>**Task 1**</font>\n",
    ">\n",
    "> Create a Python script `src/pipeline.py` that creates a file name `pipeline.yaml` from the Kubeflow pipeline you developed last week. The output file should be in YAML and not JSON format.\n",
    ">\n",
    "> If you were unable to produce a Kubeflow pipeline last week, please use the one provided below. Otherwise, replace it with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76d71ce2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/pipeline.py\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'src/pipeline.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwritefile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msrc/pipeline.py\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport argparse\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom kfp import compiler\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom kfp import dsl\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m@dsl.component(packages_to_install=[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscikit-learn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpandas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjoblib\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef model_training_op(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        dataset: dsl.Input[dsl.Dataset],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        model: dsl.Output[dsl.Model]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    import glob\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    import json\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    import os\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    import joblib\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    import pandas as pd\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    from sklearn.ensemble import RandomForestClassifier\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    from sklearn.metrics import confusion_matrix\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    from sklearn.metrics import roc_auc_score\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    from sklearn.metrics import roc_curve\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    from sklearn.model_selection import train_test_split\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    TARGET_COLUMN = \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtip_bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    TARGET_LABELS = [\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtip<20\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtip>=20\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def sanitize(path: str) -> str:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        return path.replace(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/gcs/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, 1) if path and path.startswith(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m) else path\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def get_dataframe(path: str):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        if os.path.isdir(path):  # base data directory is passed\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            files = glob.glob(f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{path}\u001b[39;49;00m\u001b[38;5;124;43m/*.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        elif \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m in path:  # a glob expression is passed\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            files = glob.glob(path)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        else:  # single file is passed\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            files = [path]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        dfs = (pd.read_csv(f, header=0) for f in files)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        return pd.concat(dfs, ignore_index=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def create_datasets(training_data_dir: str, validation_data_dir: str):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCreates training and validation datasets.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        train_dataset = get_dataframe(training_data_dir)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        if validation_data_dir:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            return train_dataset, get_dataframe(validation_data_dir)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        else:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            return train_test_split(train_dataset, test_size=.25, random_state=42)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def log_metrics(y_pred: pd.Series, y_true: pd.Series, output_dir: str):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        curve = roc_curve(y_score=y_pred, y_true=y_true)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        auc = roc_auc_score(y_score=y_pred, y_true=y_true)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        cm = confusion_matrix(labels=[False, True], y_pred=y_pred, y_true=y_true)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        with open(f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{output_dir}\u001b[39;49;00m\u001b[38;5;124;43m/metrics.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m) as f:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            metrics = \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: auc}\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            metrics[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfusion_matrix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m] = \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            metrics[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfusion_matrix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m][\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategories\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m] = TARGET_LABELS\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            metrics[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfusion_matrix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m][\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatrix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m] = cm.tolist()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            metrics[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroc_curve\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m] = \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            metrics[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroc_curve\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m][\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfpr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m] = curve[0].tolist()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            metrics[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroc_curve\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m][\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtpr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m] = curve[1].tolist()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            metrics[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroc_curve\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m][\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthresholds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m] = curve[2].tolist()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            json.dump(metrics, f, indent=2)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def split(df: pd.DataFrame):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        return df.drop(TARGET_COLUMN, axis=1), df[TARGET_COLUMN]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def train(training_data_dir: str, validation_data_dir: str, output_dir: str):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        train_df, val_df = create_datasets(training_data_dir, validation_data_dir)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        X_train, y_train = split(train_df)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        X_test, y_test = split(val_df)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        model = RandomForestClassifier()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        model.fit(X_train, y_train)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        os.makedirs(output_dir, exist_ok=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        joblib.dump(model, f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{output_dir}\u001b[39;49;00m\u001b[38;5;124;43m/model.joblib\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        y_pred = model.predict(X_test)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        log_metrics(y_pred, y_test, output_dir)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        return model.score(X_test, y_test)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    train(f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{dataset.path}\u001b[39;49;00m\u001b[38;5;124;43m/train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{dataset.path}\u001b[39;49;00m\u001b[38;5;124;43m/val\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{model.path}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m@dsl.component()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef data_validation_op(dataset: dsl.Input[dsl.Dataset]) -> str:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    return \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m@dsl.component()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef data_preparation_op():\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    pass\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m@dsl.component(packages_to_install=[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle-cloud-aiplatform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef model_validation_op(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        metrics: dsl.Input[dsl.ClassificationMetrics],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        threshold_auc: float = 0.50\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m) -> str:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    return \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m if metrics.metadata[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m] > threshold_auc else \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mnot_valid\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m@dsl.component(packages_to_install=[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle-cloud-aiplatform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef model_upload_op(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        model: dsl.Input[dsl.Model],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        serving_container_image_uri: str,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        project_id: str,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        location: str,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        model_name: str) -> str:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    from google.cloud import aiplatform\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    aiplatform.init(project=project_id, location=location)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    matches = aiplatform.Model.list(filter=f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdisplay_name=\u001b[39;49m\u001b[38;5;132;43;01m{model_name}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parent_model = matches[0].resource_name if matches else None\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    registered_model = aiplatform.Model.upload(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        display_name=model_name,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        parent_model=parent_model,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        artifact_uri=model.uri,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        serving_container_image_uri=serving_container_image_uri\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    return registered_model.versioned_resource_name\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m@dsl.component(packages_to_install=[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle-cloud-aiplatform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpandas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef model_monitoring_op(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        dataset: dsl.Input[dsl.Dataset],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        monitoring_job_name: str,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        endpoint_name: str,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        project_id: str,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        location: str):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    import pandas as pd\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    from google.cloud import aiplatform\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    from google.cloud.aiplatform import model_monitoring\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    aiplatform.init(project=project_id, location=location)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    random_sampling = model_monitoring.RandomSampleConfig(sample_rate=0.1)  # sample 10\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    schedule_config = model_monitoring.ScheduleConfig(monitor_interval=1)  # every hour\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    sample_file = f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{dataset.path}\u001b[39;49;00m\u001b[38;5;124;43m/test/000000000000.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    # assuming filename, column order (expecting target to be the last column)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    cols = pd.read_csv(sample_file, nrows=0).columns.to_list()[:-1]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    skew_config = model_monitoring.SkewDetectionConfig(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        data_source=sample_file.replace(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/gcs/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, 1),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        data_format=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        skew_thresholds=\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43mcol: 0.3 for col in cols},\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        target_field=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtip_bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    objective_config = model_monitoring.ObjectiveConfig(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        skew_config\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    emails = []\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    alerting_config = model_monitoring.EmailAlertConfig(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        user_emails=emails, enable_logging=True\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    endpoints = aiplatform.Endpoint.list(filter=f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdisplay_name=\u001b[39;49m\u001b[38;5;132;43;01m{endpoint_name}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if len(endpoints) > 0:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        monitoring_job_resource_name = endpoints[0].gca_resource.model_deployment_monitoring_job\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        if (monitoring_job_resource_name):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            # can\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mt update an existing monitoring job if it\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43ms pending so deleting it first\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            job = aiplatform.ModelDeploymentMonitoringJob(monitoring_job_resource_name)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            job.delete()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        aiplatform.ModelDeploymentMonitoringJob.create(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            display_name=monitoring_job_name,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            endpoint=endpoints[0].resource_name,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            logging_sampling_strategy=random_sampling,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            schedule_config=schedule_config,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            alert_config=alerting_config,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            objective_configs=objective_config,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            project=project_id,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            location=location\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m@dsl.component(packages_to_install=[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle-cloud-aiplatform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef model_evaluation_upload_op(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        metrics: dsl.Input[dsl.ClassificationMetrics],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        model_resource_name: str,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        project_id: str,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        location: str):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    from google.api_core import gapic_v1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    from google.cloud import aiplatform\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    from google.protobuf.struct_pb2 import Struct\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    from google.protobuf.struct_pb2 import Value\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model_evaluation = \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdisplay_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpipeline-eval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetrics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: Value(struct_value=Struct(fields=\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauRoc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: Value(number_value=metrics.metadata[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m])})),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetrics_schema_uri\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs://google-cloud-aiplatform/schema/modelevaluation/classification_metrics_1.0.0.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    }\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    aiplatform.init(project=project_id, location=location)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    api_endpoint = location + \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m-aiplatform.googleapis.com\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    client = aiplatform.gapic.ModelServiceClient(client_info=gapic_v1.client_info.ClientInfo(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        user_agent=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle-cloud-pipeline-components\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        client_options=\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapi_endpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: api_endpoint,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        })\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    client.import_model_evaluation(parent=model_resource_name, model_evaluation=model_evaluation)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m@dsl.component()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef model_evaluation_op(model: dsl.Input[dsl.Model], metrics: dsl.Output[dsl.ClassificationMetrics]):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    import json\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    with open(f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{model.path}\u001b[39;49;00m\u001b[38;5;124;43m/metrics.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m) as f:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        model_metrics = json.load(f)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    conf_matrix = model_metrics[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfusion_matrix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    metrics.log_confusion_matrix(categories=conf_matrix[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategories\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m], matrix=conf_matrix[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatrix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    curve = model_metrics[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroc_curve\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    metrics.log_roc_curve(fpr=curve[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfpr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m], tpr=curve[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtpr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m], threshold=curve[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthresholds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    metrics.metadata[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m] = model_metrics[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m@dsl.component(packages_to_install=[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle-cloud-bigquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef data_extract_op(project_id: str, location: str, dataset: dsl.Output[dsl.Dataset]):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    import os\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    from google.cloud import bigquery\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    client = bigquery.Client()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    query = \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    EXPORT DATA OPTIONS(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        uri=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;132;43;01m{path}\u001b[39;49;00m\u001b[38;5;124;43m/*.csv\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        format=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mCSV\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        overwrite=true,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        header=true,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        field_delimiter=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m) AS\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    SELECT\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        EXTRACT(MONTH from pickup_datetime) as trip_month,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        EXTRACT(DAY from pickup_datetime) as trip_day,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        EXTRACT(DAYOFWEEK from pickup_datetime) as trip_day_of_week,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        EXTRACT(HOUR from pickup_datetime) as trip_hour,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        TIMESTAMP_DIFF(dropoff_datetime, pickup_datetime, SECOND) as trip_duration,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        trip_distance,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        payment_type,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        pickup_location_id as pickup_zone,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        pickup_location_id as dropoff_zone,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        IF((SAFE_DIVIDE(tip_amount, fare_amount) >= 0.2), 1, 0) AS tip_bin\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    FROM\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_\u001b[39;49m\u001b[38;5;132;43;01m{year}\u001b[39;49;00m\u001b[38;5;124;43m` TABLESAMPLE SYSTEM (1 PERCENT)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    WHERE\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        TIMESTAMP_DIFF(dropoff_datetime, pickup_datetime, SECOND) BETWEEN 300 AND 10800\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    LIMIT \u001b[39;49m\u001b[38;5;132;43;01m{limit}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    datasets = [\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        (f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{dataset.path}\u001b[39;49;00m\u001b[38;5;124;43m/train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, 2020, 10000),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        (f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{dataset.path}\u001b[39;49;00m\u001b[38;5;124;43m/val\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, 2020, 5000),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        (f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{dataset.path}\u001b[39;49;00m\u001b[38;5;124;43m/test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, 2020, 1000)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    ]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    for ds in datasets:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        path = ds[0].replace(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/gcs/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, 1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        os.makedirs(path, exist_ok=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        # ignoring the provided location as this dataset is in US\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        job = client.query(query.format(path=path, year=ds[1], limit=ds[2]), project=project_id, location=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        job.result()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m@dsl.component(packages_to_install=[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle-cloud-aiplatform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef model_deployment_op(model_name: str, endpoint_name: str, project_id: str, location: str):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    from google.cloud import aiplatform\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    aiplatform.init(project=project_id, location=location)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    endpoints = aiplatform.Endpoint.list(filter=f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdisplay_name=\u001b[39;49m\u001b[38;5;132;43;01m{endpoint_name}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if endpoints:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        endpoint = endpoints[0]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    else:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        endpoint = aiplatform.Endpoint.create(display_name=endpoint_name, project=project_id, location=location)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    models = aiplatform.Model.list(filter=f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdisplay_name=\u001b[39;49m\u001b[38;5;132;43;01m{model_name}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if models:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        models[0].deploy(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            endpoint=endpoint,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            traffic_percentage=100,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            machine_type=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn1-standard-2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            min_replica_count=1,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            max_replica_count=4)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m@dsl.pipeline(name=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaxi-tips-training\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef training_pipeline(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        project_id: str, location: str, endpoint: str = \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmlops_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, monitoring_job: str = \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_monitoring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model_name = \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaxi-tips\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    data_extraction_task = data_extract_op(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        project_id=project_id, location=location\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    ).set_display_name(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextract-data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    data_validation_task = data_validation_op(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        dataset=data_extraction_task.outputs[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    ).set_display_name(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidate-data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    data_preparation_task = data_preparation_op().set_display_name(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprepare-data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    data_preparation_task.after(data_validation_task)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    training_task = model_training_op(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        dataset=data_extraction_task.outputs[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    ).set_display_name(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain-model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    training_task.after(data_preparation_task)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model_evaluation_task = model_evaluation_op(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        model=training_task.outputs[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    ).set_display_name(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluate-model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model_validation_task = model_validation_op(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        metrics=model_evaluation_task.outputs[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetrics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    ).set_display_name(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidate-model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    with dsl.Condition(model_validation_task.output == \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, name=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheck-performance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        model_upload_task = model_upload_op(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            model=training_task.outputs[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            model_name=model_name,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            serving_container_image_uri=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meurope-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            project_id=project_id,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            location=location\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        ).set_display_name(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mregister-model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        model_evaluation_upload_task = model_evaluation_upload_op(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            metrics=model_evaluation_task.outputs[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetrics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            model_resource_name=model_upload_task.output,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            project_id=project_id,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            location=location\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        ).set_display_name(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mregister-model-evaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        with dsl.Condition(endpoint != \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[none]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, name=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheck-if-endpoint-set\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            model_deployment_task = model_deployment_op(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                model_name=model_name,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                endpoint_name=endpoint,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                project_id=project_id,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                location=location\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            ).set_display_name(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeploy-model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            model_deployment_task.after(model_evaluation_upload_task)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            with dsl.Condition(monitoring_job != \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[none]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, name=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheck-if-monitoring-enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                model_monitoring_task = model_monitoring_op(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    dataset=data_extraction_task.outputs[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    endpoint_name=endpoint,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    monitoring_job_name=monitoring_job,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    project_id=project_id,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    location=location\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                ).set_display_name(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmonitor-model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                model_monitoring_task.after(model_deployment_task)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef compile(filename: str):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    cmp = compiler.Compiler()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    cmp.compile(pipeline_func=training_pipeline, package_path=filename)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mif __name__ == \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__main__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser = argparse.ArgumentParser()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    parser.add_argument(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--pipeline-file-name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, type=str, default=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpipeline.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    args = parser.parse_args()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    compile(args.pipeline_file_name)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/IPython/core/interactiveshell.py:2417\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2415\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2416\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2417\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/IPython/core/magics/osm.py:854\u001b[0m, in \u001b[0;36mOSMagics.writefile\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m filename)\n\u001b[1;32m    853\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mappend \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 854\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    855\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(cell)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'src/pipeline.py'"
     ]
    }
   ],
   "source": [
    "%%writefile src/pipeline.py\n",
    "\n",
    "import argparse\n",
    "\n",
    "from kfp import compiler\n",
    "from kfp import dsl\n",
    "\n",
    "\n",
    "\n",
    "@dsl.component(packages_to_install=[\"scikit-learn\", \"pandas\", \"joblib\"])\n",
    "def model_training_op(\n",
    "        dataset: dsl.Input[dsl.Dataset],\n",
    "        model: dsl.Output[dsl.Model]\n",
    "):\n",
    "    import glob\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    TARGET_COLUMN = \"tip_bin\"\n",
    "    TARGET_LABELS = [\"tip<20%\", \"tip>=20%\"]\n",
    "\n",
    "    def sanitize(path: str) -> str:\n",
    "        return path.replace(\"gs://\", \"/gcs/\", 1) if path and path.startswith(\"gs://\") else path\n",
    "\n",
    "    def get_dataframe(path: str):\n",
    "        if os.path.isdir(path):  # base data directory is passed\n",
    "            files = glob.glob(f\"{path}/*.csv\")\n",
    "        elif \"*\" in path:  # a glob expression is passed\n",
    "            files = glob.glob(path)\n",
    "        else:  # single file is passed\n",
    "            files = [path]\n",
    "        dfs = (pd.read_csv(f, header=0) for f in files)\n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    def create_datasets(training_data_dir: str, validation_data_dir: str):\n",
    "        \"\"\"Creates training and validation datasets.\"\"\"\n",
    "\n",
    "        train_dataset = get_dataframe(training_data_dir)\n",
    "\n",
    "        if validation_data_dir:\n",
    "            return train_dataset, get_dataframe(validation_data_dir)\n",
    "        else:\n",
    "            return train_test_split(train_dataset, test_size=.25, random_state=42)\n",
    "\n",
    "    def log_metrics(y_pred: pd.Series, y_true: pd.Series, output_dir: str):\n",
    "        curve = roc_curve(y_score=y_pred, y_true=y_true)\n",
    "        auc = roc_auc_score(y_score=y_pred, y_true=y_true)\n",
    "        cm = confusion_matrix(labels=[False, True], y_pred=y_pred, y_true=y_true)\n",
    "\n",
    "        with open(f\"{output_dir}/metrics.json\", \"w\") as f:\n",
    "            metrics = {\"auc\": auc}\n",
    "            metrics[\"confusion_matrix\"] = {}\n",
    "            metrics[\"confusion_matrix\"][\"categories\"] = TARGET_LABELS\n",
    "            metrics[\"confusion_matrix\"][\"matrix\"] = cm.tolist()\n",
    "            metrics[\"roc_curve\"] = {}\n",
    "            metrics[\"roc_curve\"][\"fpr\"] = curve[0].tolist()\n",
    "            metrics[\"roc_curve\"][\"tpr\"] = curve[1].tolist()\n",
    "            metrics[\"roc_curve\"][\"thresholds\"] = curve[2].tolist()\n",
    "            json.dump(metrics, f, indent=2)\n",
    "\n",
    "    def split(df: pd.DataFrame):\n",
    "        return df.drop(TARGET_COLUMN, axis=1), df[TARGET_COLUMN]\n",
    "\n",
    "    def train(training_data_dir: str, validation_data_dir: str, output_dir: str):\n",
    "        train_df, val_df = create_datasets(training_data_dir, validation_data_dir)\n",
    "\n",
    "        X_train, y_train = split(train_df)\n",
    "        X_test, y_test = split(val_df)\n",
    "\n",
    "        model = RandomForestClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        joblib.dump(model, f\"{output_dir}/model.joblib\")\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        log_metrics(y_pred, y_test, output_dir)\n",
    "\n",
    "        return model.score(X_test, y_test)\n",
    "\n",
    "    train(f\"{dataset.path}/train\", f\"{dataset.path}/val\", f\"{model.path}\")\n",
    "\n",
    "\n",
    "@dsl.component()\n",
    "def data_validation_op(dataset: dsl.Input[dsl.Dataset]) -> str:\n",
    "    return \"valid\"\n",
    "\n",
    "\n",
    "@dsl.component()\n",
    "def data_preparation_op():\n",
    "    pass\n",
    "\n",
    "\n",
    "@dsl.component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def model_validation_op(\n",
    "        metrics: dsl.Input[dsl.ClassificationMetrics],\n",
    "        threshold_auc: float = 0.50\n",
    ") -> str:\n",
    "    return \"valid\" if metrics.metadata[\"auc\"] > threshold_auc else 'not_valid'\n",
    "\n",
    "\n",
    "@dsl.component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def model_upload_op(\n",
    "        model: dsl.Input[dsl.Model],\n",
    "        serving_container_image_uri: str,\n",
    "        project_id: str,\n",
    "        location: str,\n",
    "        model_name: str) -> str:\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(project=project_id, location=location)\n",
    "    matches = aiplatform.Model.list(filter=f\"display_name={model_name}\")\n",
    "    parent_model = matches[0].resource_name if matches else None\n",
    "\n",
    "    registered_model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        parent_model=parent_model,\n",
    "        artifact_uri=model.uri,\n",
    "        serving_container_image_uri=serving_container_image_uri\n",
    "    )\n",
    "\n",
    "    return registered_model.versioned_resource_name\n",
    "\n",
    "\n",
    "@dsl.component(packages_to_install=[\"google-cloud-aiplatform\", \"pandas\"])\n",
    "def model_monitoring_op(\n",
    "        dataset: dsl.Input[dsl.Dataset],\n",
    "        monitoring_job_name: str,\n",
    "        endpoint_name: str,\n",
    "        project_id: str,\n",
    "        location: str):\n",
    "    import pandas as pd\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform import model_monitoring\n",
    "\n",
    "    aiplatform.init(project=project_id, location=location)\n",
    "\n",
    "    random_sampling = model_monitoring.RandomSampleConfig(sample_rate=0.1)  # sample 10%\n",
    "    schedule_config = model_monitoring.ScheduleConfig(monitor_interval=1)  # every hour\n",
    "    sample_file = f\"{dataset.path}/test/000000000000.csv\"\n",
    "    # assuming filename, column order (expecting target to be the last column)\n",
    "    cols = pd.read_csv(sample_file, nrows=0).columns.to_list()[:-1]\n",
    "    skew_config = model_monitoring.SkewDetectionConfig(\n",
    "        data_source=sample_file.replace(\"/gcs/\", \"gs://\", 1),\n",
    "        data_format=\"csv\",\n",
    "        skew_thresholds={col: 0.3 for col in cols},\n",
    "        target_field=\"tip_bin\"\n",
    "    )\n",
    "    objective_config = model_monitoring.ObjectiveConfig(\n",
    "        skew_config\n",
    "    )\n",
    "    emails = []\n",
    "    alerting_config = model_monitoring.EmailAlertConfig(\n",
    "        user_emails=emails, enable_logging=True\n",
    "    )\n",
    "\n",
    "    endpoints = aiplatform.Endpoint.list(filter=f\"display_name={endpoint_name}\")\n",
    "    if len(endpoints) > 0:\n",
    "        monitoring_job_resource_name = endpoints[0].gca_resource.model_deployment_monitoring_job\n",
    "        if (monitoring_job_resource_name):\n",
    "            # can't update an existing monitoring job if it's pending so deleting it first\n",
    "            job = aiplatform.ModelDeploymentMonitoringJob(monitoring_job_resource_name)\n",
    "            job.delete()\n",
    "\n",
    "        aiplatform.ModelDeploymentMonitoringJob.create(\n",
    "            display_name=monitoring_job_name,\n",
    "            endpoint=endpoints[0].resource_name,\n",
    "            logging_sampling_strategy=random_sampling,\n",
    "            schedule_config=schedule_config,\n",
    "            alert_config=alerting_config,\n",
    "            objective_configs=objective_config,\n",
    "            project=project_id,\n",
    "            location=location\n",
    "        )\n",
    "\n",
    "\n",
    "@dsl.component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def model_evaluation_upload_op(\n",
    "        metrics: dsl.Input[dsl.ClassificationMetrics],\n",
    "        model_resource_name: str,\n",
    "        project_id: str,\n",
    "        location: str):\n",
    "    from google.api_core import gapic_v1\n",
    "    from google.cloud import aiplatform\n",
    "    from google.protobuf.struct_pb2 import Struct\n",
    "    from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "    model_evaluation = {\n",
    "        \"display_name\": \"pipeline-eval\",\n",
    "        \"metrics\": Value(struct_value=Struct(fields={\"auRoc\": Value(number_value=metrics.metadata[\"auc\"])})),\n",
    "        \"metrics_schema_uri\": \"gs://google-cloud-aiplatform/schema/modelevaluation/classification_metrics_1.0.0.yaml\"\n",
    "    }\n",
    "\n",
    "    aiplatform.init(project=project_id, location=location)\n",
    "    api_endpoint = location + '-aiplatform.googleapis.com'\n",
    "    client = aiplatform.gapic.ModelServiceClient(client_info=gapic_v1.client_info.ClientInfo(\n",
    "        user_agent=\"google-cloud-pipeline-components\"),\n",
    "        client_options={\n",
    "            \"api_endpoint\": api_endpoint,\n",
    "        })\n",
    "    client.import_model_evaluation(parent=model_resource_name, model_evaluation=model_evaluation)\n",
    "\n",
    "\n",
    "@dsl.component()\n",
    "def model_evaluation_op(model: dsl.Input[dsl.Model], metrics: dsl.Output[dsl.ClassificationMetrics]):\n",
    "    import json\n",
    "\n",
    "    with open(f\"{model.path}/metrics.json\", \"r\") as f:\n",
    "        model_metrics = json.load(f)\n",
    "\n",
    "    conf_matrix = model_metrics[\"confusion_matrix\"]\n",
    "    metrics.log_confusion_matrix(categories=conf_matrix[\"categories\"], matrix=conf_matrix[\"matrix\"])\n",
    "\n",
    "    curve = model_metrics[\"roc_curve\"]\n",
    "    metrics.log_roc_curve(fpr=curve[\"fpr\"], tpr=curve[\"tpr\"], threshold=curve[\"thresholds\"])\n",
    "\n",
    "    metrics.metadata[\"auc\"] = model_metrics[\"auc\"]\n",
    "\n",
    "\n",
    "@dsl.component(packages_to_install=[\"google-cloud-bigquery\"])\n",
    "def data_extract_op(project_id: str, location: str, dataset: dsl.Output[dsl.Dataset]):\n",
    "    import os\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"\n",
    "    EXPORT DATA OPTIONS(\n",
    "        uri='{path}/*.csv',\n",
    "        format='CSV',\n",
    "        overwrite=true,\n",
    "        header=true,\n",
    "        field_delimiter=',') AS\n",
    "    SELECT\n",
    "        EXTRACT(MONTH from pickup_datetime) as trip_month,\n",
    "        EXTRACT(DAY from pickup_datetime) as trip_day,\n",
    "        EXTRACT(DAYOFWEEK from pickup_datetime) as trip_day_of_week,\n",
    "        EXTRACT(HOUR from pickup_datetime) as trip_hour,\n",
    "        TIMESTAMP_DIFF(dropoff_datetime, pickup_datetime, SECOND) as trip_duration,\n",
    "        trip_distance,\n",
    "        payment_type,\n",
    "        pickup_location_id as pickup_zone,\n",
    "        pickup_location_id as dropoff_zone,\n",
    "        IF((SAFE_DIVIDE(tip_amount, fare_amount) >= 0.2), 1, 0) AS tip_bin\n",
    "    FROM\n",
    "        `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_{year}` TABLESAMPLE SYSTEM (1 PERCENT)\n",
    "    WHERE\n",
    "        TIMESTAMP_DIFF(dropoff_datetime, pickup_datetime, SECOND) BETWEEN 300 AND 10800\n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "    datasets = [\n",
    "        (f\"{dataset.path}/train\", 2020, 10000),\n",
    "        (f\"{dataset.path}/val\", 2020, 5000),\n",
    "        (f\"{dataset.path}/test\", 2020, 1000)\n",
    "    ]\n",
    "    for ds in datasets:\n",
    "        path = ds[0].replace(\"/gcs/\", \"gs://\", 1)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        # ignoring the provided location as this dataset is in US\n",
    "        job = client.query(query.format(path=path, year=ds[1], limit=ds[2]), project=project_id, location=\"us\")\n",
    "        job.result()\n",
    "\n",
    "\n",
    "@dsl.component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def model_deployment_op(model_name: str, endpoint_name: str, project_id: str, location: str):\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(project=project_id, location=location)\n",
    "    endpoints = aiplatform.Endpoint.list(filter=f\"display_name={endpoint_name}\")\n",
    "    if endpoints:\n",
    "        endpoint = endpoints[0]\n",
    "    else:\n",
    "        endpoint = aiplatform.Endpoint.create(display_name=endpoint_name, project=project_id, location=location)\n",
    "\n",
    "    models = aiplatform.Model.list(filter=f\"display_name={model_name}\")\n",
    "    if models:\n",
    "        models[0].deploy(\n",
    "            endpoint=endpoint,\n",
    "            traffic_percentage=100,\n",
    "            machine_type=\"n1-standard-2\",\n",
    "            min_replica_count=1,\n",
    "            max_replica_count=4)\n",
    "\n",
    "\n",
    "@dsl.pipeline(name=\"taxi-tips-training\")\n",
    "def training_pipeline(\n",
    "        project_id: str, location: str, endpoint: str = \"mlops_model\", monitoring_job: str = \"model_monitoring\"):\n",
    "    model_name = \"taxi-tips\"\n",
    "\n",
    "    data_extraction_task = data_extract_op(\n",
    "        project_id=project_id, location=location\n",
    "    ).set_display_name(\"extract-data\")\n",
    "\n",
    "    data_validation_task = data_validation_op(\n",
    "        dataset=data_extraction_task.outputs[\"dataset\"]\n",
    "    ).set_display_name(\"validate-data\")\n",
    "\n",
    "    data_preparation_task = data_preparation_op().set_display_name(\"prepare-data\")\n",
    "    data_preparation_task.after(data_validation_task)\n",
    "\n",
    "    training_task = model_training_op(\n",
    "        dataset=data_extraction_task.outputs[\"dataset\"],\n",
    "    ).set_display_name(\"train-model\")\n",
    "    training_task.after(data_preparation_task)\n",
    "\n",
    "    model_evaluation_task = model_evaluation_op(\n",
    "        model=training_task.outputs[\"model\"]\n",
    "    ).set_display_name(\"evaluate-model\")\n",
    "\n",
    "    model_validation_task = model_validation_op(\n",
    "        metrics=model_evaluation_task.outputs[\"metrics\"],\n",
    "    ).set_display_name(\"validate-model\")\n",
    "\n",
    "    with dsl.Condition(model_validation_task.output == \"valid\", name=\"check-performance\"):\n",
    "        model_upload_task = model_upload_op(\n",
    "            model=training_task.outputs[\"model\"],\n",
    "            model_name=model_name,\n",
    "            serving_container_image_uri=\"europe-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest\",\n",
    "            project_id=project_id,\n",
    "            location=location\n",
    "        ).set_display_name(\"register-model\")\n",
    "\n",
    "        model_evaluation_upload_task = model_evaluation_upload_op(\n",
    "            metrics=model_evaluation_task.outputs[\"metrics\"],\n",
    "            model_resource_name=model_upload_task.output,\n",
    "            project_id=project_id,\n",
    "            location=location\n",
    "        ).set_display_name(\"register-model-evaluation\")\n",
    "\n",
    "        with dsl.Condition(endpoint != \"[none]\", name=\"check-if-endpoint-set\"):\n",
    "            model_deployment_task = model_deployment_op(\n",
    "                model_name=model_name,\n",
    "                endpoint_name=endpoint,\n",
    "                project_id=project_id,\n",
    "                location=location\n",
    "            ).set_display_name(\"deploy-model\")\n",
    "            model_deployment_task.after(model_evaluation_upload_task)\n",
    "\n",
    "            with dsl.Condition(monitoring_job != \"[none]\", name=\"check-if-monitoring-enabled\"):\n",
    "                model_monitoring_task = model_monitoring_op(\n",
    "                    dataset=data_extraction_task.outputs[\"dataset\"],\n",
    "                    endpoint_name=endpoint,\n",
    "                    monitoring_job_name=monitoring_job,\n",
    "                    project_id=project_id,\n",
    "                    location=location\n",
    "                ).set_display_name(\"monitor-model\")\n",
    "                model_monitoring_task.after(model_deployment_task)\n",
    "\n",
    "\n",
    "def compile(filename: str):\n",
    "    cmp = compiler.Compiler()\n",
    "    cmp.compile(pipeline_func=training_pipeline, package_path=filename)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--pipeline-file-name\", type=str, default=\"pipeline.yaml\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    compile(args.pipeline_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d4148",
   "metadata": {},
   "source": [
    "Using the next command, you can test the materialized pipeline generated by your script. You can view the output in a file named `pipeline.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "54c94289",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66bb273c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# PIPELINE DEFINITION\n",
      "# Name: taxi-tips-training\n",
      "# Inputs:\n",
      "#    endpoint: str [Default: '[none]']\n",
      "#    location: str\n",
      "#    monitoring_job: str [Default: '[none]']\n",
      "#    project_id: str\n",
      "# Outputs:\n",
      "#    model-evaluation-op-metrics: system.ClassificationMetrics\n",
      "components:\n",
      "  comp-condition-1:\n",
      "    dag:\n",
      "      tasks:\n",
      "        condition-2:\n",
      "          componentRef:\n",
      "            name: comp-condition-2\n",
      "          dependentTasks:\n",
      "          - model-evaluation-upload-op\n",
      "          inputs:\n",
      "            artifacts:\n"
     ]
    }
   ],
   "source": [
    "!head -n20 pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca25b67-c432-4d42-8a10-2b9b94e7a980",
   "metadata": {},
   "source": [
    "### Test the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef73fcf-6a9c-49da-bb65-7f724a412c5e",
   "metadata": {},
   "source": [
    "> <font color='green'>**Task 2**</font>\n",
    "> Write unit/integration tests for the pipeline you created to ensure the component logic that you added works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "88e162c6-1cbb-456c-8b5c-d513fbe34064",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p src/tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8186aaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/tests/test_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/tests/test_pipeline.py\n",
    "\n",
    "import unittest\n",
    "from pipeline import training_pipeline\n",
    "\n",
    "class TestBasicPipeline(unittest.TestCase):\n",
    "    \n",
    "    def test_pipeline(self):\n",
    "        pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a7b41-9996-42c1-8849-70aef63273f9",
   "metadata": {},
   "source": [
    "Using the next command, you can run the tests in the script using python `unittest` test runner. It discovers all the test files that start with `test_*`\n",
    "\n",
    "You can also use other testing framework of your choice (e.g. `pytest`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "191cfe48-0206-477c-a894-7c4ca66d6eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=src python -m unittest discover -s src/tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a576513",
   "metadata": {},
   "source": [
    "### Create a script to submit your compile kubeflow pipeline (`pipeline.yaml`) to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "20c708bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/submit-pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/submit-pipeline.py\n",
    "import os\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "import google.auth\n",
    "\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "if not PROJECT_ID:\n",
    "    creds, PROJECT_ID = google.auth.default()\n",
    "\n",
    "REGION = os.environ[\"REGION\"]\n",
    "BUCKET_NAME = os.environ[\"BUCKET_NAME\"]\n",
    "EXPERIMENT_NAME = os.environ[\"EXPERIMENT_NAME\"]\n",
    "ENDPOINT_NAME = os.environ[\"ENDPOINT_NAME\"]\n",
    "PIPELINE_NAME = os.environ[\"PIPELINE_NAME\"]\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "sync_pipeline = os.getenv(\"SUBMIT_PIPELINE_SYNC\", 'False').lower() in ('true', '1', 't')\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path='pipeline.yaml',\n",
    "    location=REGION,\n",
    "    project=PROJECT_ID,\n",
    "    enable_caching=True,\n",
    "    pipeline_root=f'gs://{BUCKET_NAME}',\n",
    "    parameter_values={'project_id':PROJECT_ID, 'location':REGION}\n",
    ")\n",
    "print(f\"Submitting pipeline {PIPELINE_NAME} in experiment {EXPERIMENT_NAME}.\")\n",
    "job.submit(experiment=EXPERIMENT_NAME)\n",
    "\n",
    "if sync_pipeline:\n",
    "    job.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3de2c1",
   "metadata": {},
   "source": [
    "Let's test this script in the Notebook. You can check the pipeline's status by clicking on the link printed by the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7d630863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: REGION=us-central1\n",
      "env: BUCKET_NAME=mlops-dummy-vertex-ai-test-365213\n",
      "env: EXPERIMENT_NAME=mlops-dummy-experiment\n",
      "env: PIPELINE_NAME=mlops-dummy-pipeline\n",
      "env: ENDPOINT_NAME=mlops-dummy-endpoint\n",
      "env: SUBMIT_PIPELINE_SYNC=1\n",
      "Submitting pipeline mlops-dummy-pipeline in experiment mlops-dummy-experiment.\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/taxi-tips-training-20230116150907\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/taxi-tips-training-20230116150907')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/taxi-tips-training-20230116150907?project=370018035372\n",
      "Associating projects/370018035372/locations/us-central1/pipelineJobs/taxi-tips-training-20230116150907 to Experiment: mlops-dummy-experiment\n",
      "PipelineJob projects/370018035372/locations/us-central1/pipelineJobs/taxi-tips-training-20230116150907 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/taxi-tips-training-20230116150907\n"
     ]
    }
   ],
   "source": [
    "%set_env REGION=$REGION\n",
    "%set_env BUCKET_NAME=$BUCKET_NAME\n",
    "%set_env EXPERIMENT_NAME=$EXPERIMENT_NAME\n",
    "%set_env PIPELINE_NAME=$PIPELINE_NAME\n",
    "%set_env ENDPOINT_NAME=$ENDPOINT_NAME\n",
    "%set_env SUBMIT_PIPELINE_SYNC=1\n",
    "\n",
    "!python src/submit-pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82cf0be",
   "metadata": {},
   "source": [
    "### Automate Kubeflow pipeline compilation, template generation, and execution through Cloud Build\n",
    "\n",
    "Cloud Build is a service that executes your builds on Google Cloud. In this exercise, we want to use it to both compile and run your machine learning pipeline. For more information, please refer to the [Cloud Build documentation](https://cloud.google.com/build/docs/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "243972cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/cloudbuild.yaml\n",
    "steps:\n",
    "  # Install dependencies\n",
    "  - name: 'python'\n",
    "    entrypoint: 'pip'\n",
    "    args: [\"install\", \"-r\", \"requirements.txt\", \"--user\"]\n",
    "\n",
    "  # Compile pipeline\n",
    "  - name: 'python'\n",
    "    entrypoint: 'python'\n",
    "    args: ['pipeline.py']\n",
    "    id: 'compile'\n",
    "\n",
    "  # Test the Pipeline Components \n",
    "  - name: 'python'\n",
    "    entrypoint: 'python'\n",
    "    args: ['-m', 'unittest', 'discover', 'tests/']\n",
    "    id: 'test_pipeline'\n",
    "    waitFor: ['compile']\n",
    "\n",
    "  # Upload compiled pipeline to GCS\n",
    "  - name: 'gcr.io/cloud-builders/gsutil'\n",
    "    args: ['cp', 'pipeline.yaml', 'gs://${_BUCKET_NAME}']\n",
    "    id: 'upload'\n",
    "    waitFor: ['test_pipeline']\n",
    "        \n",
    "  # Run the Vertex AI Pipeline (synchronously for test/qa environment).\n",
    "  - name: 'python'\n",
    "    id: 'test'\n",
    "    entrypoint: 'python'\n",
    "    env: ['BUCKET_NAME=${_BUCKET_NAME}', 'EXPERIMENT_NAME=qa-${_EXPERIMENT_NAME}', 'PIPELINE_NAME=${_PIPELINE_NAME}',\n",
    "          'REGION=${_REGION}', 'ENDPOINT_NAME=qa-${_ENDPOINT_NAME}', 'SUBMIT_PIPELINE_SYNC=true']\n",
    "    args: ['submit-pipeline.py']\n",
    "    \n",
    "  # Run the Vertex AI Pipeline (asynchronously for prod environment). In a real production scenario, this would run in a different GCP project.\n",
    "  - name: 'python'\n",
    "    id: 'prod'\n",
    "    entrypoint: 'python'\n",
    "    env: ['BUCKET_NAME=${_BUCKET_NAME}', 'EXPERIMENT_NAME=prod-${_EXPERIMENT_NAME}', 'PIPELINE_NAME=${_PIPELINE_NAME}',\n",
    "          'REGION=${_REGION}', 'ENDPOINT_NAME=prod-${_ENDPOINT_NAME}', 'SUBMIT_PIPELINE_SYNC=false']\n",
    "    args: ['submit-pipeline.py']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4958d76",
   "metadata": {},
   "source": [
    "Cloud Build uses a special service account to execute builds on your behalf. When you enable the Cloud Build API on a Google Cloud project, the Cloud Build service account is automatically created and granted the Cloud Build Service Account role for the project. This role gives the service account permissions to perform several tasks, however you can grant more permissions to the service account to perform additional tasks. [This page](https://cloud.google.com/build/docs/securing-builds/configure-access-for-cloud-build-service-account) explains how to grant and revoke permissions to the Cloud Build service account.\n",
    "\n",
    "For Cloud Build to be able to deploy your pipeline, you need to give its' service account `{PROJECT_NUMBER}@cloudbuild.gserviceaccount.com` the **Vertex AI User** and **Service Account User** role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d6d3ea3b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 10 file(s) totalling 42.5 KiB before compression.\n",
      "Uploading tarball of [./src] to [gs://vertex-ai-test-365213_cloudbuild/source/1673957466.801475-f5a9588be6a64b0c9f6102b09ce96470.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/vertex-ai-test-365213/locations/global/builds/5cab7ada-9289-4caf-88c0-61668dbc92ea].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/5cab7ada-9289-4caf-88c0-61668dbc92ea?project=370018035372 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"5cab7ada-9289-4caf-88c0-61668dbc92ea\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://vertex-ai-test-365213_cloudbuild/source/1673957466.801475-f5a9588be6a64b0c9f6102b09ce96470.tgz#1673957467173731\n",
      "Copying gs://vertex-ai-test-365213_cloudbuild/source/1673957466.801475-f5a9588be6a64b0c9f6102b09ce96470.tgz#1673957467173731...\n",
      "/ [1 files][ 13.6 KiB/ 13.6 KiB]                                                \n",
      "Operation completed over 1 objects/13.6 KiB.\n",
      "BUILD\n",
      "Starting Step #0\n",
      "Step #0: Pulling image: python\n",
      "Step #0: Using default tag: latest\n",
      "Step #0: latest: Pulling from library/python\n",
      "Step #0: bbeef03cda1f: Pulling fs layer\n",
      "Step #0: f049f75f014e: Pulling fs layer\n",
      "Step #0: 56261d0e6b05: Pulling fs layer\n",
      "Step #0: 9bd150679dbd: Pulling fs layer\n",
      "Step #0: 5b282ee9da04: Pulling fs layer\n",
      "Step #0: 03f027d5e312: Pulling fs layer\n",
      "Step #0: 741e8aa30cc2: Pulling fs layer\n",
      "Step #0: 5ceb79cda400: Pulling fs layer\n",
      "Step #0: f2c00c886d59: Pulling fs layer\n",
      "Step #0: 9bd150679dbd: Waiting\n",
      "Step #0: 5b282ee9da04: Waiting\n",
      "Step #0: 03f027d5e312: Waiting\n",
      "Step #0: 741e8aa30cc2: Waiting\n",
      "Step #0: 5ceb79cda400: Waiting\n",
      "Step #0: f2c00c886d59: Waiting\n",
      "Step #0: f049f75f014e: Verifying Checksum\n",
      "Step #0: f049f75f014e: Download complete\n",
      "Step #0: 56261d0e6b05: Verifying Checksum\n",
      "Step #0: 56261d0e6b05: Download complete\n",
      "Step #0: 9bd150679dbd: Verifying Checksum\n",
      "Step #0: 9bd150679dbd: Download complete\n",
      "Step #0: 03f027d5e312: Verifying Checksum\n",
      "Step #0: 03f027d5e312: Download complete\n",
      "Step #0: 741e8aa30cc2: Verifying Checksum\n",
      "Step #0: 741e8aa30cc2: Download complete\n",
      "Step #0: bbeef03cda1f: Verifying Checksum\n",
      "Step #0: bbeef03cda1f: Download complete\n",
      "Step #0: 5ceb79cda400: Verifying Checksum\n",
      "Step #0: 5ceb79cda400: Download complete\n",
      "Step #0: f2c00c886d59: Verifying Checksum\n",
      "Step #0: f2c00c886d59: Download complete\n",
      "Step #0: 5b282ee9da04: Verifying Checksum\n",
      "Step #0: 5b282ee9da04: Download complete\n",
      "Step #0: bbeef03cda1f: Pull complete\n",
      "Step #0: f049f75f014e: Pull complete\n",
      "Step #0: 56261d0e6b05: Pull complete\n",
      "Step #0: 9bd150679dbd: Pull complete\n",
      "Step #0: 5b282ee9da04: Pull complete\n",
      "Step #0: 03f027d5e312: Pull complete\n",
      "Step #0: 741e8aa30cc2: Pull complete\n",
      "Step #0: 5ceb79cda400: Pull complete\n",
      "Step #0: f2c00c886d59: Pull complete\n",
      "Step #0: Digest: sha256:5f004bbd8b9b6ae1478fee7d4dfbf38305af7188946aa79925667fa658458ba9\n",
      "Step #0: Status: Downloaded newer image for python:latest\n",
      "Step #0: docker.io/library/python:latest\n",
      "Step #0: Collecting kfp==2.0.0b9\n",
      "Step #0:   Downloading kfp-2.0.0b9.tar.gz (351 kB)\n",
      "Step #0:       351.1/351.1 kB 25.6 MB/s eta 0:00:00\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting pytest==7.2.0\n",
      "Step #0:   Downloading pytest-7.2.0-py3-none-any.whl (316 kB)\n",
      "Step #0:       316.8/316.8 kB 47.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting pytz==2022.7\n",
      "Step #0:   Downloading pytz-2022.7-py2.py3-none-any.whl (499 kB)\n",
      "Step #0:       499.4/499.4 kB 63.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-aiplatform==1.20.0\n",
      "Step #0:   Downloading google_cloud_aiplatform-1.20.0-py2.py3-none-any.whl (2.3 MB)\n",
      "Step #0:       2.3/2.3 MB 107.8 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-api-core==2.10.2\n",
      "Step #0:   Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
      "Step #0:       115.6/115.6 kB 22.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-auth==1.35.0\n",
      "Step #0:   Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Step #0:       152.9/152.9 kB 29.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-bigquery==1.20.0\n",
      "Step #0:   Downloading google_cloud_bigquery-1.20.0-py2.py3-none-any.whl (154 kB)\n",
      "Step #0:       155.0/155.0 kB 30.3 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-core==1.7.3\n",
      "Step #0:   Downloading google_cloud_core-1.7.3-py2.py3-none-any.whl (28 kB)\n",
      "Step #0: Collecting google-cloud-resource-manager==1.6.3\n",
      "Step #0:   Downloading google_cloud_resource_manager-1.6.3-py2.py3-none-any.whl (233 kB)\n",
      "Step #0:       233.8/233.8 kB 37.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-storage==2.2.1\n",
      "Step #0:   Downloading google_cloud_storage-2.2.1-py2.py3-none-any.whl (107 kB)\n",
      "Step #0:       107.1/107.1 kB 19.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting absl-py<2,>=0.9\n",
      "Step #0:   Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Step #0:       126.5/126.5 kB 24.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting click<9,>=7.1.2\n",
      "Step #0:   Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Step #0:       96.6/96.6 kB 19.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting docstring-parser<1,>=0.7.3\n",
      "Step #0:   Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Step #0: Collecting kfp-pipeline-spec<0.2.0,>=0.1.16\n",
      "Step #0:   Downloading kfp_pipeline_spec-0.1.17-py3-none-any.whl (12 kB)\n",
      "Step #0: Collecting kfp-server-api<3.0.0,>=2.0.0a0\n",
      "Step #0:   Downloading kfp-server-api-2.0.0a6.tar.gz (60 kB)\n",
      "Step #0:       60.5/60.5 kB 11.4 MB/s eta 0:00:00\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting kubernetes<24,>=8.0.0\n",
      "Step #0:   Downloading kubernetes-23.6.0-py2.py3-none-any.whl (1.5 MB)\n",
      "Step #0:       1.5/1.5 MB 113.9 MB/s eta 0:00:00\n",
      "Step #0: Collecting protobuf<4,>=3.13.0\n",
      "Step #0:   Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Step #0:       162.1/162.1 kB 31.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting PyYAML<6,>=5.3\n",
      "Step #0:   Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
      "Step #0:       175.1/175.1 kB 31.7 MB/s eta 0:00:00\n",
      "Step #0:   Installing build dependencies: started\n",
      "Step #0:   Installing build dependencies: finished with status 'done'\n",
      "Step #0:   Getting requirements to build wheel: started\n",
      "Step #0:   Getting requirements to build wheel: finished with status 'done'\n",
      "Step #0:   Preparing metadata (pyproject.toml): started\n",
      "Step #0:   Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Step #0: Collecting requests-toolbelt<1,>=0.8.0\n",
      "Step #0:   Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "Step #0:       54.5/54.5 kB 9.8 MB/s eta 0:00:00\n",
      "Step #0: Collecting tabulate<1,>=0.8.6\n",
      "Step #0:   Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Step #0: Collecting cloudpickle<3,>=2.0.0\n",
      "Step #0:   Downloading cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
      "Step #0: Collecting Deprecated<2,>=1.2.7\n",
      "Step #0:   Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Step #0: Collecting fire<1,>=0.3.1\n",
      "Step #0:   Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "Step #0:       88.3/88.3 kB 20.0 MB/s eta 0:00:00\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting jsonschema<4,>=3.0.1\n",
      "Step #0:   Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Step #0:       56.3/56.3 kB 9.9 MB/s eta 0:00:00\n",
      "Step #0: Collecting strip-hints<1,>=0.1.8\n",
      "Step #0:   Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting uritemplate<4,>=3.0.1\n",
      "Step #0:   Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Step #0: Collecting typer<1.0,>=0.3.2\n",
      "Step #0:   Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Step #0: Collecting attrs>=19.2.0\n",
      "Step #0:   Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "Step #0:       60.0/60.0 kB 11.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting iniconfig\n",
      "Step #0:   Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Step #0: Collecting packaging\n",
      "Step #0:   Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "Step #0:       42.7/42.7 kB 8.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting pluggy<2.0,>=0.12\n",
      "Step #0:   Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Step #0: Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0\n",
      "Step #0:   Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "Step #0:       120.3/120.3 kB 24.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting proto-plus<2.0.0dev,>=1.22.0\n",
      "Step #0:   Downloading proto_plus-1.22.2-py3-none-any.whl (47 kB)\n",
      "Step #0:       47.9/47.9 kB 8.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting packaging\n",
      "Step #0:   Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Step #0:       40.8/40.8 kB 6.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "Step #0:   Downloading googleapis_common_protos-1.58.0-py2.py3-none-any.whl (223 kB)\n",
      "Step #0:       223.0/223.0 kB 39.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting requests<3.0.0dev,>=2.18.0\n",
      "Step #0:   Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "Step #0:       62.8/62.8 kB 12.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting cachetools<5.0,>=2.0.0\n",
      "Step #0:   Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Step #0: Collecting pyasn1-modules>=0.2.1\n",
      "Step #0:   Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Step #0:       155.3/155.3 kB 30.2 MB/s eta 0:00:00\n",
      "Step #0: Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.11/site-packages (from google-auth==1.35.0->-r requirements.txt (line 6)) (65.5.1)\n",
      "Step #0: Collecting six>=1.9.0\n",
      "Step #0:   Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Step #0: Collecting rsa<5,>=3.1.4\n",
      "Step #0:   Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Step #0: Collecting google-resumable-media>=0.3.1\n",
      "Step #0:   Downloading google_resumable_media-2.4.0-py2.py3-none-any.whl (77 kB)\n",
      "Step #0:       77.4/77.4 kB 14.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "Step #0:   Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Step #0: Collecting wrapt<2,>=1.10\n",
      "Step #0:   Downloading wrapt-1.14.1.tar.gz (50 kB)\n",
      "Step #0:       50.9/50.9 kB 10.1 MB/s eta 0:00:00\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting termcolor\n",
      "Step #0:   Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Step #0: Collecting grpcio<2.0dev,>=1.33.2\n",
      "Step #0:   Downloading grpcio-1.51.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "Step #0:       4.8/4.8 MB 73.6 MB/s eta 0:00:00\n",
      "Step #0: Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "Step #0:   Downloading grpcio_status-1.51.1-py3-none-any.whl (5.1 kB)\n",
      "Step #0: Collecting google-crc32c<2.0dev,>=1.0\n",
      "Step #0:   Downloading google_crc32c-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Step #0: Collecting pyrsistent>=0.14.0\n",
      "Step #0:   Downloading pyrsistent-0.19.3-py3-none-any.whl (57 kB)\n",
      "Step #0:       57.5/57.5 kB 12.0 MB/s eta 0:00:00\n",
      "Step #0: Collecting urllib3>=1.15\n",
      "Step #0:   Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "Step #0:       140.6/140.6 kB 26.8 MB/s eta 0:00:00\n",
      "Step #0: Collecting certifi\n",
      "Step #0:   Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Step #0:       155.3/155.3 kB 32.9 MB/s eta 0:00:00\n",
      "Step #0: Collecting python-dateutil\n",
      "Step #0:   Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Step #0:       247.7/247.7 kB 38.3 MB/s eta 0:00:00\n",
      "Step #0: Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
      "Step #0:   Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
      "Step #0:       55.3/55.3 kB 11.3 MB/s eta 0:00:00\n",
      "Step #0: Collecting requests-oauthlib\n",
      "Step #0:   Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Step #0: Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "Step #0:   Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Step #0:       98.3/98.3 kB 19.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting pyasn1<0.5.0,>=0.4.6\n",
      "Step #0:   Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Step #0:       77.1/77.1 kB 16.6 MB/s eta 0:00:00\n",
      "Step #0: Collecting charset-normalizer<4,>=2\n",
      "Step #0:   Downloading charset_normalizer-3.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (196 kB)\n",
      "Step #0:       196.8/196.8 kB 33.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting idna<4,>=2.5\n",
      "Step #0:   Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "Step #0:       61.5/61.5 kB 11.7 MB/s eta 0:00:00\n",
      "Step #0: Requirement already satisfied: wheel in /usr/local/lib/python3.11/site-packages (from strip-hints<1,>=0.1.8->kfp==2.0.0b9->-r requirements.txt (line 1)) (0.38.4)\n",
      "Step #0: Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "Step #0:   Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
      "Step #0:   Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
      "Step #0:   Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
      "Step #0: Collecting oauthlib>=3.0.0\n",
      "Step #0:   Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Step #0:       151.7/151.7 kB 26.8 MB/s eta 0:00:00\n",
      "Step #0: Building wheels for collected packages: kfp, fire, kfp-server-api, PyYAML, strip-hints, wrapt\n",
      "Step #0:   Building wheel for kfp (setup.py): started\n",
      "Step #0:   Building wheel for kfp (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for kfp: filename=kfp-2.0.0b9-py3-none-any.whl size=499551 sha256=50d068a4c09fea94b9920ae84106ddbeaf259b50be5099efc35214ffa01c7aa1\n",
      "Step #0:   Stored in directory: /builder/home/.cache/pip/wheels/56/3b/6a/1dd1419372a030307d98e3bfcafb2e18d41863fd3be9da0fca\n",
      "Step #0:   Building wheel for fire (setup.py): started\n",
      "Step #0:   Building wheel for fire (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=e03901d93157920e196a60bf9b50a669471981138dc86514e08cf3aed3233684\n",
      "Step #0:   Stored in directory: /builder/home/.cache/pip/wheels/9a/9a/bd/afea12fa537e804618c037066dd825c8688b08d50e26077d6f\n",
      "Step #0:   Building wheel for kfp-server-api (setup.py): started\n",
      "Step #0:   Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for kfp-server-api: filename=kfp_server_api-2.0.0a6-py3-none-any.whl size=104199 sha256=57a146aabe7e1da4976e7dddfb697b41ad923452a309adf14806163a6d1233f4\n",
      "Step #0:   Stored in directory: /builder/home/.cache/pip/wheels/73/8b/1e/2ebf3b125b030ca9e5d5d8b56fa51e5cb7b61bdef7dde681de\n",
      "Step #0:   Building wheel for PyYAML (pyproject.toml): started\n",
      "Step #0:   Building wheel for PyYAML (pyproject.toml): finished with status 'done'\n",
      "Step #0:   Created wheel for PyYAML: filename=PyYAML-5.4.1-cp311-cp311-linux_x86_64.whl size=611395 sha256=b9539d6e348e4f91d0b4122e2b1f4f5accc00c8738bc35e62ddf41b70ae275d6\n",
      "Step #0:   Stored in directory: /builder/home/.cache/pip/wheels/13/d5/5b/082ef0599cd0dde3d1f273ceebebe6cda67201058a70ade961\n",
      "Step #0:   Building wheel for strip-hints (setup.py): started\n",
      "Step #0:   Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22283 sha256=234a9cca6a7c8ecaf7129da7fb0d1be4c050b4fa83a026490747df2a4b369e1a\n",
      "Step #0:   Stored in directory: /builder/home/.cache/pip/wheels/6a/39/57/0dfdaf032e70cf10c29edc72355217a6fd1b1544d979eafd4b\n",
      "Step #0:   Building wheel for wrapt (setup.py): started\n",
      "Step #0:   Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for wrapt: filename=wrapt-1.14.1-cp311-cp311-linux_x86_64.whl size=77722 sha256=9e94b174fd6501776414505c4459a640b062a49c2a0c3c344dce5f82a9f63b21\n",
      "Step #0:   Stored in directory: /builder/home/.cache/pip/wheels/6c/5f/ff/a07f5f99c11c1480c6d2d1d6e0adadd8c1d05bdd668173bc42\n",
      "Step #0: Successfully built kfp fire kfp-server-api PyYAML strip-hints wrapt\n",
      "Step #0: Installing collected packages: pytz, pyasn1, charset-normalizer, wrapt, websocket-client, urllib3, uritemplate, termcolor, tabulate, strip-hints, six, rsa, PyYAML, pyrsistent, pyparsing, pyasn1-modules, protobuf, pluggy, oauthlib, iniconfig, idna, grpcio, google-crc32c, docstring-parser, cloudpickle, click, certifi, cachetools, attrs, absl-py, typer, requests, python-dateutil, proto-plus, packaging, kfp-pipeline-spec, jsonschema, googleapis-common-protos, google-resumable-media, google-auth, fire, Deprecated, requests-toolbelt, requests-oauthlib, pytest, kfp-server-api, grpcio-status, google-api-core, kubernetes, grpc-google-iam-v1, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, kfp, google-cloud-aiplatform\n",
      "Step #0:   WARNING: The script normalizer is installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The script wsdump is installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The script tabulate is installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The script strip-hints is installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The script jsonschema is installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The scripts py.test and pytest are installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The scripts dsl-compile, dsl-compile-deprecated and kfp are installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The script tb-gcp-uploader is installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0: Successfully installed Deprecated-1.2.13 PyYAML-5.4.1 absl-py-1.4.0 attrs-22.2.0 cachetools-4.2.4 certifi-2022.12.7 charset-normalizer-3.0.1 click-8.1.3 cloudpickle-2.2.0 docstring-parser-0.15 fire-0.5.0 google-api-core-2.10.2 google-auth-1.35.0 google-cloud-aiplatform-1.20.0 google-cloud-bigquery-1.20.0 google-cloud-core-1.7.3 google-cloud-resource-manager-1.6.3 google-cloud-storage-2.2.1 google-crc32c-1.5.0 google-resumable-media-2.4.0 googleapis-common-protos-1.58.0 grpc-google-iam-v1-0.12.6 grpcio-1.51.1 grpcio-status-1.48.2 idna-3.4 iniconfig-2.0.0 jsonschema-3.2.0 kfp-2.0.0b9 kfp-pipeline-spec-0.1.17 kfp-server-api-2.0.0a6 kubernetes-23.6.0 oauthlib-3.2.2 packaging-21.3 pluggy-1.0.0 proto-plus-1.22.2 protobuf-3.20.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyparsing-3.0.9 pyrsistent-0.19.3 pytest-7.2.0 python-dateutil-2.8.2 pytz-2022.7 requests-2.28.2 requests-oauthlib-1.3.1 requests-toolbelt-0.10.1 rsa-4.9 six-1.16.0 strip-hints-0.1.10 tabulate-0.9.0 termcolor-2.2.0 typer-0.7.0 uritemplate-3.0.1 urllib3-1.26.14 websocket-client-1.4.2 wrapt-1.14.1\n",
      "Step #0: WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Finished Step #0\n",
      "Starting Step #1 - \"compile\"\n",
      "Step #1 - \"compile\": Already have image: python\n",
      "Finished Step #1 - \"compile\"\n",
      "Starting Step #2 - \"test_pipeline\"\n",
      "Step #2 - \"test_pipeline\": Already have image: python\n",
      "Step #2 - \"test_pipeline\": .\n",
      "Step #2 - \"test_pipeline\": ----------------------------------------------------------------------\n",
      "Step #2 - \"test_pipeline\": Ran 1 test in 0.000s\n",
      "Step #2 - \"test_pipeline\": \n",
      "Step #2 - \"test_pipeline\": OK\n",
      "Finished Step #2 - \"test_pipeline\"\n",
      "Starting Step #3 - \"upload\"\n",
      "Step #3 - \"upload\": Already have image (with digest): gcr.io/cloud-builders/gsutil\n",
      "Step #3 - \"upload\": Copying file://pipeline.yaml [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 37.6 KiB/ 37.6 KiB]                                                                    \n",
      "Step #3 - \"upload\": Operation completed over 1 objects/37.6 KiB.\n",
      "Finished Step #3 - \"upload\"\n",
      "Starting Step #4 - \"test\"\n",
      "Step #4 - \"test\": Already have image: python\n",
      "Step #4 - \"test\": Submitting pipeline mlops-dummy-pipeline in experiment qa-mlops-dummy-experiment.\n",
      "Step #4 - \"test\": Creating PipelineJob\n",
      "Step #4 - \"test\": PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/taxi-tips-training-20230117121209\n",
      "Step #4 - \"test\": To use this PipelineJob in another session:\n",
      "Step #4 - \"test\": pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/taxi-tips-training-20230117121209')\n",
      "Step #4 - \"test\": View Pipeline Job:\n",
      "Step #4 - \"test\": https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/taxi-tips-training-20230117121209?project=370018035372\n",
      "Step #4 - \"test\": Associating projects/370018035372/locations/us-central1/pipelineJobs/taxi-tips-training-20230117121209 to Experiment: qa-mlops-dummy-experiment\n",
      "Step #4 - \"test\": PipelineJob projects/370018035372/locations/us-central1/pipelineJobs/taxi-tips-training-20230117121209 current state:\n",
      "Step #4 - \"test\": 3\n",
      "Step #4 - \"test\": PipelineJob run completed. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/taxi-tips-training-20230117121209\n",
      "Finished Step #4 - \"test\"\n",
      "Starting Step #5 - \"template\"\n",
      "Step #5 - \"template\": Already have image: python\n",
      "Finished Step #5 - \"template\"\n",
      "Starting Step #6 - \"prod\"\n",
      "Step #6 - \"prod\": Already have image: python\n",
      "Step #6 - \"prod\": Submitting pipeline mlops-dummy-pipeline in experiment prod-mlops-dummy-experiment.\n",
      "Step #6 - \"prod\": Creating PipelineJob\n",
      "Step #6 - \"prod\": PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/taxi-tips-training-20230117121232\n",
      "Step #6 - \"prod\": To use this PipelineJob in another session:\n",
      "Step #6 - \"prod\": pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/taxi-tips-training-20230117121232')\n",
      "Step #6 - \"prod\": View Pipeline Job:\n",
      "Step #6 - \"prod\": https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/taxi-tips-training-20230117121232?project=370018035372\n",
      "Step #6 - \"prod\": Associating projects/370018035372/locations/us-central1/pipelineJobs/taxi-tips-training-20230117121232 to Experiment: prod-mlops-dummy-experiment\n",
      "Finished Step #6 - \"prod\"\n",
      "PUSH\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                               IMAGES  STATUS\n",
      "5cab7ada-9289-4caf-88c0-61668dbc92ea  2023-01-17T12:11:07+00:00  1M30S     gs://vertex-ai-test-365213_cloudbuild/source/1673957466.801475-f5a9588be6a64b0c9f6102b09ce96470.tgz  -       SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit ./src --config=src/cloudbuild.yaml --substitutions=_BUCKET_NAME=$BUCKET_NAME,_EXPERIMENT_NAME=$EXPERIMENT_NAME,_PIPELINE_NAME=$PIPELINE_NAME,_REGION=$REGION,_ENDPOINT_NAME=$ENDPOINT_NAME"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m96",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m96"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
