{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2d4e1a8-7e09-4128-94db-dded48bef1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https:/www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a66b6f",
   "metadata": {},
   "source": [
    "# Automated MLOps pipeline build, testing and deployment\n",
    "\n",
    "In the previous notebook, you created a machine learning pipeline to train a model. In this session, it's all about automating the training and deployment of this model. Hence, the objective this week is to:\n",
    "\n",
    "1. Write a pipeline into a Python file that can be compiled into YAML in an automated fashion.\n",
    "2. Define some dummy unit tests\n",
    "3. Write a script to deploy a compiled Kubeflow pipeline to Vertex AI.\n",
    "4. Use Code Build (CI/CD) to compile, test, and run your Kubeflow pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33a133e3-9336-429a-8bd2-9df3b7e9e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-mlops\" \n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Experiments\n",
    "EXPERIMENT_NAME = \"test-experiment\"\n",
    "PIPELINE_NAME = \"mlops-pipeline-prod\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a84feaa",
   "metadata": {},
   "source": [
    "### Create a script containing your Vertex AI/Kubeflow Pipeline to compile the pipeline into `pipeline.json`\n",
    "\n",
    "> <font color='green'>**Task 1**</font>\n",
    ">\n",
    "> Create a Python script `src/pipeline.py` that creates a file name `pipeline.json` from the Kubeflow pipeline you developed last week. The output file should be in YAML and not JSON format.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a6430d80-a7bd-4de5-852d-60fded9350ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85aeb7c3-590f-46cb-88bf-8fca8f315962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/requirements.txt\n",
    "kfp==1.8.18\n",
    "pytest==7.2.0\n",
    "pytz==2022.7\n",
    "google-cloud-aiplatform==1.20.0\n",
    "google-api-core==2.10.2\n",
    "google-auth==1.35.0\n",
    "google-cloud-bigquery==1.20.0\n",
    "google-cloud-core==1.7.3\n",
    "google-cloud-resource-manager==1.6.3\n",
    "google-cloud-storage==2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "76d71ce2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/pipeline.py\n",
    "\n",
    "import argparse\n",
    "\n",
    "import kfp.v2.compiler as compiler\n",
    "import kfp.v2.dsl as dsl\n",
    "\n",
    "\n",
    "@dsl.component(packages_to_install=[\"scikit-learn\", \"pandas\", \"joblib\"])\n",
    "def model_training_op(\n",
    "        dataset: dsl.Input[dsl.Dataset],\n",
    "        model: dsl.Output[dsl.Model]\n",
    "):\n",
    "    import glob\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    TARGET_COLUMN = \"tip_bin\"\n",
    "    TARGET_LABELS = [\"tip<20%\", \"tip>=20%\"]\n",
    "\n",
    "    def sanitize(path: str) -> str:\n",
    "        return path.replace(\"gs://\", \"/gcs/\", 1) if path and path.startswith(\"gs://\") else path\n",
    "\n",
    "    def get_dataframe(path: str):\n",
    "        if os.path.isdir(path):  # base data directory is passed\n",
    "            files = glob.glob(f\"{path}/*.csv\")\n",
    "        elif \"*\" in path:  # a glob expression is passed\n",
    "            files = glob.glob(path)\n",
    "        else:  # single file is passed\n",
    "            files = [path]\n",
    "        dfs = (pd.read_csv(f, header=0) for f in files)\n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    def create_datasets(training_data_dir: str, validation_data_dir: str):\n",
    "        \"\"\"Creates training and validation datasets.\"\"\"\n",
    "\n",
    "        train_dataset = get_dataframe(training_data_dir)\n",
    "\n",
    "        if validation_data_dir:\n",
    "            return train_dataset, get_dataframe(validation_data_dir)\n",
    "        else:\n",
    "            return train_test_split(train_dataset, test_size=.25, random_state=42)\n",
    "\n",
    "    def log_metrics(y_pred: pd.Series, y_true: pd.Series, output_dir: str):\n",
    "        curve = roc_curve(y_score=y_pred, y_true=y_true)\n",
    "        auc = roc_auc_score(y_score=y_pred, y_true=y_true)\n",
    "        cm = confusion_matrix(labels=[False, True], y_pred=y_pred, y_true=y_true)\n",
    "\n",
    "        with open(f\"{output_dir}/metrics.json\", \"w\") as f:\n",
    "            metrics = {\"auc\": auc}\n",
    "            metrics[\"confusion_matrix\"] = {}\n",
    "            metrics[\"confusion_matrix\"][\"categories\"] = TARGET_LABELS\n",
    "            metrics[\"confusion_matrix\"][\"matrix\"] = cm.tolist()\n",
    "            metrics[\"roc_curve\"] = {}\n",
    "            metrics[\"roc_curve\"][\"fpr\"] = curve[0].tolist()\n",
    "            metrics[\"roc_curve\"][\"tpr\"] = curve[1].tolist()\n",
    "            metrics[\"roc_curve\"][\"thresholds\"] = curve[2].tolist()\n",
    "            json.dump(metrics, f, indent=2)\n",
    "\n",
    "    def split(df: pd.DataFrame):\n",
    "        return df.drop(TARGET_COLUMN, axis=1), df[TARGET_COLUMN]\n",
    "\n",
    "    def train(training_data_dir: str, validation_data_dir: str, output_dir: str):\n",
    "        train_df, val_df = create_datasets(training_data_dir, validation_data_dir)\n",
    "\n",
    "        X_train, y_train = split(train_df)\n",
    "        X_test, y_test = split(val_df)\n",
    "\n",
    "        model = RandomForestClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        joblib.dump(model, f\"{output_dir}/model.joblib\")\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        log_metrics(y_pred, y_test, output_dir)\n",
    "\n",
    "        return model.score(X_test, y_test)\n",
    "\n",
    "    train(f\"{dataset.path}/train\", f\"{dataset.path}/val\", f\"{model.path}\")\n",
    "\n",
    "\n",
    "@dsl.component()\n",
    "def data_validation_op(dataset: dsl.Input[dsl.Dataset]) -> str:\n",
    "    return \"valid\"\n",
    "\n",
    "\n",
    "@dsl.component()\n",
    "def data_preparation_op():\n",
    "    pass\n",
    "\n",
    "\n",
    "@dsl.component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def model_validation_op(\n",
    "        metrics: dsl.Input[dsl.ClassificationMetrics],\n",
    "        threshold_auc: float = 0.50\n",
    ") -> str:\n",
    "    return \"valid\" if metrics.metadata[\"auc\"] > threshold_auc else 'not_valid'\n",
    "\n",
    "\n",
    "@dsl.component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def model_upload_op(\n",
    "        model: dsl.Input[dsl.Model],\n",
    "        serving_container_image_uri: str,\n",
    "        project_id: str,\n",
    "        location: str,\n",
    "        model_name: str) -> str:\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(project=project_id, location=location)\n",
    "    matches = aiplatform.Model.list(filter=f\"display_name={model_name}\")\n",
    "    parent_model = matches[0].resource_name if matches else None\n",
    "\n",
    "    registered_model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        parent_model=parent_model,\n",
    "        artifact_uri=model.uri,\n",
    "        serving_container_image_uri=serving_container_image_uri\n",
    "    )\n",
    "\n",
    "    return registered_model.versioned_resource_name\n",
    "\n",
    "\n",
    "@dsl.component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def model_evaluation_upload_op(\n",
    "        metrics: dsl.Input[dsl.ClassificationMetrics],\n",
    "        model_resource_name: str,\n",
    "        project_id: str,\n",
    "        location: str):\n",
    "    from google.api_core import gapic_v1\n",
    "    from google.cloud import aiplatform\n",
    "    from google.protobuf.struct_pb2 import Struct\n",
    "    from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "    model_evaluation = {\n",
    "        \"display_name\": \"pipeline-eval\",\n",
    "        \"metrics\": Value(struct_value=Struct(fields={\"auRoc\": Value(number_value=metrics.metadata[\"auc\"])})),\n",
    "        \"metrics_schema_uri\": \"gs://google-cloud-aiplatform/schema/modelevaluation/classification_metrics_1.0.0.yaml\"\n",
    "    }\n",
    "\n",
    "    aiplatform.init(project=project_id, location=location)\n",
    "    api_endpoint = location + '-aiplatform.googleapis.com'\n",
    "    client = aiplatform.gapic.ModelServiceClient(client_info=gapic_v1.client_info.ClientInfo(\n",
    "        user_agent=\"google-cloud-pipeline-components\"),\n",
    "        client_options={\n",
    "            \"api_endpoint\": api_endpoint,\n",
    "        })\n",
    "    client.import_model_evaluation(parent=model_resource_name, model_evaluation=model_evaluation)\n",
    "\n",
    "\n",
    "@dsl.component()\n",
    "def model_evaluation_op(model: dsl.Input[dsl.Model], metrics: dsl.Output[dsl.ClassificationMetrics]):\n",
    "    import json\n",
    "\n",
    "    with open(f\"{model.path}/metrics.json\", \"r\") as f:\n",
    "        model_metrics = json.load(f)\n",
    "\n",
    "    conf_matrix = model_metrics[\"confusion_matrix\"]\n",
    "    metrics.log_confusion_matrix(categories=conf_matrix[\"categories\"], matrix=conf_matrix[\"matrix\"])\n",
    "\n",
    "    curve = model_metrics[\"roc_curve\"]\n",
    "    metrics.log_roc_curve(fpr=curve[\"fpr\"], tpr=curve[\"tpr\"], threshold=curve[\"thresholds\"])\n",
    "\n",
    "    metrics.metadata[\"auc\"] = model_metrics[\"auc\"]\n",
    "\n",
    "\n",
    "@dsl.component(packages_to_install=[\"google-cloud-bigquery\"])\n",
    "def data_extract_op(project_id: str, location: str, dataset: dsl.Output[dsl.Dataset]):\n",
    "    import os\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"\n",
    "    EXPORT DATA OPTIONS(\n",
    "        uri='{path}/*.csv',\n",
    "        format='CSV',\n",
    "        overwrite=true,\n",
    "        header=true,\n",
    "        field_delimiter=',') AS\n",
    "    SELECT\n",
    "        EXTRACT(MONTH from pickup_datetime) as trip_month,\n",
    "        EXTRACT(DAY from pickup_datetime) as trip_day,\n",
    "        EXTRACT(DAYOFWEEK from pickup_datetime) as trip_day_of_week,\n",
    "        EXTRACT(HOUR from pickup_datetime) as trip_hour,\n",
    "        TIMESTAMP_DIFF(dropoff_datetime, pickup_datetime, SECOND) as trip_duration,\n",
    "        trip_distance,\n",
    "        payment_type,\n",
    "        pickup_location_id as pickup_zone,\n",
    "        pickup_location_id as dropoff_zone,\n",
    "        IF((SAFE_DIVIDE(tip_amount, fare_amount) >= 0.2), 1, 0) AS tip_bin\n",
    "    FROM\n",
    "        `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_{year}` TABLESAMPLE SYSTEM (1 PERCENT)\n",
    "    WHERE\n",
    "        TIMESTAMP_DIFF(dropoff_datetime, pickup_datetime, SECOND) BETWEEN 300 AND 10800\n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "    datasets = [\n",
    "        (f\"{dataset.path}/train\", 2020, 10000),\n",
    "        (f\"{dataset.path}/val\", 2020, 5000),\n",
    "        (f\"{dataset.path}/test\", 2020, 1000)\n",
    "    ]\n",
    "    for ds in datasets:\n",
    "        path = ds[0].replace(\"/gcs/\", \"gs://\", 1)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        # ignoring the provided location as this dataset is in US\n",
    "        job = client.query(query.format(path=path, year=ds[1], limit=ds[2]), project=project_id, location=\"us\")\n",
    "        job.result()\n",
    "\n",
    "\n",
    "\n",
    "@dsl.pipeline(name=\"taxi-tips-training\")\n",
    "def training_pipeline(\n",
    "        project_id: str, location: str):\n",
    "    model_name = \"taxi-tips\"\n",
    "\n",
    "    data_extraction_task = data_extract_op(\n",
    "        project_id=project_id, location=location\n",
    "    ).set_display_name(\"extract-data\")\n",
    "\n",
    "    data_validation_task = data_validation_op(\n",
    "        dataset=data_extraction_task.outputs[\"dataset\"]\n",
    "    ).set_display_name(\"validate-data\")\n",
    "\n",
    "    data_preparation_task = data_preparation_op().set_display_name(\"prepare-data\")\n",
    "    data_preparation_task.after(data_validation_task)\n",
    "\n",
    "    training_task = model_training_op(\n",
    "        dataset=data_extraction_task.outputs[\"dataset\"],\n",
    "    ).set_display_name(\"train-model\")\n",
    "    training_task.after(data_preparation_task)\n",
    "\n",
    "    model_evaluation_task = model_evaluation_op(\n",
    "        model=training_task.outputs[\"model\"]\n",
    "    ).set_display_name(\"evaluate-model\")\n",
    "\n",
    "    model_validation_task = model_validation_op(\n",
    "        metrics=model_evaluation_task.outputs[\"metrics\"],\n",
    "    ).set_display_name(\"validate-model\")\n",
    "\n",
    "    with dsl.Condition(model_validation_task.output == \"valid\", name=\"check-performance\"):\n",
    "        model_upload_task = model_upload_op(\n",
    "            model=training_task.outputs[\"model\"],\n",
    "            model_name=model_name,\n",
    "            serving_container_image_uri=\"europe-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest\",\n",
    "            project_id=project_id,\n",
    "            location=location\n",
    "        ).set_display_name(\"register-model\")\n",
    "\n",
    "        model_evaluation_upload_task = model_evaluation_upload_op(\n",
    "            metrics=model_evaluation_task.outputs[\"metrics\"],\n",
    "            model_resource_name=model_upload_task.output,\n",
    "            project_id=project_id,\n",
    "            location=location\n",
    "        ).set_display_name(\"register-model-evaluation\")\n",
    "\n",
    "def compile(filename: str):\n",
    "    cmp = compiler.Compiler()\n",
    "    cmp.compile(pipeline_func=training_pipeline, package_path=filename)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--pipeline-file-name\", type=str, default=\"pipeline.json\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    compile(args.pipeline_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d4148",
   "metadata": {},
   "source": [
    "Using the next command, you can test the materialized pipeline generated by your script. You can view the output in a file named `pipeline.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "54c94289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "!python src/pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "66bb273c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"pipelineSpec\": {\n",
      "    \"components\": {\n",
      "      \"comp-condition-check-performance-1\": {\n",
      "        \"dag\": {\n",
      "          \"tasks\": {\n",
      "            \"model-evaluation-upload-op\": {\n",
      "              \"cachingOptions\": {\n",
      "                \"enableCache\": true\n",
      "              },\n",
      "              \"componentRef\": {\n",
      "                \"name\": \"comp-model-evaluation-upload-op\"\n",
      "              },\n",
      "              \"dependentTasks\": [\n",
      "                \"model-upload-op\"\n",
      "              ],\n",
      "              \"inputs\": {\n",
      "                \"artifacts\": {\n",
      "                  \"metrics\": {\n",
      "                    \"componentInputArtifact\": \"pipelineparam--model-evaluation-op-metrics\"\n"
     ]
    }
   ],
   "source": [
    "!head -n20 pipeline.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca25b67-c432-4d42-8a10-2b9b94e7a980",
   "metadata": {},
   "source": [
    "### Test the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef73fcf-6a9c-49da-bb65-7f724a412c5e",
   "metadata": {},
   "source": [
    "> <font color='green'>**Task 2**</font>\n",
    "> Write unit/integration tests for the pipeline you created to ensure the component logic that you added works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "88e162c6-1cbb-456c-8b5c-d513fbe34064",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p src/tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8186aaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/tests/test_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/tests/test_pipeline.py\n",
    "\n",
    "import unittest\n",
    "from pipeline import training_pipeline\n",
    "\n",
    "class TestBasicPipeline(unittest.TestCase):\n",
    "    \n",
    "    def test_pipeline(self):\n",
    "        pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a7b41-9996-42c1-8849-70aef63273f9",
   "metadata": {},
   "source": [
    "Using the next command, you can run the tests in the script using python `unittest` test runner. It discovers all the test files that start with `test_*`\n",
    "\n",
    "You can also use other testing framework of your choice (e.g. `pytest`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "191cfe48-0206-477c-a894-7c4ca66d6eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=src python -m unittest discover -s src/tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a576513",
   "metadata": {},
   "source": [
    "### Create a script to submit your compile kubeflow pipeline (`pipeline.json`) to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20c708bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/submit-pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/submit-pipeline.py\n",
    "import os\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "import google.auth\n",
    "\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "if not PROJECT_ID:\n",
    "    creds, PROJECT_ID = google.auth.default()\n",
    "\n",
    "REGION = os.environ[\"REGION\"]\n",
    "BUCKET_NAME = os.environ[\"BUCKET_NAME\"]\n",
    "PIPELINE_NAME = os.environ[\"PIPELINE_NAME\"]\n",
    "EXPERIMENT_NAME = os.environ.get(\"EXPERIMENT_NAME\", \"dummy-experiment\")\n",
    "ENDPOINT_NAME = os.environ.get(\"ENDPOINT_NAME\",\"dummy-endpoint\")\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "sync_pipeline = os.getenv(\"SUBMIT_PIPELINE_SYNC\", 'False').lower() in ('true', '1', 't')\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path='pipeline.json',\n",
    "    location=REGION,\n",
    "    project=PROJECT_ID,\n",
    "    enable_caching=True,\n",
    "    pipeline_root=f'gs://{BUCKET_NAME}',\n",
    "    parameter_values={'project_id':PROJECT_ID, 'location':REGION}\n",
    ")\n",
    "print(f\"Submitting pipeline {PIPELINE_NAME} in experiment {EXPERIMENT_NAME}.\")\n",
    "job.submit(experiment=EXPERIMENT_NAME)\n",
    "\n",
    "if sync_pipeline:\n",
    "    job.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3de2c1",
   "metadata": {},
   "source": [
    "Let's test this script in the Notebook. You can check the pipeline's status by clicking on the link printed by the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7d630863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: REGION=us-central1\n",
      "env: BUCKET_NAME=qinetiq-workshop23lon-5240-mlops\n",
      "env: EXPERIMENT_NAME=test-experiment\n",
      "env: PIPELINE_NAME=mlops-pipeline-prod\n",
      "env: ENDPOINT_NAME=$ENDPOINT_NAME\n",
      "env: SUBMIT_PIPELINE_SYNC=1\n",
      "Submitting pipeline mlops-pipeline-prod in experiment test-experiment.\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/450304494793/locations/us-central1/pipelineJobs/taxi-tips-training-20230122184911\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/450304494793/locations/us-central1/pipelineJobs/taxi-tips-training-20230122184911')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/taxi-tips-training-20230122184911?project=450304494793\n",
      "Associating projects/450304494793/locations/us-central1/pipelineJobs/taxi-tips-training-20230122184911 to Experiment: test-experiment\n",
      "PipelineJob projects/450304494793/locations/us-central1/pipelineJobs/taxi-tips-training-20230122184911 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/450304494793/locations/us-central1/pipelineJobs/taxi-tips-training-20230122184911\n"
     ]
    }
   ],
   "source": [
    "%set_env REGION=$REGION\n",
    "%set_env BUCKET_NAME=$BUCKET_NAME\n",
    "%set_env EXPERIMENT_NAME=$EXPERIMENT_NAME\n",
    "%set_env PIPELINE_NAME=$PIPELINE_NAME\n",
    "%set_env ENDPOINT_NAME=$ENDPOINT_NAME\n",
    "%set_env SUBMIT_PIPELINE_SYNC=1\n",
    "\n",
    "!python src/submit-pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82cf0be",
   "metadata": {},
   "source": [
    "### Automate Kubeflow pipeline compilation, template generation, and execution through Cloud Build\n",
    "\n",
    "Cloud Build is a service that executes your builds on Google Cloud. In this exercise, we want to use it to both compile and run your machine learning pipeline. For more information, please refer to the [Cloud Build documentation](https://cloud.google.com/build/docs/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "243972cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/cloudbuild.yaml\n",
    "steps:\n",
    "  # Install dependencies\n",
    "  - name: 'python'\n",
    "    entrypoint: 'pip'\n",
    "    args: [\"install\", \"-r\", \"requirements.txt\", \"--user\"]\n",
    "\n",
    "  # Compile pipeline\n",
    "  - name: 'python'\n",
    "    entrypoint: 'python'\n",
    "    args: ['pipeline.py']\n",
    "    id: 'compile'\n",
    "\n",
    "  # Test the Pipeline Components \n",
    "  - name: 'python'\n",
    "    entrypoint: 'python'\n",
    "    args: ['-m', 'unittest', 'discover', 'tests/']\n",
    "    id: 'test_pipeline'\n",
    "    waitFor: ['compile']\n",
    "\n",
    "  # Upload compiled pipeline to GCS\n",
    "  - name: 'gcr.io/cloud-builders/gsutil'\n",
    "    args: ['cp', 'pipeline.json', 'gs://${_BUCKET_NAME}']\n",
    "    id: 'upload'\n",
    "    waitFor: ['test_pipeline']\n",
    "        \n",
    "  # Run the Vertex AI Pipeline (synchronously for test/qa environment).\n",
    "  - name: 'python'\n",
    "    id: 'test'\n",
    "    entrypoint: 'python'\n",
    "    env: ['BUCKET_NAME=${_BUCKET_NAME}', 'EXPERIMENT_NAME=qa-${_EXPERIMENT_NAME}', 'PIPELINE_NAME=${_PIPELINE_NAME}',\n",
    "          'REGION=${_REGION}', 'ENDPOINT_NAME=qa-${_ENDPOINT_NAME}', 'SUBMIT_PIPELINE_SYNC=true']\n",
    "    args: ['submit-pipeline.py']\n",
    "    \n",
    "  # Run the Vertex AI Pipeline (asynchronously for prod environment). In a real production scenario, this would run in a different GCP project.\n",
    "  - name: 'python'\n",
    "    id: 'prod'\n",
    "    entrypoint: 'python'\n",
    "    env: ['BUCKET_NAME=${_BUCKET_NAME}', 'EXPERIMENT_NAME=prod-${_EXPERIMENT_NAME}', 'PIPELINE_NAME=${_PIPELINE_NAME}',\n",
    "          'REGION=${_REGION}', 'ENDPOINT_NAME=prod-${_ENDPOINT_NAME}', 'SUBMIT_PIPELINE_SYNC=false']\n",
    "    args: ['submit-pipeline.py']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4958d76",
   "metadata": {},
   "source": [
    "Cloud Build uses a special service account to execute builds on your behalf. When you enable the Cloud Build API on a Google Cloud project, the Cloud Build service account is automatically created and granted the Cloud Build Service Account role for the project. This role gives the service account permissions to perform several tasks, however you can grant more permissions to the service account to perform additional tasks. [This page](https://cloud.google.com/build/docs/securing-builds/configure-access-for-cloud-build-service-account) explains how to grant and revoke permissions to the Cloud Build service account.\n",
    "\n",
    "For Cloud Build to be able to deploy your pipeline, you need to give its' service account `{PROJECT_NUMBER}@cloudbuild.gserviceaccount.com` the **Vertex AI User** and **Service Account User** role. (Step already done as part of the workshop setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4d136-3976-40b8-b3e9-626a40e6d3f1",
   "metadata": {},
   "source": [
    "Now you are ready to trigger this pipeline. This CICD pipeline can be embedded in your repository and triggered when a file changes with any\n",
    "git provider, see more about triggering [here](https://cloud.google.com/build/docs/automating-builds/create-manage-triggers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d6d3ea3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 7 file(s) totalling 27.0 KiB before compression.\n",
      "Uploading tarball of [./src] to [gs://qinetiq-workshop23lon-5240_cloudbuild/source/1674412711.713397-699cbb6eff4b4cb39f6b826850350ccc.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qinetiq-workshop23lon-5240/locations/global/builds/aa69153a-7355-4855-98f6-0396eb6ba97e].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/aa69153a-7355-4855-98f6-0396eb6ba97e?project=450304494793 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"aa69153a-7355-4855-98f6-0396eb6ba97e\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qinetiq-workshop23lon-5240_cloudbuild/source/1674412711.713397-699cbb6eff4b4cb39f6b826850350ccc.tgz#1674412711972244\n",
      "Copying gs://qinetiq-workshop23lon-5240_cloudbuild/source/1674412711.713397-699cbb6eff4b4cb39f6b826850350ccc.tgz#1674412711972244...\n",
      "/ [1 files][ 10.1 KiB/ 10.1 KiB]                                                \n",
      "Operation completed over 1 objects/10.1 KiB.\n",
      "BUILD\n",
      "Starting Step #0\n",
      "Step #0: Pulling image: python\n",
      "Step #0: Using default tag: latest\n",
      "Step #0: latest: Pulling from library/python\n",
      "Step #0: bbeef03cda1f: Already exists\n",
      "Step #0: f049f75f014e: Already exists\n",
      "Step #0: 56261d0e6b05: Already exists\n",
      "Step #0: 9bd150679dbd: Already exists\n",
      "Step #0: 5b282ee9da04: Already exists\n",
      "Step #0: 03f027d5e312: Already exists\n",
      "Step #0: db6ee1ace097: Pulling fs layer\n",
      "Step #0: 0a86d528f1ea: Pulling fs layer\n",
      "Step #0: 4cfb032ae58b: Pulling fs layer\n",
      "Step #0: 0a86d528f1ea: Verifying Checksum\n",
      "Step #0: 0a86d528f1ea: Download complete\n",
      "Step #0: 4cfb032ae58b: Verifying Checksum\n",
      "Step #0: 4cfb032ae58b: Download complete\n",
      "Step #0: db6ee1ace097: Verifying Checksum\n",
      "Step #0: db6ee1ace097: Download complete\n",
      "Step #0: db6ee1ace097: Pull complete\n",
      "Step #0: 0a86d528f1ea: Pull complete\n",
      "Step #0: 4cfb032ae58b: Pull complete\n",
      "Step #0: Digest: sha256:a3c0c6766535f85f18e7304d3a0111de5208d73935bcf1b024217005ad5ce195\n",
      "Step #0: Status: Downloaded newer image for python:latest\n",
      "Step #0: docker.io/library/python:latest\n",
      "Step #0: Collecting kfp==1.8.18\n",
      "Step #0:   Downloading kfp-1.8.18.tar.gz (304 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 304.8/304.8 kB 7.1 MB/s eta 0:00:00\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting pytest==7.2.0\n",
      "Step #0:   Downloading pytest-7.2.0-py3-none-any.whl (316 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 316.8/316.8 kB 30.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting pytz==2022.7\n",
      "Step #0:   Downloading pytz-2022.7-py2.py3-none-any.whl (499 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 499.4/499.4 kB 51.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-aiplatform==1.20.0\n",
      "Step #0:   Downloading google_cloud_aiplatform-1.20.0-py2.py3-none-any.whl (2.3 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 81.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-api-core==2.10.2\n",
      "Step #0:   Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.6/115.6 kB 23.0 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-auth==1.35.0\n",
      "Step #0:   Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152.9/152.9 kB 24.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-bigquery==1.20.0\n",
      "Step #0:   Downloading google_cloud_bigquery-1.20.0-py2.py3-none-any.whl (154 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.0/155.0 kB 26.6 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-core==1.7.3\n",
      "Step #0:   Downloading google_cloud_core-1.7.3-py2.py3-none-any.whl (28 kB)\n",
      "Step #0: Collecting google-cloud-resource-manager==1.6.3\n",
      "Step #0:   Downloading google_cloud_resource_manager-1.6.3-py2.py3-none-any.whl (233 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.8/233.8 kB 34.9 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-storage==2.2.1\n",
      "Step #0:   Downloading google_cloud_storage-2.2.1-py2.py3-none-any.whl (107 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107.1/107.1 kB 18.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting absl-py<2,>=0.9\n",
      "Step #0:   Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 24.6 MB/s eta 0:00:00\n",
      "Step #0: Collecting PyYAML<6,>=5.3\n",
      "Step #0:   Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 175.1/175.1 kB 28.1 MB/s eta 0:00:00\n",
      "Step #0:   Installing build dependencies: started\n",
      "Step #0:   Installing build dependencies: finished with status 'done'\n",
      "Step #0:   Getting requirements to build wheel: started\n",
      "Step #0:   Getting requirements to build wheel: finished with status 'done'\n",
      "Step #0:   Preparing metadata (pyproject.toml): started\n",
      "Step #0:   Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Step #0: Collecting kubernetes<20,>=8.0.0\n",
      "Step #0:   Downloading kubernetes-19.15.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 94.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-api-python-client<2,>=1.7.8\n",
      "Step #0:   Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.1/62.1 kB 11.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting requests-toolbelt<1,>=0.8.0\n",
      "Step #0:   Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 kB 10.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting cloudpickle<3,>=2.0.0\n",
      "Step #0:   Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Step #0: Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "Step #0:   Downloading kfp-server-api-1.8.5.tar.gz (58 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.1/58.1 kB 10.6 MB/s eta 0:00:00\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting jsonschema<4,>=3.0.1\n",
      "Step #0:   Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 kB 11.0 MB/s eta 0:00:00\n",
      "Step #0: Collecting tabulate<1,>=0.8.6\n",
      "Step #0:   Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Step #0: Collecting click<9,>=7.1.2\n",
      "Step #0:   Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 kB 17.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting Deprecated<2,>=1.2.7\n",
      "Step #0:   Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Step #0: Collecting strip-hints<1,>=0.1.8\n",
      "Step #0:   Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting docstring-parser<1,>=0.7.3\n",
      "Step #0:   Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Step #0: Collecting kfp-pipeline-spec<0.2.0,>=0.1.16\n",
      "Step #0:   Downloading kfp_pipeline_spec-0.1.16-py3-none-any.whl (19 kB)\n",
      "Step #0: Collecting fire<1,>=0.3.1\n",
      "Step #0:   Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 17.0 MB/s eta 0:00:00\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting protobuf<4,>=3.13.0\n",
      "Step #0:   Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 162.1/162.1 kB 19.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting uritemplate<4,>=3.0.1\n",
      "Step #0:   Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Step #0: Collecting pydantic<2,>=1.8.2\n",
      "Step #0:   Downloading pydantic-1.10.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 117.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting typer<1.0,>=0.3.2\n",
      "Step #0:   Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Step #0: Collecting attrs>=19.2.0\n",
      "Step #0:   Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.0/60.0 kB 13.6 MB/s eta 0:00:00\n",
      "Step #0: Collecting iniconfig\n",
      "Step #0:   Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Step #0: Collecting packaging\n",
      "Step #0:   Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.7/42.7 kB 8.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting pluggy<2.0,>=0.12\n",
      "Step #0:   Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Step #0: Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0\n",
      "Step #0:   Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120.3/120.3 kB 20.8 MB/s eta 0:00:00\n",
      "Step #0: Collecting proto-plus<2.0.0dev,>=1.22.0\n",
      "Step #0:   Downloading proto_plus-1.22.2-py3-none-any.whl (47 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.9/47.9 kB 8.6 MB/s eta 0:00:00\n",
      "Step #0: Collecting packaging\n",
      "Step #0:   Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 7.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "Step #0:   Downloading googleapis_common_protos-1.58.0-py2.py3-none-any.whl (223 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 223.0/223.0 kB 40.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting requests<3.0.0dev,>=2.18.0\n",
      "Step #0:   Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 13.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting cachetools<5.0,>=2.0.0\n",
      "Step #0:   Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Step #0: Collecting pyasn1-modules>=0.2.1\n",
      "Step #0:   Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 28.7 MB/s eta 0:00:00\n",
      "Step #0: Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.11/site-packages (from google-auth==1.35.0->-r requirements.txt (line 6)) (65.5.1)\n",
      "Step #0: Collecting six>=1.9.0\n",
      "Step #0:   Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Step #0: Collecting rsa<5,>=3.1.4\n",
      "Step #0:   Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Step #0: Collecting google-resumable-media>=0.3.1\n",
      "Step #0:   Downloading google_resumable_media-2.4.1-py2.py3-none-any.whl (77 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.7/77.7 kB 16.0 MB/s eta 0:00:00\n",
      "Step #0: Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "Step #0:   Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Step #0: Collecting wrapt<2,>=1.10\n",
      "Step #0:   Downloading wrapt-1.14.1.tar.gz (50 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.9/50.9 kB 10.8 MB/s eta 0:00:00\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting termcolor\n",
      "Step #0:   Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Step #0: Collecting grpcio<2.0dev,>=1.33.2\n",
      "Step #0:   Downloading grpcio-1.51.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 67.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "Step #0:   Downloading grpcio_status-1.51.1-py3-none-any.whl (5.1 kB)\n",
      "Step #0: Collecting httplib2<1dev,>=0.15.0\n",
      "Step #0:   Downloading httplib2-0.21.0-py3-none-any.whl (96 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.8/96.8 kB 18.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-auth-httplib2>=0.0.3\n",
      "Step #0:   Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Step #0: Collecting google-crc32c<2.0dev,>=1.0\n",
      "Step #0:   Downloading google_crc32c-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Step #0: Collecting pyrsistent>=0.14.0\n",
      "Step #0:   Downloading pyrsistent-0.19.3-py3-none-any.whl (57 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 9.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting urllib3>=1.15\n",
      "Step #0:   Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 26.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting certifi\n",
      "Step #0:   Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 27.6 MB/s eta 0:00:00\n",
      "Step #0: Collecting python-dateutil\n",
      "Step #0:   Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 40.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
      "Step #0:   Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.3/55.3 kB 10.9 MB/s eta 0:00:00\n",
      "Step #0: Collecting requests-oauthlib\n",
      "Step #0:   Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Step #0: Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "Step #0:   Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.3/98.3 kB 19.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting pyasn1<0.5.0,>=0.4.6\n",
      "Step #0:   Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 14.9 MB/s eta 0:00:00\n",
      "Step #0: Collecting typing-extensions>=4.2.0\n",
      "Step #0:   Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Step #0: Collecting charset-normalizer<4,>=2\n",
      "Step #0:   Downloading charset_normalizer-3.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (196 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.8/196.8 kB 33.9 MB/s eta 0:00:00\n",
      "Step #0: Collecting idna<4,>=2.5\n",
      "Step #0:   Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.5/61.5 kB 10.6 MB/s eta 0:00:00\n",
      "Step #0: Requirement already satisfied: wheel in /usr/local/lib/python3.11/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.18->-r requirements.txt (line 1)) (0.38.4)\n",
      "Step #0: Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "Step #0:   Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
      "Step #0:   Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
      "Step #0:   Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
      "Step #0: Collecting oauthlib>=3.0.0\n",
      "Step #0:   Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 26.5 MB/s eta 0:00:00\n",
      "Step #0: Building wheels for collected packages: kfp, fire, kfp-server-api, PyYAML, strip-hints, wrapt\n",
      "Step #0:   Building wheel for kfp (setup.py): started\n",
      "Step #0:   Building wheel for kfp (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for kfp: filename=kfp-1.8.18-py3-none-any.whl size=426959 sha256=c32abd0d5d7d9a93e38f7672b519ce4f707f827002ac77e45298875062b44e26\n",
      "Step #0:   Stored in directory: /builder/home/.cache/pip/wheels/2e/3a/90/364c70c59f3c23c21353000a526b0816fc84ab984140c46e88\n",
      "Step #0:   Building wheel for fire (setup.py): started\n",
      "Step #0:   Building wheel for fire (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=3436e8fae99d7af3b9d8ed09c75de6e8bca6e7ffc63bab53123bcf84a3fa8b36\n",
      "Step #0:   Stored in directory: /builder/home/.cache/pip/wheels/9a/9a/bd/afea12fa537e804618c037066dd825c8688b08d50e26077d6f\n",
      "Step #0:   Building wheel for kfp-server-api (setup.py): started\n",
      "Step #0:   Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for kfp-server-api: filename=kfp_server_api-1.8.5-py3-none-any.whl size=99699 sha256=f6bb090f2d82168435caccf3f55232eb3b31b0ee00d0b556146bb8f4640712e5\n",
      "Step #0:   Stored in directory: /builder/home/.cache/pip/wheels/32/7f/b3/61a7f6ffd6fc7bf11a8935cb539eefd33f6daff9a95938f531\n",
      "Step #0:   Building wheel for PyYAML (pyproject.toml): started\n",
      "Step #0:   Building wheel for PyYAML (pyproject.toml): finished with status 'done'\n",
      "Step #0:   Created wheel for PyYAML: filename=PyYAML-5.4.1-cp311-cp311-linux_x86_64.whl size=611427 sha256=9ca22d465ea7f20a25136d2e648dc550993f87db79952bc08014ae516723ace2\n",
      "Step #0:   Stored in directory: /builder/home/.cache/pip/wheels/13/d5/5b/082ef0599cd0dde3d1f273ceebebe6cda67201058a70ade961\n",
      "Step #0:   Building wheel for strip-hints (setup.py): started\n",
      "Step #0:   Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22283 sha256=87a4e467d5cb5c8a32dfc81623c6966357a82a9195d36c3236408df8b60ec1bc\n",
      "Step #0:   Stored in directory: /builder/home/.cache/pip/wheels/6a/39/57/0dfdaf032e70cf10c29edc72355217a6fd1b1544d979eafd4b\n",
      "Step #0:   Building wheel for wrapt (setup.py): started\n",
      "Step #0:   Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for wrapt: filename=wrapt-1.14.1-cp311-cp311-linux_x86_64.whl size=77749 sha256=67931554ff74ffb1aba5a20d4171efc73ab2d9042215f4e37ec3d2723d691bf4\n",
      "Step #0:   Stored in directory: /builder/home/.cache/pip/wheels/6c/5f/ff/a07f5f99c11c1480c6d2d1d6e0adadd8c1d05bdd668173bc42\n",
      "Step #0: Successfully built kfp fire kfp-server-api PyYAML strip-hints wrapt\n",
      "Step #0: Installing collected packages: pytz, pyasn1, charset-normalizer, wrapt, websocket-client, urllib3, uritemplate, typing-extensions, termcolor, tabulate, strip-hints, six, rsa, PyYAML, pyrsistent, pyparsing, pyasn1-modules, protobuf, pluggy, oauthlib, iniconfig, idna, grpcio, google-crc32c, docstring-parser, cloudpickle, click, certifi, cachetools, attrs, absl-py, typer, requests, python-dateutil, pydantic, proto-plus, packaging, kfp-pipeline-spec, jsonschema, httplib2, googleapis-common-protos, google-resumable-media, google-auth, fire, Deprecated, requests-toolbelt, requests-oauthlib, pytest, kfp-server-api, grpcio-status, google-auth-httplib2, google-api-core, kubernetes, grpc-google-iam-v1, google-cloud-core, google-api-python-client, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, kfp, google-cloud-aiplatform\n",
      "Step #0:   WARNING: The script normalizer is installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The script wsdump is installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The script tabulate is installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The script strip-hints is installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The script jsonschema is installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The scripts py.test and pytest are installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The scripts dsl-compile, dsl-compile-v2 and kfp are installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The script tb-gcp-uploader is installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0: Successfully installed Deprecated-1.2.13 PyYAML-5.4.1 absl-py-1.4.0 attrs-22.2.0 cachetools-4.2.4 certifi-2022.12.7 charset-normalizer-3.0.1 click-8.1.3 cloudpickle-2.2.1 docstring-parser-0.15 fire-0.5.0 google-api-core-2.10.2 google-api-python-client-1.12.11 google-auth-1.35.0 google-auth-httplib2-0.1.0 google-cloud-aiplatform-1.20.0 google-cloud-bigquery-1.20.0 google-cloud-core-1.7.3 google-cloud-resource-manager-1.6.3 google-cloud-storage-2.2.1 google-crc32c-1.5.0 google-resumable-media-2.4.1 googleapis-common-protos-1.58.0 grpc-google-iam-v1-0.12.6 grpcio-1.51.1 grpcio-status-1.48.2 httplib2-0.21.0 idna-3.4 iniconfig-2.0.0 jsonschema-3.2.0 kfp-1.8.18 kfp-pipeline-spec-0.1.16 kfp-server-api-1.8.5 kubernetes-19.15.0 oauthlib-3.2.2 packaging-21.3 pluggy-1.0.0 proto-plus-1.22.2 protobuf-3.20.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydantic-1.10.4 pyparsing-3.0.9 pyrsistent-0.19.3 pytest-7.2.0 python-dateutil-2.8.2 pytz-2022.7 requests-2.28.2 requests-oauthlib-1.3.1 requests-toolbelt-0.10.1 rsa-4.9 six-1.16.0 strip-hints-0.1.10 tabulate-0.9.0 termcolor-2.2.0 typer-0.7.0 typing-extensions-4.4.0 uritemplate-3.0.1 urllib3-1.26.14 websocket-client-1.4.2 wrapt-1.14.1\n",
      "Step #0: WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Finished Step #0\n",
      "Starting Step #1 - \"compile\"\n",
      "Step #1 - \"compile\": Already have image: python\n",
      "Step #1 - \"compile\": /builder/home/.local/lib/python3.11/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "Step #1 - \"compile\":   warnings.warn(\n",
      "Finished Step #1 - \"compile\"\n",
      "Starting Step #2 - \"test_pipeline\"\n",
      "Step #2 - \"test_pipeline\": Already have image: python\n",
      "Step #2 - \"test_pipeline\": .\n",
      "Step #2 - \"test_pipeline\": ----------------------------------------------------------------------\n",
      "Step #2 - \"test_pipeline\": Ran 1 test in 0.000s\n",
      "Step #2 - \"test_pipeline\": \n",
      "Step #2 - \"test_pipeline\": OK\n",
      "Finished Step #2 - \"test_pipeline\"\n",
      "Starting Step #3 - \"upload\"\n",
      "Step #3 - \"upload\": Already have image (with digest): gcr.io/cloud-builders/gsutil\n",
      "Step #3 - \"upload\": Copying file://pipeline.json [Content-Type=application/json]...\n",
      "/ [1 files][ 30.9 KiB/ 30.9 KiB]                                                                    \n",
      "Step #3 - \"upload\": Operation completed over 1 objects/30.9 KiB.                                     \n",
      "Finished Step #3 - \"upload\"\n",
      "Starting Step #4 - \"test\"\n",
      "Step #4 - \"test\": Already have image: python\n",
      "Step #4 - \"test\": Submitting pipeline mlops-pipeline-prod in experiment qa-test-experiment.\n",
      "Step #4 - \"test\": Creating PipelineJob\n",
      "Step #4 - \"test\": PipelineJob created. Resource name: projects/450304494793/locations/us-central1/pipelineJobs/taxi-tips-training-20230122183930\n",
      "Step #4 - \"test\": To use this PipelineJob in another session:\n",
      "Step #4 - \"test\": pipeline_job = aiplatform.PipelineJob.get('projects/450304494793/locations/us-central1/pipelineJobs/taxi-tips-training-20230122183930')\n",
      "Step #4 - \"test\": View Pipeline Job:\n",
      "Step #4 - \"test\": https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/taxi-tips-training-20230122183930?project=450304494793\n",
      "Step #4 - \"test\": Associating projects/450304494793/locations/us-central1/pipelineJobs/taxi-tips-training-20230122183930 to Experiment: qa-test-experiment\n",
      "Step #4 - \"test\": PipelineJob run completed. Resource name: projects/450304494793/locations/us-central1/pipelineJobs/taxi-tips-training-20230122183930\n",
      "Finished Step #4 - \"test\"\n",
      "Starting Step #5 - \"prod\"\n",
      "Step #5 - \"prod\": Already have image: python\n",
      "Step #5 - \"prod\": Submitting pipeline mlops-pipeline-prod in experiment prod-test-experiment.\n",
      "Step #5 - \"prod\": Creating PipelineJob\n",
      "Step #5 - \"prod\": PipelineJob created. Resource name: projects/450304494793/locations/us-central1/pipelineJobs/taxi-tips-training-20230122183943\n",
      "Step #5 - \"prod\": To use this PipelineJob in another session:\n",
      "Step #5 - \"prod\": pipeline_job = aiplatform.PipelineJob.get('projects/450304494793/locations/us-central1/pipelineJobs/taxi-tips-training-20230122183943')\n",
      "Step #5 - \"prod\": View Pipeline Job:\n",
      "Step #5 - \"prod\": https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/taxi-tips-training-20230122183943?project=450304494793\n",
      "Step #5 - \"prod\": Associating projects/450304494793/locations/us-central1/pipelineJobs/taxi-tips-training-20230122183943 to Experiment: prod-test-experiment\n",
      "Finished Step #5 - \"prod\"\n",
      "PUSH\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                    IMAGES  STATUS\n",
      "aa69153a-7355-4855-98f6-0396eb6ba97e  2023-01-22T18:38:32+00:00  1M16S     gs://qinetiq-workshop23lon-5240_cloudbuild/source/1674412711.713397-699cbb6eff4b4cb39f6b826850350ccc.tgz  -       SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit ./src --config=src/cloudbuild.yaml --substitutions=_BUCKET_NAME=$BUCKET_NAME,_EXPERIMENT_NAME=$EXPERIMENT_NAME,_PIPELINE_NAME=$PIPELINE_NAME,_REGION=$REGION,_ENDPOINT_NAME=$ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fd18c4-dd5f-4598-9cbb-f69e4312700f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m102"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
