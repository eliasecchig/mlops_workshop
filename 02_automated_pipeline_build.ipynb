{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2d4e1a8-7e09-4128-94db-dded48bef1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https:/www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a66b6f",
   "metadata": {},
   "source": [
    "# Automated MLOps pipeline build, testing and deployment\n",
    "\n",
    "In the previous notebook, you created a machine learning pipeline to train a model. In this session, it's all about automating the training and deployment of this model. Hence, the objective this notebook is to:\n",
    "\n",
    "1. Write a pipeline into a Python file that can be compiled into YAML in an automated fashion.\n",
    "2. Define some dummy unit tests\n",
    "3. Write a script to deploy a compiled Kubeflow pipeline to Vertex AI.\n",
    "4. Use Code Build (CI/CD) to compile, test, and run your Kubeflow pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a133e3-9336-429a-8bd2-9df3b7e9e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-mlops\" \n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Experiments\n",
    "EXPERIMENT_NAME = \"test-experiment\"\n",
    "PIPELINE_NAME = \"mlops-pipeline-prod\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a84feaa",
   "metadata": {},
   "source": [
    "### Create a script containing your Vertex AI/Kubeflow Pipeline to compile the pipeline into `pipeline.json`\n",
    "\n",
    "> <font color='green'>**Task 1**</font>\n",
    ">\n",
    "> Create a Python script `src/pipeline.py` that creates a file name `pipeline.json` from a dummy Vertex pipeline\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6430d80-a7bd-4de5-852d-60fded9350ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85aeb7c3-590f-46cb-88bf-8fca8f315962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/requirements.txt\n",
    "kfp==1.8.18\n",
    "pytest==7.2.0\n",
    "pytz==2022.7\n",
    "google-cloud-aiplatform==1.20.0\n",
    "google-api-core==2.10.2\n",
    "google-auth==1.35.0\n",
    "google-cloud-bigquery==1.20.0\n",
    "google-cloud-core==1.7.3\n",
    "google-cloud-resource-manager==1.6.3\n",
    "google-cloud-storage==2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76d71ce2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/pipeline.py\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp.v2.dsl import component, pipeline\n",
    "import kfp.v2.compiler as compiler\n",
    "\n",
    "\n",
    "@component() \n",
    "def concat(a: str, b: str) -> str:\n",
    "    return a + b\n",
    "\n",
    "@component()\n",
    "def reverse(a: str) -> NamedTuple(\"outputs\", [(\"before\", str), (\"after\", str)]):\n",
    "    return a, a[::-1]\n",
    "\n",
    "@pipeline(name=\"mlops-dummy-pipeline\")\n",
    "def basic_pipeline(a: str='stres', b: str='sed'):\n",
    "    concat_task = concat(a=a, b=b)\n",
    "    reverse_task = reverse(a=concat_task.output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    compiler.Compiler().compile(pipeline_func=basic_pipeline, package_path=\"pipeline.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d4148",
   "metadata": {},
   "source": [
    "Using the next command, you can test the materialized pipeline generated by your script. You can view the output in a file named `pipeline.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54c94289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "!python src/pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66bb273c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"pipelineSpec\": {\n",
      "    \"components\": {\n",
      "      \"comp-concat\": {\n",
      "        \"executorLabel\": \"exec-concat\",\n",
      "        \"inputDefinitions\": {\n",
      "          \"parameters\": {\n",
      "            \"a\": {\n",
      "              \"type\": \"STRING\"\n",
      "            },\n",
      "            \"b\": {\n",
      "              \"type\": \"STRING\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"outputDefinitions\": {\n",
      "          \"parameters\": {\n",
      "            \"Output\": {\n",
      "              \"type\": \"STRING\"\n",
      "            }\n"
     ]
    }
   ],
   "source": [
    "!head -n20 pipeline.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca25b67-c432-4d42-8a10-2b9b94e7a980",
   "metadata": {},
   "source": [
    "### Test the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef73fcf-6a9c-49da-bb65-7f724a412c5e",
   "metadata": {},
   "source": [
    "> <font color='green'>**Task 2**</font>\n",
    "> Write unit/integration tests for the pipeline you created to ensure the component logic that you added works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88e162c6-1cbb-456c-8b5c-d513fbe34064",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p src/tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8186aaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/tests/test_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/tests/test_pipeline.py\n",
    "\n",
    "import unittest\n",
    "from pipeline import concat, reverse, basic_pipeline\n",
    "\n",
    "class TestBasicPipeline(unittest.TestCase):\n",
    "    # def setUp(self):\n",
    "        # Get relevant component\n",
    "    \n",
    "    def test_concat_component(self):\n",
    "        self.assertEqual(concat.python_func(3, 3), 6)\n",
    "\n",
    "    def test_reverse(self):\n",
    "        self.assertEqual(reverse.python_func(\"stressed\")[1], \"desserts\")\n",
    "\n",
    "    def test_pipeline(self):\n",
    "        pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a7b41-9996-42c1-8849-70aef63273f9",
   "metadata": {},
   "source": [
    "Using the next command, you can run the tests in the script using python `unittest` test runner. It discovers all the test files that start with `test_*`\n",
    "\n",
    "You can also use other testing framework of your choice (e.g. `pytest`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "191cfe48-0206-477c-a894-7c4ca66d6eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=src python -m unittest discover -s src/tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a576513",
   "metadata": {},
   "source": [
    "### Create a script to submit your compile kubeflow pipeline (`pipeline.json`) to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20c708bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/submit-pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/submit-pipeline.py\n",
    "import os\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "import google.auth\n",
    "\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "if not PROJECT_ID:\n",
    "    creds, PROJECT_ID = google.auth.default()\n",
    "\n",
    "REGION = os.environ[\"REGION\"]\n",
    "BUCKET_NAME = os.environ[\"BUCKET_NAME\"]\n",
    "PIPELINE_NAME = os.environ[\"PIPELINE_NAME\"]\n",
    "EXPERIMENT_NAME = os.environ.get(\"EXPERIMENT_NAME\", \"dummy-experiment\")\n",
    "ENDPOINT_NAME = os.environ.get(\"ENDPOINT_NAME\",\"dummy-endpoint\")\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "sync_pipeline = os.getenv(\"SUBMIT_PIPELINE_SYNC\", 'False').lower() in ('true', '1', 't')\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path='pipeline.json',\n",
    "    location=REGION,\n",
    "    project=PROJECT_ID,\n",
    "    enable_caching=True,\n",
    "    pipeline_root=f'gs://{BUCKET_NAME}',\n",
    ")\n",
    "print(f\"Submitting pipeline {PIPELINE_NAME} in experiment {EXPERIMENT_NAME}.\")\n",
    "job.submit(experiment=EXPERIMENT_NAME)\n",
    "\n",
    "if sync_pipeline:\n",
    "    job.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3de2c1",
   "metadata": {},
   "source": [
    "Let's test this script in the Notebook. You can check the pipeline's status by clicking on the link printed by the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d630863",
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env REGION=$REGION\n",
    "%set_env BUCKET_NAME=$BUCKET_NAME\n",
    "%set_env EXPERIMENT_NAME=$EXPERIMENT_NAME\n",
    "%set_env PIPELINE_NAME=$PIPELINE_NAME\n",
    "%set_env ENDPOINT_NAME=$ENDPOINT_NAME\n",
    "%set_env SUBMIT_PIPELINE_SYNC=1\n",
    "\n",
    "!python src/submit-pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82cf0be",
   "metadata": {},
   "source": [
    "### Automate Kubeflow pipeline compilation, template generation, and execution through Cloud Build\n",
    "\n",
    "Cloud Build is a service that executes your builds on Google Cloud. In this exercise, we want to use it to both compile and run your machine learning pipeline. For more information, please refer to the [Cloud Build documentation](https://cloud.google.com/build/docs/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "243972cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/cloudbuild.yaml\n",
    "steps:\n",
    "  # Install dependencies\n",
    "  - name: 'python'\n",
    "    entrypoint: 'pip'\n",
    "    args: [\"install\", \"-r\", \"requirements.txt\", \"--user\"]\n",
    "\n",
    "  # Compile pipeline\n",
    "  - name: 'python'\n",
    "    entrypoint: 'python'\n",
    "    args: ['pipeline.py']\n",
    "    id: 'compile'\n",
    "\n",
    "  # Test the Pipeline Components \n",
    "  - name: 'python'\n",
    "    entrypoint: 'python'\n",
    "    args: ['-m', 'unittest', 'discover', 'tests/']\n",
    "    id: 'test_pipeline'\n",
    "    waitFor: ['compile']\n",
    "\n",
    "  # Upload compiled pipeline to GCS\n",
    "  - name: 'gcr.io/cloud-builders/gsutil'\n",
    "    args: ['cp', 'pipeline.json', 'gs://${_BUCKET_NAME}']\n",
    "    id: 'upload'\n",
    "    waitFor: ['test_pipeline']\n",
    "        \n",
    "  # Run the Vertex AI Pipeline (synchronously for test/qa environment).\n",
    "  - name: 'python'\n",
    "    id: 'test'\n",
    "    entrypoint: 'python'\n",
    "    env: ['BUCKET_NAME=${_BUCKET_NAME}', 'EXPERIMENT_NAME=qa-${_EXPERIMENT_NAME}', 'PIPELINE_NAME=${_PIPELINE_NAME}',\n",
    "          'REGION=${_REGION}', 'ENDPOINT_NAME=qa-${_ENDPOINT_NAME}', 'SUBMIT_PIPELINE_SYNC=true']\n",
    "    args: ['submit-pipeline.py']\n",
    "    \n",
    "  # Run the Vertex AI Pipeline (asynchronously for prod environment). In a real production scenario, this would run in a different GCP project.\n",
    "  - name: 'python'\n",
    "    id: 'prod'\n",
    "    entrypoint: 'python'\n",
    "    env: ['BUCKET_NAME=${_BUCKET_NAME}', 'EXPERIMENT_NAME=prod-${_EXPERIMENT_NAME}', 'PIPELINE_NAME=${_PIPELINE_NAME}',\n",
    "          'REGION=${_REGION}', 'ENDPOINT_NAME=prod-${_ENDPOINT_NAME}', 'SUBMIT_PIPELINE_SYNC=false']\n",
    "    args: ['submit-pipeline.py']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4958d76",
   "metadata": {},
   "source": [
    "Cloud Build uses a special service account to execute builds on your behalf. When you enable the Cloud Build API on a Google Cloud project, the Cloud Build service account is automatically created and granted the Cloud Build Service Account role for the project. This role gives the service account permissions to perform several tasks, however you can grant more permissions to the service account to perform additional tasks. [This page](https://cloud.google.com/build/docs/securing-builds/configure-access-for-cloud-build-service-account) explains how to grant and revoke permissions to the Cloud Build service account.\n",
    "\n",
    "For Cloud Build to be able to deploy your pipeline, you need to give its' service account `{PROJECT_NUMBER}@cloudbuild.gserviceaccount.com` the **Vertex AI User** and **Service Account User** role. (Step already done as part of the workshop setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4d136-3976-40b8-b3e9-626a40e6d3f1",
   "metadata": {},
   "source": [
    "Now you are ready to trigger this pipeline. This CICD pipeline can be embedded in your repository and triggered when a file changes with any\n",
    "git provider, see more about triggering [here](https://cloud.google.com/build/docs/automating-builds/create-manage-triggers). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b94498-2429-4f2c-af40-313279218907",
   "metadata": {},
   "source": [
    "When you submit your pipeline you can also see the status of it by clicking on the link relative to the Logs (Logs are available at [..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6d3ea3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 7 file(s) totalling 5.6 KiB before compression.\n",
      "Uploading tarball of [./src] to [gs://vertex-ai-test-365213_cloudbuild/source/1674498988.849607-f2a74d912ba04da58ac0a1bccaea3928.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/vertex-ai-test-365213/locations/global/builds/367b1e8e-14d6-4228-8ad5-48e3dbb7683a].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/367b1e8e-14d6-4228-8ad5-48e3dbb7683a?project=370018035372 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"367b1e8e-14d6-4228-8ad5-48e3dbb7683a\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://vertex-ai-test-365213_cloudbuild/source/1674498988.849607-f2a74d912ba04da58ac0a1bccaea3928.tgz#1674498989124292\n",
      "Copying gs://vertex-ai-test-365213_cloudbuild/source/1674498988.849607-f2a74d912ba04da58ac0a1bccaea3928.tgz#1674498989124292...\n",
      "/ [1 files][  3.0 KiB/  3.0 KiB]                                                \n",
      "Operation completed over 1 objects/3.0 KiB.\n",
      "BUILD\n",
      "Starting Step #0\n",
      "Step #0: Pulling image: python\n",
      "Step #0: Using default tag: latest\n",
      "Step #0: latest: Pulling from library/python\n",
      "Step #0: bbeef03cda1f: Already exists\n",
      "Step #0: f049f75f014e: Already exists\n",
      "Step #0: 56261d0e6b05: Already exists\n",
      "Step #0: 9bd150679dbd: Already exists\n",
      "Step #0: 5b282ee9da04: Already exists\n",
      "Step #0: 03f027d5e312: Already exists\n",
      "Step #0: db6ee1ace097: Pulling fs layer\n",
      "Step #0: 0a86d528f1ea: Pulling fs layer\n",
      "Step #0: 4cfb032ae58b: Pulling fs layer\n",
      "Step #0: 0a86d528f1ea: Verifying Checksum\n",
      "Step #0: 0a86d528f1ea: Download complete\n",
      "Step #0: 4cfb032ae58b: Verifying Checksum\n",
      "Step #0: 4cfb032ae58b: Download complete\n",
      "Step #0: db6ee1ace097: Verifying Checksum\n",
      "Step #0: db6ee1ace097: Download complete\n",
      "Step #0: db6ee1ace097: Pull complete\n",
      "Step #0: 0a86d528f1ea: Pull complete\n",
      "Step #0: 4cfb032ae58b: Pull complete\n",
      "Step #0: Digest: sha256:a3c0c6766535f85f18e7304d3a0111de5208d73935bcf1b024217005ad5ce195\n",
      "Step #0: Status: Downloaded newer image for python:latest\n",
      "Step #0: docker.io/library/python:latest\n",
      "Step #0: Collecting kfp==1.8.18\n",
      "Step #0:   Downloading kfp-1.8.18.tar.gz (304 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 304.8/304.8 kB 22.3 MB/s eta 0:00:00\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting pytest==7.2.0\n",
      "Step #0:   Downloading pytest-7.2.0-py3-none-any.whl (316 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 316.8/316.8 kB 34.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting pytz==2022.7\n",
      "Step #0:   Downloading pytz-2022.7-py2.py3-none-any.whl (499 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 499.4/499.4 kB 46.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-aiplatform==1.20.0\n",
      "Step #0:   Downloading google_cloud_aiplatform-1.20.0-py2.py3-none-any.whl (2.3 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 74.8 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-api-core==2.10.2\n",
      "Step #0:   Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.6/115.6 kB 17.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-auth==1.35.0\n",
      "Step #0:   Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152.9/152.9 kB 24.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-bigquery==1.20.0\n",
      "Step #0:   Downloading google_cloud_bigquery-1.20.0-py2.py3-none-any.whl (154 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.0/155.0 kB 23.6 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-core==1.7.3\n",
      "Step #0:   Downloading google_cloud_core-1.7.3-py2.py3-none-any.whl (28 kB)\n",
      "Step #0: Collecting google-cloud-resource-manager==1.6.3\n",
      "Step #0:   Downloading google_cloud_resource_manager-1.6.3-py2.py3-none-any.whl (233 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.8/233.8 kB 31.0 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-storage==2.2.1\n",
      "Step #0:   Downloading google_cloud_storage-2.2.1-py2.py3-none-any.whl (107 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107.1/107.1 kB 18.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting absl-py<2,>=0.9\n",
      "Step #0:   Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 21.0 MB/s eta 0:00:00\n",
      "Step #0: Collecting PyYAML<6,>=5.3\n",
      "Step #0:   Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 175.1/175.1 kB 27.6 MB/s eta 0:00:00\n",
      "Step #0:   Installing build dependencies: started\n",
      "Step #0:   Installing build dependencies: finished with status 'done'\n",
      "Step #0:   Getting requirements to build wheel: started\n",
      "Step #0:   Getting requirements to build wheel: finished with status 'done'\n",
      "Step #0:   Preparing metadata (pyproject.toml): started\n",
      "Step #0:   Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Step #0: Collecting kubernetes<20,>=8.0.0\n",
      "Step #0:   Downloading kubernetes-19.15.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 60.8 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-api-python-client<2,>=1.7.8\n",
      "Step #0:   Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.1/62.1 kB 10.0 MB/s eta 0:00:00\n",
      "Step #0: Collecting requests-toolbelt<1,>=0.8.0\n",
      "Step #0:   Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 kB 9.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting cloudpickle<3,>=2.0.0\n",
      "Step #0:   Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Step #0: Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "Step #0:   Downloading kfp-server-api-1.8.5.tar.gz (58 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.1/58.1 kB 9.0 MB/s eta 0:00:00\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting jsonschema<4,>=3.0.1\n",
      "Step #0:   Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 kB 7.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting tabulate<1,>=0.8.6\n",
      "Step #0:   Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Step #0: Collecting click<9,>=7.1.2\n",
      "Step #0:   Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 kB 13.9 MB/s eta 0:00:00\n",
      "Step #0: Collecting Deprecated<2,>=1.2.7\n",
      "Step #0:   Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Step #0: Collecting strip-hints<1,>=0.1.8\n",
      "Step #0:   Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting docstring-parser<1,>=0.7.3\n",
      "Step #0:   Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Step #0: Collecting kfp-pipeline-spec<0.2.0,>=0.1.16\n",
      "Step #0:   Downloading kfp_pipeline_spec-0.1.16-py3-none-any.whl (19 kB)\n",
      "Step #0: Collecting fire<1,>=0.3.1\n",
      "Step #0:   Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 15.7 MB/s eta 0:00:00\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting protobuf<4,>=3.13.0\n",
      "Step #0:   Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 162.1/162.1 kB 22.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting uritemplate<4,>=3.0.1\n",
      "Step #0:   Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Step #0: Collecting pydantic<2,>=1.8.2\n",
      "Step #0:   Downloading pydantic-1.10.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 82.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting typer<1.0,>=0.3.2\n",
      "Step #0:   Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Step #0: Collecting attrs>=19.2.0\n",
      "Step #0:   Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.0/60.0 kB 8.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting iniconfig\n",
      "Step #0:   Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Step #0: Collecting packaging\n",
      "Step #0:   Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.7/42.7 kB 6.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting pluggy<2.0,>=0.12\n",
      "Step #0:   Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Step #0: Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0\n",
      "Step #0:   Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120.3/120.3 kB 18.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting proto-plus<2.0.0dev,>=1.22.0\n",
      "Step #0:   Downloading proto_plus-1.22.2-py3-none-any.whl (47 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.9/47.9 kB 8.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting packaging\n",
      "Step #0:   Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 6.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "Step #0:   Downloading googleapis_common_protos-1.58.0-py2.py3-none-any.whl (223 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 223.0/223.0 kB 30.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting requests<3.0.0dev,>=2.18.0\n",
      "Step #0:   Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 9.9 MB/s eta 0:00:00\n",
      "Step #0: Collecting cachetools<5.0,>=2.0.0\n",
      "Step #0:   Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Step #0: Collecting pyasn1-modules>=0.2.1\n",
      "Step #0:   Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 22.5 MB/s eta 0:00:00\n",
      "Step #0: Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.11/site-packages (from google-auth==1.35.0->-r requirements.txt (line 6)) (65.5.1)\n",
      "Step #0: Collecting six>=1.9.0\n",
      "Step #0:   Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Step #0: Collecting rsa<5,>=3.1.4\n",
      "Step #0:   Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Step #0: Collecting google-resumable-media>=0.3.1\n",
      "Step #0:   Downloading google_resumable_media-2.4.1-py2.py3-none-any.whl (77 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.7/77.7 kB 14.0 MB/s eta 0:00:00\n",
      "Step #0: Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "Step #0:   Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Step #0: Collecting wrapt<2,>=1.10\n",
      "Step #0:   Downloading wrapt-1.14.1.tar.gz (50 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.9/50.9 kB 8.6 MB/s eta 0:00:00\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting termcolor\n",
      "Step #0:   Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Step #0: Collecting grpcio<2.0dev,>=1.33.2\n",
      "Step #0:   Downloading grpcio-1.51.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 56.3 MB/s eta 0:00:00\n",
      "Step #0: Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "Step #0:   Downloading grpcio_status-1.51.1-py3-none-any.whl (5.1 kB)\n",
      "Step #0: Collecting httplib2<1dev,>=0.15.0\n",
      "Step #0:   Downloading httplib2-0.21.0-py3-none-any.whl (96 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.8/96.8 kB 13.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-auth-httplib2>=0.0.3\n",
      "Step #0:   Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Step #0: Collecting google-crc32c<2.0dev,>=1.0\n",
      "Step #0:   Downloading google_crc32c-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Step #0: Collecting pyrsistent>=0.14.0\n",
      "Step #0:   Downloading pyrsistent-0.19.3-py3-none-any.whl (57 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 8.8 MB/s eta 0:00:00\n",
      "Step #0: Collecting urllib3>=1.15\n",
      "Step #0:   Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 22.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting certifi\n",
      "Step #0:   Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 20.8 MB/s eta 0:00:00\n",
      "Step #0: Collecting python-dateutil\n",
      "Step #0:   Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 31.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
      "Step #0:   Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.3/55.3 kB 8.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting requests-oauthlib\n",
      "Step #0:   Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Step #0: Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "Step #0:   Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.3/98.3 kB 16.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting pyasn1<0.5.0,>=0.4.6\n",
      "Step #0:   Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 12.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting typing-extensions>=4.2.0\n",
      "Step #0:   Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Step #0: Collecting charset-normalizer<4,>=2\n",
      "Step #0:   Downloading charset_normalizer-3.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (196 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.8/196.8 kB 16.3 MB/s eta 0:00:00\n",
      "Step #0: Collecting idna<4,>=2.5\n",
      "Step #0:   Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.5/61.5 kB 10.2 MB/s eta 0:00:00\n",
      "Step #0: Requirement already satisfied: wheel in /usr/local/lib/python3.11/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.18->-r requirements.txt (line 1)) (0.38.4)\n",
      "Step #0: Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "Step #0:   Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
      "Step #0:   Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
      "Step #0:   Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
      "Step #0: Collecting oauthlib>=3.0.0\n",
      "Step #0:   Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 19.3 MB/s eta 0:00:00\n",
      "Step #0: Building wheels for collected packages: kfp, fire, kfp-server-api, PyYAML, strip-hints, wrapt\n",
      "Step #0:   Building wheel for kfp (setup.py): started\n",
      "Step #0:   Building wheel for kfp (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for kfp: filename=kfp-1.8.18-py3-none-any.whl size=426959 sha256=b2465090e9778dee0710bb5447bc2cef2af7a1144b4339c5e332d8a0db81a844\n",
      "Step #0:   Stored in directory: /builder/home/.cache/pip/wheels/2e/3a/90/364c70c59f3c23c21353000a526b0816fc84ab984140c46e88\n",
      "Step #0:   Building wheel for fire (setup.py): started\n",
      "Step #0:   Building wheel for fire (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=efeadd1a2024ed3c6c13a499c9ef49446b806bfadedb8c732f345ea987830449\n",
      "Step #0:   Stored in directory: /builder/home/.cache/pip/wheels/9a/9a/bd/afea12fa537e804618c037066dd825c8688b08d50e26077d6f\n",
      "Step #0:   Building wheel for kfp-server-api (setup.py): started\n",
      "Step #0:   Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for kfp-server-api: filename=kfp_server_api-1.8.5-py3-none-any.whl size=99699 sha256=198df1502359827eb0eb29f7dfc0ec7c1d6339b272fffbe2dd52e6945f0a6ca3\n",
      "Step #0:   Stored in directory: /builder/home/.cache/pip/wheels/32/7f/b3/61a7f6ffd6fc7bf11a8935cb539eefd33f6daff9a95938f531\n",
      "Step #0:   Building wheel for PyYAML (pyproject.toml): started\n",
      "Step #0:   Building wheel for PyYAML (pyproject.toml): finished with status 'done'\n",
      "Step #0:   Created wheel for PyYAML: filename=PyYAML-5.4.1-cp311-cp311-linux_x86_64.whl size=611432 sha256=a47474f8890edf388ffe510239924801e2c25397904d4d6bf5f87ff5bddfaa12\n",
      "Step #0:   Stored in directory: /builder/home/.cache/pip/wheels/13/d5/5b/082ef0599cd0dde3d1f273ceebebe6cda67201058a70ade961\n",
      "Step #0:   Building wheel for strip-hints (setup.py): started\n",
      "Step #0:   Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22283 sha256=2cd7386f7e997e938ead4c893e8d56b2cdaf34ac937ca70314cccf95cf0da6d3\n",
      "Step #0:   Stored in directory: /builder/home/.cache/pip/wheels/6a/39/57/0dfdaf032e70cf10c29edc72355217a6fd1b1544d979eafd4b\n",
      "Step #0:   Building wheel for wrapt (setup.py): started\n",
      "Step #0:   Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for wrapt: filename=wrapt-1.14.1-cp311-cp311-linux_x86_64.whl size=77744 sha256=2b558a18d3bfee2ccf07aa26c0c13993d39cb13f6fbf33cb1b02cccbad4bc384\n",
      "Step #0:   Stored in directory: /builder/home/.cache/pip/wheels/6c/5f/ff/a07f5f99c11c1480c6d2d1d6e0adadd8c1d05bdd668173bc42\n",
      "Step #0: Successfully built kfp fire kfp-server-api PyYAML strip-hints wrapt\n",
      "Step #0: Installing collected packages: pytz, pyasn1, charset-normalizer, wrapt, websocket-client, urllib3, uritemplate, typing-extensions, termcolor, tabulate, strip-hints, six, rsa, PyYAML, pyrsistent, pyparsing, pyasn1-modules, protobuf, pluggy, oauthlib, iniconfig, idna, grpcio, google-crc32c, docstring-parser, cloudpickle, click, certifi, cachetools, attrs, absl-py, typer, requests, python-dateutil, pydantic, proto-plus, packaging, kfp-pipeline-spec, jsonschema, httplib2, googleapis-common-protos, google-resumable-media, google-auth, fire, Deprecated, requests-toolbelt, requests-oauthlib, pytest, kfp-server-api, grpcio-status, google-auth-httplib2, google-api-core, kubernetes, grpc-google-iam-v1, google-cloud-core, google-api-python-client, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, kfp, google-cloud-aiplatform\n",
      "Step #0:   WARNING: The script normalizer is installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The script wsdump is installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The script tabulate is installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The script strip-hints is installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The script jsonschema is installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The scripts py.test and pytest are installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The scripts dsl-compile, dsl-compile-v2 and kfp are installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0:   WARNING: The script tb-gcp-uploader is installed in '/builder/home/.local/bin' which is not on PATH.\n",
      "Step #0:   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Step #0: Successfully installed Deprecated-1.2.13 PyYAML-5.4.1 absl-py-1.4.0 attrs-22.2.0 cachetools-4.2.4 certifi-2022.12.7 charset-normalizer-3.0.1 click-8.1.3 cloudpickle-2.2.1 docstring-parser-0.15 fire-0.5.0 google-api-core-2.10.2 google-api-python-client-1.12.11 google-auth-1.35.0 google-auth-httplib2-0.1.0 google-cloud-aiplatform-1.20.0 google-cloud-bigquery-1.20.0 google-cloud-core-1.7.3 google-cloud-resource-manager-1.6.3 google-cloud-storage-2.2.1 google-crc32c-1.5.0 google-resumable-media-2.4.1 googleapis-common-protos-1.58.0 grpc-google-iam-v1-0.12.6 grpcio-1.51.1 grpcio-status-1.48.2 httplib2-0.21.0 idna-3.4 iniconfig-2.0.0 jsonschema-3.2.0 kfp-1.8.18 kfp-pipeline-spec-0.1.16 kfp-server-api-1.8.5 kubernetes-19.15.0 oauthlib-3.2.2 packaging-21.3 pluggy-1.0.0 proto-plus-1.22.2 protobuf-3.20.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydantic-1.10.4 pyparsing-3.0.9 pyrsistent-0.19.3 pytest-7.2.0 python-dateutil-2.8.2 pytz-2022.7 requests-2.28.2 requests-oauthlib-1.3.1 requests-toolbelt-0.10.1 rsa-4.9 six-1.16.0 strip-hints-0.1.10 tabulate-0.9.0 termcolor-2.2.0 typer-0.7.0 typing-extensions-4.4.0 uritemplate-3.0.1 urllib3-1.26.14 websocket-client-1.4.2 wrapt-1.14.1\n",
      "Step #0: WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Finished Step #0\n",
      "Starting Step #1 - \"compile\"\n",
      "Step #1 - \"compile\": Already have image: python\n",
      "Step #1 - \"compile\": /builder/home/.local/lib/python3.11/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "Step #1 - \"compile\":   warnings.warn(\n",
      "Finished Step #1 - \"compile\"\n",
      "Starting Step #2 - \"test_pipeline\"\n",
      "Step #2 - \"test_pipeline\": Already have image: python\n",
      "Step #2 - \"test_pipeline\": ...\n",
      "Step #2 - \"test_pipeline\": ----------------------------------------------------------------------\n",
      "Step #2 - \"test_pipeline\": Ran 3 tests in 0.000s\n",
      "Step #2 - \"test_pipeline\": \n",
      "Step #2 - \"test_pipeline\": OK\n",
      "Finished Step #2 - \"test_pipeline\"\n",
      "Starting Step #3 - \"upload\"\n",
      "Step #3 - \"upload\": Already have image (with digest): gcr.io/cloud-builders/gsutil\n",
      "Step #3 - \"upload\": Copying file://pipeline.json [Content-Type=application/json]...\n",
      "/ [1 files][  4.9 KiB/  4.9 KiB]                                                                    \n",
      "Step #3 - \"upload\": Operation completed over 1 objects/4.9 KiB.\n",
      "Finished Step #3 - \"upload\"\n",
      "Starting Step #4 - \"test\"\n",
      "Step #4 - \"test\": Already have image: python\n",
      "Step #4 - \"test\": Submitting pipeline mlops-pipeline-prod in experiment qa-test-experiment.\n",
      "Step #4 - \"test\": Creating PipelineJob\n",
      "Step #4 - \"test\": PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/mlops-coaching-pipeline-20230123183747\n",
      "Step #4 - \"test\": To use this PipelineJob in another session:\n",
      "Step #4 - \"test\": pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/mlops-coaching-pipeline-20230123183747')\n",
      "Step #4 - \"test\": View Pipeline Job:\n",
      "Step #4 - \"test\": https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/mlops-coaching-pipeline-20230123183747?project=370018035372\n",
      "Step #4 - \"test\": Associating projects/370018035372/locations/us-central1/pipelineJobs/mlops-coaching-pipeline-20230123183747 to Experiment: qa-test-experiment\n",
      "Step #4 - \"test\": PipelineJob run completed. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/mlops-coaching-pipeline-20230123183747\n",
      "Finished Step #4 - \"test\"\n",
      "Starting Step #5 - \"prod\"\n",
      "Step #5 - \"prod\": Already have image: python\n",
      "Step #5 - \"prod\": Submitting pipeline mlops-pipeline-prod in experiment prod-test-experiment.\n",
      "Step #5 - \"prod\": Creating PipelineJob\n",
      "Step #5 - \"prod\": PipelineJob created. Resource name: projects/370018035372/locations/us-central1/pipelineJobs/mlops-coaching-pipeline-20230123183802\n",
      "Step #5 - \"prod\": To use this PipelineJob in another session:\n",
      "Step #5 - \"prod\": pipeline_job = aiplatform.PipelineJob.get('projects/370018035372/locations/us-central1/pipelineJobs/mlops-coaching-pipeline-20230123183802')\n",
      "Step #5 - \"prod\": View Pipeline Job:\n",
      "Step #5 - \"prod\": https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/mlops-coaching-pipeline-20230123183802?project=370018035372\n",
      "Step #5 - \"prod\": Associating projects/370018035372/locations/us-central1/pipelineJobs/mlops-coaching-pipeline-20230123183802 to Experiment: prod-test-experiment\n",
      "Finished Step #5 - \"prod\"\n",
      "PUSH\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                               IMAGES  STATUS\n",
      "367b1e8e-14d6-4228-8ad5-48e3dbb7683a  2023-01-23T18:36:29+00:00  1M39S     gs://vertex-ai-test-365213_cloudbuild/source/1674498988.849607-f2a74d912ba04da58ac0a1bccaea3928.tgz  -       SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit ./src --config=src/cloudbuild.yaml --substitutions=_BUCKET_NAME=$BUCKET_NAME,_EXPERIMENT_NAME=$EXPERIMENT_NAME,_PIPELINE_NAME=$PIPELINE_NAME,_REGION=$REGION,_ENDPOINT_NAME=$ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599bb0a8-244a-4a5c-b9c1-e8f1bebc819c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m102"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
